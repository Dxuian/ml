{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load saved bm.keras and optimize it and then also train on test data and train data then upload final \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#default ml libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sklearn as sk\n",
    "#tf libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import kfold from sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test.csv file\n",
    "DEV = True\n",
    "loaddata = pd.read_csv('train.csv')\n",
    "testdata = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  person_age  person_income person_home_ownership  person_emp_length  \\\n",
      "0   0          37          35000                  RENT                0.0   \n",
      "1   1          22          56000                   OWN                6.0   \n",
      "2   2          29          28800                   OWN                8.0   \n",
      "3   3          30          70000                  RENT               14.0   \n",
      "4   4          22          60000                  RENT                2.0   \n",
      "\n",
      "  loan_intent loan_grade  loan_amnt  loan_int_rate  loan_percent_income  \\\n",
      "0   EDUCATION          B       6000          11.49                 0.17   \n",
      "1     MEDICAL          C       4000          13.35                 0.07   \n",
      "2    PERSONAL          A       6000           8.90                 0.21   \n",
      "3     VENTURE          B      12000          11.11                 0.17   \n",
      "4     MEDICAL          A       6000           6.92                 0.10   \n",
      "\n",
      "  cb_person_default_on_file  cb_person_cred_hist_length  loan_status  \n",
      "0                         N                          14            0  \n",
      "1                         N                           2            0  \n",
      "2                         N                          10            0  \n",
      "3                         N                           5            0  \n",
      "4                         N                           3            0  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      id  person_age  person_income person_home_ownership  person_emp_length  \\\n",
      "0  58645          23          69000                  RENT                3.0   \n",
      "1  58646          26          96000              MORTGAGE                6.0   \n",
      "2  58647          26          30000                  RENT                5.0   \n",
      "3  58648          33          50000                  RENT                4.0   \n",
      "4  58649          26         102000              MORTGAGE                8.0   \n",
      "\n",
      "         loan_intent loan_grade  loan_amnt  loan_int_rate  \\\n",
      "0    HOMEIMPROVEMENT          F      25000          15.76   \n",
      "1           PERSONAL          C      10000          12.68   \n",
      "2            VENTURE          E       4000          17.19   \n",
      "3  DEBTCONSOLIDATION          A       7000           8.90   \n",
      "4    HOMEIMPROVEMENT          D      15000          16.32   \n",
      "\n",
      "   loan_percent_income cb_person_default_on_file  cb_person_cred_hist_length  \n",
      "0                 0.36                         N                           2  \n",
      "1                 0.10                         Y                           4  \n",
      "2                 0.13                         Y                           2  \n",
      "3                 0.14                         N                           7  \n",
      "4                 0.15                         Y                           4  \n"
     ]
    }
   ],
   "source": [
    "train = loaddata.copy()\n",
    "print(train.head())\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "test = testdata.copy()\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1838/2847353923.py:68: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['person_home_ownership'] = test['person_home_ownership'].replace({'RENT': 0, 'MORTGAGE': 1, 'OWN': 2, 'OTHER': 3}).astype('category')\n",
      "/tmp/ipykernel_1838/2847353923.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['loan_intent'] = test['loan_intent'].replace({'EDUCATION': 0, 'MEDICAL': 1, 'PERSONAL': 2, 'VENTURE': 3, 'DEBTCONSOLIDATION': 4, 'HOMEIMPROVEMENT': 5}).astype('category')\n",
      "/tmp/ipykernel_1838/2847353923.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['loan_grade'] = test['loan_grade'].replace({'A':5, 'B':4, 'C':3, 'D':2, 'E':1, 'F':0, 'G':0}).astype('category')\n",
      "/tmp/ipykernel_1838/2847353923.py:71: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['cb_person_default_on_file'] = test['cb_person_default_on_file'].replace({'N': 0, 'Y': 1}).astype('category')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, ..., 3136, 3137, 3138, 3139, 3140]\n",
       "Length: 3141\n",
       "Categories (3141, int64): [0, 1, 2, 3, ..., 3137, 3138, 3139, 3140]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature engineering\n",
    "test[\"loantoincome\"] = ((test[\"loan_amnt\"] / test[\"person_income\"])).astype('Float64')\n",
    "test[\"loan_percent_incometoincome\"] = ((test[\"loan_percent_income\"] / test[\"person_income\"])).astype('Float64')\n",
    "test['person_age_to_person_income'] = (test['person_age'] / test['person_income']).astype(str).astype('Float64')\n",
    "test['person_emp_length_to_person_age'] = (test['person_emp_length'] / test['person_age']).astype('Float64')\n",
    "test['loan_int_rate_to_loan_amnt'] = (test['loan_int_rate'] / test['loan_amnt']).astype('Float64')\n",
    "\n",
    "\n",
    "\n",
    "test['income_to_age'] = test['person_income'] / test['person_age']\n",
    "test['loan_to_income'] = test['loan_amnt'] / test['person_income']\n",
    "test['rate_to_loan'] = test['loan_int_rate'] / test['loan_amnt']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test['log_income'] = (np.log1p(test['person_income'])).astype('Float64')\n",
    "test['age_credit_history_interaction'] = (test['person_age'] * test['cb_person_cred_hist_length']).astype('float64')\n",
    "test['high_loan_to_income'] = (test['loan_percent_income'] > 0.5).astype('float64')\n",
    "test['is_new_credit_user'] = (test['cb_person_cred_hist_length'] < 2).astype('float64')\n",
    "test['high_interest_rate'] = (test['loan_int_rate'] > test['loan_int_rate'].mean()).astype('float64')\n",
    "test['loan_to_employment'] = test['loan_amnt'] / (test['person_emp_length'] + 1)\n",
    "test['rate_to_grade'] = test.groupby('loan_grade')['loan_int_rate'].transform('mean')\n",
    "\n",
    "\n",
    "def categorize_age(age):\n",
    "    if age <= 25:\n",
    "        return 0\n",
    "    elif age <= 35:\n",
    "        return 1\n",
    "    elif age <= 45:\n",
    "        return 2\n",
    "    elif age <= 55:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# test['income_category'] = pd.qcut(test['person_income'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "# test['intent_grade_interaction'] = test['loan_intent'].astype(str) + '_' + test['loan_grade'].astype(str)\n",
    "# test['home_ownership_intent'] = test['person_home_ownership'].astype(str) + '_' + test['loan_intent'].astype(str)\n",
    "\n",
    "# Function to calculate and store quantiles\n",
    "\n",
    "\n",
    "\n",
    "test['age_category'] = test['person_age'].apply(categorize_age).astype('category')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test['person_home_ownership'] = test['person_home_ownership'].replace({'RENT': 0, 'MORTGAGE': 1, 'OWN': 2, 'OTHER': 3}).astype('category')\n",
    "test['loan_intent'] = test['loan_intent'].replace({'EDUCATION': 0, 'MEDICAL': 1, 'PERSONAL': 2, 'VENTURE': 3, 'DEBTCONSOLIDATION': 4, 'HOMEIMPROVEMENT': 5}).astype('category')\n",
    "test['loan_grade'] = test['loan_grade'].replace({'A':5, 'B':4, 'C':3, 'D':2, 'E':1, 'F':0, 'G':0}).astype('category')\n",
    "test['cb_person_default_on_file'] = test['cb_person_default_on_file'].replace({'N': 0, 'Y': 1}).astype('category')\n",
    "test[\"person_home_ownership_income\"] = pd.Series(pd.factorize((test[\"person_home_ownership\"].astype(str) + test[\"person_income\"].astype(str)).to_numpy())[0]).astype('category')\n",
    "\n",
    "test['person_home_ownership_income'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all files that start with lgbm and xgb \n",
    "\n",
    "for item in os.listdir():\n",
    "    if item.startswith(\"lgbm\") or item.startswith(\"xgb\"):\n",
    "        os.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1838/864817692.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['person_home_ownership'] = train['person_home_ownership'].replace({'RENT': 0, 'MORTGAGE': 1, 'OWN': 2, 'OTHER': 3}).astype('category')\n",
      "/tmp/ipykernel_1838/864817692.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['loan_intent'] = train['loan_intent'].replace({'EDUCATION': 0, 'MEDICAL': 1, 'PERSONAL': 2, 'VENTURE': 3, 'DEBTCONSOLIDATION': 4, 'HOMEIMPROVEMENT': 5}).astype('category')\n",
      "/tmp/ipykernel_1838/864817692.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['loan_grade'] = train['loan_grade'].replace({'A':5, 'B':4, 'C':3, 'D':2, 'E':1, 'F':0, 'G':0}).astype('category')\n",
      "/tmp/ipykernel_1838/864817692.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['cb_person_default_on_file'] = train['cb_person_default_on_file'].replace({'N': 0, 'Y': 1}).astype('category')\n",
      "/tmp/ipykernel_1838/864817692.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  train['rate_to_grade'] = train.groupby('loan_grade')['loan_int_rate'].transform('mean').astype('float64')\n",
      "/tmp/ipykernel_1838/864817692.py:51: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  train['normalized_loan_amount'] = train.groupby('loan_intent')['loan_amnt'].transform(lambda x: (x - x.mean()) / x.std()).astype('float64')\n",
      "/tmp/ipykernel_1838/864817692.py:52: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test['normalized_loan_amount'] = test.groupby('loan_intent')['loan_amnt'].transform(lambda x: (x - x.mean()) / x.std()).astype('float64')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04, 0.13, 0.25, 0.34, 0.15, ..., 0.31, 0.30, 0.20, 0.10, 0.00]\n",
       "Length: 36\n",
       "Categories (36, float64): [0.00, 0.01, 0.02, 0.03, ..., 0.52, 0.53, 0.54, 0.55]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature engineering\n",
    "train[\"loantoincome\"] = ((train[\"loan_amnt\"] / train[\"person_income\"])).astype('Float64')\n",
    "train[\"loan_percent_incometoincome\"] = ((train[\"loan_percent_income\"] / train[\"person_income\"])).astype('Float64')\n",
    "train['person_age_to_person_income'] = (train['person_age'] / train['person_income']).astype(str).astype('Float64')\n",
    "train['person_emp_length_to_person_age'] = (train['person_emp_length'] / train['person_age']).astype('Float64')\n",
    "train['loan_int_rate_to_loan_amnt'] = (train['loan_int_rate'] / train['loan_amnt']).astype('Float64')\n",
    "\n",
    "\n",
    "train['income_to_age'] = train['person_income'] / train['person_age']\n",
    "train['rate_to_loan'] = train['loan_int_rate'] / train['loan_amnt']\n",
    "train['person_home_ownership'] = train['person_home_ownership'].replace({'RENT': 0, 'MORTGAGE': 1, 'OWN': 2, 'OTHER': 3}).astype('category')\n",
    "train['loan_intent'] = train['loan_intent'].replace({'EDUCATION': 0, 'MEDICAL': 1, 'PERSONAL': 2, 'VENTURE': 3, 'DEBTCONSOLIDATION': 4, 'HOMEIMPROVEMENT': 5}).astype('category')\n",
    "train['loan_grade'] = train['loan_grade'].replace({'A':5, 'B':4, 'C':3, 'D':2, 'E':1, 'F':0, 'G':0}).astype('category')\n",
    "train['cb_person_default_on_file'] = train['cb_person_default_on_file'].replace({'N': 0, 'Y': 1}).astype('category')\n",
    "train[\"person_home_ownership_income\"] = pd.Series(pd.factorize((train[\"person_home_ownership\"].astype(str) + train[\"person_income\"].astype(str)).to_numpy())[0]).astype('category')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train['log_income'] = np.log1p(train['person_income']).astype('float64')\n",
    "train['age_credit_history_interaction'] = (train['person_age'] * train['cb_person_cred_hist_length']).astype('float64')\n",
    "train['high_loan_to_income'] = (train['loan_percent_income'] > 0.5).astype('float64')\n",
    "train['is_new_credit_user'] = (train['cb_person_cred_hist_length'] < 2).astype('float64')\n",
    "train['high_interest_rate'] = (train['loan_int_rate'] > train['loan_int_rate'].mean()).astype('float64')\n",
    "train['loan_to_employment'] = (train['loan_amnt'] / (train['person_emp_length'] + 1)).astype('float64')\n",
    "train['rate_to_grade'] = train.groupby('loan_grade')['loan_int_rate'].transform('mean').astype('float64')\n",
    "train['age_category'] = train['person_age'].apply(categorize_age).astype('category')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train['age_to_credit_history'] = (train['person_age'] / (train['cb_person_cred_hist_length'] + 1)).astype('float64')\n",
    "test['age_to_credit_history'] = (test['person_age'] / (test['cb_person_cred_hist_length'] + 1)).astype('float64')\n",
    "\n",
    "\n",
    "\n",
    "train['income_to_loan'] = (train['person_income'] / train['loan_amnt']).astype('float64')\n",
    "test['income_to_loan'] = (test['person_income'] / test['loan_amnt']).astype('float64')\n",
    "train['normalized_loan_amount'] = train.groupby('loan_intent')['loan_amnt'].transform(lambda x: (x - x.mean()) / x.std()).astype('float64')\n",
    "test['normalized_loan_amount'] = test.groupby('loan_intent')['loan_amnt'].transform(lambda x: (x - x.mean()) / x.std()).astype('float64')\n",
    "train['log_loan_amnt'] = np.log1p(train['loan_amnt']).astype('float64')\n",
    "test['log_loan_amnt'] = np.log1p(test['loan_amnt']).astype('float64')\n",
    "\n",
    "\n",
    "train['income_home_mismatch'] = ((train['person_income'] > train['person_income'].quantile(0.8)) & (train['person_home_ownership'] == 0)).astype('float64')\n",
    "test['income_home_mismatch'] = ((test['person_income'] > test['person_income'].quantile(0.8)) & (test['person_home_ownership'] == 0)).astype('float64')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df['default_grade_interaction'] = df['cb_person_default_on_file'].astype(str) + '_' + df['loan_grade'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate and store quantiles in a dictionary\n",
    "def calculate_quantiles(data, column):\n",
    "    quantiles = {}\n",
    "    for q in [0.2, 0.4, 0.6, 0.8]:\n",
    "        quantiles[f'q{int(q*100)}'] = data[column].quantile(q)\n",
    "    return quantiles\n",
    "\n",
    "# Calculate quantiles for the train dataset\n",
    "income_quantiles = calculate_quantiles(train, 'person_income')\n",
    "\n",
    "# Function to categorize income using cached quantiles\n",
    "def categorize_income(income, quantiles):\n",
    "    if income <= quantiles['q20']:\n",
    "        return 0\n",
    "    elif income <= quantiles['q40']:\n",
    "        return 0.1\n",
    "    elif income <= quantiles['q60']:\n",
    "        return 0.2\n",
    "    elif income <= quantiles['q80']:\n",
    "        return 0.3\n",
    "    else:\n",
    "        return 0.4\n",
    "\n",
    "# Apply the categorize_income function to create the income_category column\n",
    "train['income_category'] = train['person_income'].apply(categorize_income, args=(income_quantiles,)).astype('category')\n",
    "test['income_category'] = test['person_income'].apply(categorize_income, args=(income_quantiles,)).astype('category')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train['risk_score'] = train['loan_percent_income'] * train['loan_int_rate'] * (5 - train['loan_grade'].astype('float64'))\n",
    "test['risk_score'] = test['loan_percent_income'] * test['loan_int_rate'] * (5 - test['loan_grade'].astype('float64'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add the transformed columns\n",
    "\n",
    "\n",
    "train['loan_intent_grade'] = ((train['loan_intent'].astype('float64') * 10 + train['loan_grade'].astype('float64'))/100).astype('category')\n",
    "test['loan_intent_grade'] = ((test['loan_intent'].astype('float64') * 10 + test['loan_grade'].astype('float64'))/100).astype('category')\n",
    " \n",
    "\n",
    "# more feature engineering\n",
    "\n",
    "train['loan_intent_grade'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_status\n",
      "0    50295\n",
      "1     8350\n",
      "Name: count, dtype: int64\n",
      "loan_status\n",
      "0    0.857618\n",
      "1    0.142382\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Plot histograms for numerical columns\n",
    "numerical_columns = ['person_age','loantoincome','person_emp_length_to_person_age',\n",
    "                     'loan_int_rate_to_loan_amnt','loan_percent_incometoincome',\n",
    "                     'person_age_to_person_income','person_income', 'loan_amnt',\"person_emp_length\" ,\n",
    "                     'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length','income_to_age','rate_to_loan','log_income',\n",
    "                     'age_credit_history_interaction','high_loan_to_income','is_new_credit_user','high_interest_rate','loan_to_employment','rate_to_grade',\n",
    "                     'risk_score','age_to_credit_history','income_to_loan','normalized_loan_amount','log_loan_amnt','income_home_mismatch'\n",
    "                     ]\n",
    "train[numerical_columns].hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot count plots for categorical columns\n",
    "categorical_columns = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file', 'loan_status',\n",
    "    \"person_home_ownership_income\",\n",
    "    'age_category',\n",
    "    'loan_intent_grade',\n",
    "    'income_category',\n",
    "    # 'intent_grade_interaction','home_ownership_intent'\n",
    "]\n",
    "numerical_features = numerical_columns\n",
    "categorical_features = categorical_columns\n",
    "if  not  DEV:\n",
    "    fig, axes = plt.subplots(math.ceil(len(categorical_columns)/2)+1, 2, figsize=(15, 15))  # Adjusted to 3x2 grid\n",
    "    for ax, col in zip(axes.flatten(), categorical_columns):\n",
    "        sns.countplot(data=train, x=col, ax=ax)\n",
    "        ax.set_title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "features = numerical_columns + categorical_columns \n",
    "categorical_columns.remove('loan_status')\n",
    "features.remove('loan_status')\n",
    "# Define the preprocessing for numerical and categorical features\n",
    "#print how many 'load_status' 0 and 1 and find the ratio\n",
    "print(train['loan_status'].value_counts())\n",
    "print(train['loan_status'].value_counts(normalize=True))\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58645, 37)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_home_ownership</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_intent</th>\n",
       "      <th>loan_grade</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>loantoincome</th>\n",
       "      <th>loan_percent_incometoincome</th>\n",
       "      <th>person_age_to_person_income</th>\n",
       "      <th>person_emp_length_to_person_age</th>\n",
       "      <th>loan_int_rate_to_loan_amnt</th>\n",
       "      <th>income_to_age</th>\n",
       "      <th>rate_to_loan</th>\n",
       "      <th>person_home_ownership_income</th>\n",
       "      <th>log_income</th>\n",
       "      <th>age_credit_history_interaction</th>\n",
       "      <th>high_loan_to_income</th>\n",
       "      <th>is_new_credit_user</th>\n",
       "      <th>high_interest_rate</th>\n",
       "      <th>loan_to_employment</th>\n",
       "      <th>rate_to_grade</th>\n",
       "      <th>age_category</th>\n",
       "      <th>age_to_credit_history</th>\n",
       "      <th>income_to_loan</th>\n",
       "      <th>normalized_loan_amount</th>\n",
       "      <th>log_loan_amnt</th>\n",
       "      <th>income_home_mismatch</th>\n",
       "      <th>income_category</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>loan_intent_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>35000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6000</td>\n",
       "      <td>11.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>945.945946</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0</td>\n",
       "      <td>10.463132</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>11.034733</td>\n",
       "      <td>2</td>\n",
       "      <td>2.466667</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>-0.558667</td>\n",
       "      <td>8.699681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9533</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>56000</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4000</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>2545.454545</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>1</td>\n",
       "      <td>10.933125</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>571.428571</td>\n",
       "      <td>13.510343</td>\n",
       "      <td>0</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>-0.896247</td>\n",
       "      <td>8.294300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.8690</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>28800</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6000</td>\n",
       "      <td>8.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>993.103448</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>2</td>\n",
       "      <td>10.268165</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>666.666667</td>\n",
       "      <td>7.335176</td>\n",
       "      <td>1</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>-0.584460</td>\n",
       "      <td>8.699681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>70000</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12000</td>\n",
       "      <td>11.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>2333.333333</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>3</td>\n",
       "      <td>11.156265</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>11.034733</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>0.492919</td>\n",
       "      <td>9.392745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.8887</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  person_age  person_income person_home_ownership  person_emp_length  \\\n",
       "0   0          37          35000                     0                0.0   \n",
       "1   1          22          56000                     2                6.0   \n",
       "2   2          29          28800                     2                8.0   \n",
       "3   3          30          70000                     0               14.0   \n",
       "\n",
       "  loan_intent loan_grade  loan_amnt  loan_int_rate  loan_percent_income  \\\n",
       "0           0          4       6000          11.49                 0.17   \n",
       "1           1          3       4000          13.35                 0.07   \n",
       "2           2          5       6000           8.90                 0.21   \n",
       "3           3          4      12000          11.11                 0.17   \n",
       "\n",
       "  cb_person_default_on_file  cb_person_cred_hist_length  loan_status  \\\n",
       "0                         0                          14            0   \n",
       "1                         0                           2            0   \n",
       "2                         0                          10            0   \n",
       "3                         0                           5            0   \n",
       "\n",
       "   loantoincome  loan_percent_incometoincome  person_age_to_person_income  \\\n",
       "0      0.171429                     0.000005                     0.001057   \n",
       "1      0.071429                     0.000001                     0.000393   \n",
       "2      0.208333                     0.000007                     0.001007   \n",
       "3      0.171429                     0.000002                     0.000429   \n",
       "\n",
       "   person_emp_length_to_person_age  loan_int_rate_to_loan_amnt  income_to_age  \\\n",
       "0                              0.0                    0.001915     945.945946   \n",
       "1                         0.272727                    0.003337    2545.454545   \n",
       "2                         0.275862                    0.001483     993.103448   \n",
       "3                         0.466667                    0.000926    2333.333333   \n",
       "\n",
       "   rate_to_loan person_home_ownership_income  log_income  \\\n",
       "0      0.001915                            0   10.463132   \n",
       "1      0.003337                            1   10.933125   \n",
       "2      0.001483                            2   10.268165   \n",
       "3      0.000926                            3   11.156265   \n",
       "\n",
       "   age_credit_history_interaction  high_loan_to_income  is_new_credit_user  \\\n",
       "0                           518.0                  0.0                 0.0   \n",
       "1                            44.0                  0.0                 0.0   \n",
       "2                           290.0                  0.0                 0.0   \n",
       "3                           150.0                  0.0                 0.0   \n",
       "\n",
       "   high_interest_rate  loan_to_employment  rate_to_grade age_category  \\\n",
       "0                 1.0         6000.000000      11.034733            2   \n",
       "1                 1.0          571.428571      13.510343            0   \n",
       "2                 0.0          666.666667       7.335176            1   \n",
       "3                 1.0          800.000000      11.034733            1   \n",
       "\n",
       "   age_to_credit_history  income_to_loan  normalized_loan_amount  \\\n",
       "0               2.466667        5.833333               -0.558667   \n",
       "1               7.333333       14.000000               -0.896247   \n",
       "2               2.636364        4.800000               -0.584460   \n",
       "3               5.000000        5.833333                0.492919   \n",
       "\n",
       "   log_loan_amnt  income_home_mismatch income_category  risk_score  \\\n",
       "0       8.699681                   0.0             0.0      1.9533   \n",
       "1       8.294300                   0.0             0.2      1.8690   \n",
       "2       8.699681                   0.0             0.0      0.0000   \n",
       "3       9.392745                   0.0             0.3      1.8887   \n",
       "\n",
       "  loan_intent_grade  \n",
       "0              0.04  \n",
       "1              0.13  \n",
       "2              0.25  \n",
       "3              0.34  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train.head(4))\n",
    "#print all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(train.shape)\n",
    "train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person_age',\n",
       " 'loantoincome',\n",
       " 'person_emp_length_to_person_age',\n",
       " 'loan_int_rate_to_loan_amnt',\n",
       " 'loan_percent_incometoincome',\n",
       " 'person_age_to_person_income',\n",
       " 'person_income',\n",
       " 'loan_amnt',\n",
       " 'person_emp_length',\n",
       " 'loan_int_rate',\n",
       " 'loan_percent_income',\n",
       " 'cb_person_cred_hist_length',\n",
       " 'income_to_age',\n",
       " 'rate_to_loan',\n",
       " 'log_income',\n",
       " 'age_credit_history_interaction',\n",
       " 'high_loan_to_income',\n",
       " 'is_new_credit_user',\n",
       " 'high_interest_rate',\n",
       " 'loan_to_employment',\n",
       " 'rate_to_grade',\n",
       " 'risk_score',\n",
       " 'age_to_credit_history',\n",
       " 'income_to_loan',\n",
       " 'normalized_loan_amount',\n",
       " 'log_loan_amnt',\n",
       " 'income_home_mismatch',\n",
       " 'person_home_ownership',\n",
       " 'loan_intent',\n",
       " 'loan_grade',\n",
       " 'cb_person_default_on_file',\n",
       " 'person_home_ownership_income',\n",
       " 'age_category',\n",
       " 'loan_intent_grade',\n",
       " 'income_category']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((58645, 35), (58645, 1), (39098, 35), (39098,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "target = ['loan_status']  # Replace with the actual target column name\n",
    "\n",
    "# Preprocess the data\n",
    "X = train[features]\n",
    "y = train[target]\n",
    "ids = train['id']\n",
    "testx = test[features]\n",
    "test_ids = test['id']\n",
    "\n",
    "X.shape , y.shape , testx.shape,  test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140748, 35)\n",
      "(35187, 35)\n",
      "(140748, 1)\n",
      "(35187, 1)\n",
      "(140748,)\n",
      "(35187,)\n",
      "(39098, 35)\n",
      "(39098,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(X, y, ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    # ('onehot', OneHotEncoder(handle_unknown='error'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "testx = preprocessor.transform(testx)\n",
    "\n",
    "# Function to add noise\n",
    "def add_noise(data, noise_level=0.01):\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "# Add noise to the numerical features\n",
    "X_train_noisy = X_train.copy()\n",
    "X_test_noisy = X_test.copy()\n",
    "X_train_noisy[:, :len(numerical_features)] = add_noise(X_train[:, :len(numerical_features)])\n",
    "X_test_noisy[:, :len(numerical_features)] = add_noise(X_test[:, :len(numerical_features)])\n",
    "\n",
    "\n",
    "X_train_less_noise = X_train.copy()\n",
    "X_test_less_noise = X_test.copy()\n",
    "X_train_less_noise[:, :len(numerical_features)] = add_noise(X_train[:, :len(numerical_features)],noise_level=0.001)\n",
    "X_test_less_noise[:, :len(numerical_features)] = add_noise(X_test[:, :len(numerical_features)],noise_level=0.001)\n",
    "\n",
    "# Concatenate the original data with the noisy data vertically\n",
    "X_train_combined = np.vstack((X_train, X_train_noisy, X_train_less_noise))\n",
    "X_test_combined = np.vstack((X_test, X_test_noisy, X_test_less_noise))\n",
    "\n",
    "# Concatenate the target variable as well\n",
    "y_train_combined = np.vstack((y_train, y_train, y_train))\n",
    "y_test_combined = np.vstack((y_test, y_test,y_test))\n",
    "\n",
    "# Concatenate the ids as well\n",
    "ids_train_combined = np.hstack((ids_train, ids_train, ids_train))\n",
    "ids_test_combined = np.hstack((ids_test, ids_test, ids_test))\n",
    "\n",
    "# Update the original variables\n",
    "X_train = X_train_combined\n",
    "X_test = X_test_combined\n",
    "y_train = y_train_combined\n",
    "y_test = y_test_combined\n",
    "ids_train = ids_train_combined\n",
    "ids_test = ids_test_combined\n",
    "xult , yult  , idsult= np.vstack((X_train, X_test)), np.vstack((y_train, y_test)) , np.hstack((ids_train, ids_test))\n",
    "print(X_train.shape)  # Should output (46916 + 46916, 26)\n",
    "print(X_test.shape)   # Should output (11729 + 11729, 26)\n",
    "print(y_train.shape)  # Should output (46916 + 46916, 1)\n",
    "print(y_test.shape)   # Should output (11729 + 11729, 1)\n",
    "print(ids_train.shape)  # Should output (46916 + 46916,)\n",
    "print(ids_test.shape)   # Should output (11729 + 11729,)\n",
    "print(testx.shape)   # Should output (11729 + 11729,)\n",
    "print(test_ids.shape)   # Should output (11729 + 11729,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmax(value=False):\n",
    "    #see if fmax.txt exists and if it does then read the value and return it\n",
    "    #if it does not exist then return 0\n",
    "    #if value exists then save it to fmax.txt\n",
    "    if(value):\n",
    "        try:\n",
    "            with open(\"fmax.txt\", \"w\") as f:\n",
    "                f.write(str(value))\n",
    "            return 0    \n",
    "        except:\n",
    "            return 0\n",
    "    try:\n",
    "        with open(\"fmax.txt\", \"r\") as f:\n",
    "            return float(f.read())\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ensemble:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        \n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            pred = model.predict(X_test)\n",
    "            predictions.append(pred)\n",
    "            count_greater_than_0_5 = (pred > model.THRESHOLD).sum()\n",
    "            count_less_than_or_equal_0_5 = (pred <= model.THRESHOLD).sum()\n",
    "            print(f'Percentage of predictions greater than {model.THRESHOLD}: {count_greater_than_0_5 / len(pred) * 100:.2f}%')\n",
    "            passc = count_greater_than_0_5 / len(pred) * 100\n",
    "            if passc < 5:\n",
    "                predictions.pop()\n",
    "                continue\n",
    "            print(f'Percentage of predictions less than or equal to {model.THRESHOLD}: {count_less_than_or_equal_0_5 / len(pred) * 100:.2f}%')\n",
    "        \n",
    "        # Stack predictions to form a 2D array\n",
    "        stacked_predictions = np.hstack(predictions)\n",
    "        \n",
    "        # Average the predictions across models\n",
    "        y_pred = np.mean(stacked_predictions, axis=1)\n",
    "        \n",
    "        # Apply threshold\n",
    "        # y_pred = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        # Flatten the predictions to form a 1D array\n",
    "        y_pred = y_pred.flatten()\n",
    "        \n",
    "        # Assuming test_ids is defined elsewhere in your code\n",
    "        ids = test_ids\n",
    "        predictions_df = pd.DataFrame({'id': ids, 'loan_status': y_pred})\n",
    "        return predictions_df\n",
    "    \n",
    "    def save(self, testx, path=\"ftt.csv\"):\n",
    "        df = self.predict(testx)\n",
    "        df.to_csv(path, index=False)\n",
    "        csvfile =  pd.read_csv(path)\n",
    "        csvfile\n",
    "        \n",
    "    def rmamodel(self):\n",
    "        self.models = []\n",
    "\n",
    "ens = ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ktrain( model , xult , yult,splits=5,epochs=15,batch_size=32,random_state=42):\n",
    "    \n",
    "    if splits==1:\n",
    "        class temp:\n",
    "            def split(self, X):\n",
    "                n_samples = len(X)\n",
    "                indices = list(range(n_samples))\n",
    "                yield indices, indices  # Use the same indices for train and test\n",
    "\n",
    "        kf = temp()\n",
    "    else:\n",
    "        kf = KFold(n_splits=splits, shuffle=True, random_state=random_state)\n",
    "        obj = kf \n",
    "    losses, aucs, precisions, recalls, f1s, roc_aucs = [], [], [], [], [], []\n",
    "    iter = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        xt, xv = xult[train_index], xult[test_index]\n",
    "        yt, yv = yult[train_index], yult[test_index]\n",
    "        \n",
    "        # Initialize and compile the model\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(xt, yt, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        results = model.evaluate(xv, yv)\n",
    "        loss, auc, precision, recall = results[0], results[1], results[2], results[3]\n",
    "        \n",
    "        # Predict probabilities for the test set\n",
    "        y_pred_prob = model.predict(xult)\n",
    "        y_pred = (y_pred_prob > model.THRESHOLD).astype(int)\n",
    "        \n",
    "        # Calculate F1 score and ROC AUC score\n",
    "        f1 = f1_score(yult, y_pred)\n",
    "        #count which iteration is this\n",
    "        count = train_index[0]\n",
    "        roc_auc = roc_auc_score(yult, y_pred_prob)\n",
    "        if f1 > fmax():\n",
    "            model.f1max(iter,epochs,splits)\n",
    "            fmax(f1)\n",
    "            model.save('best.keras')\n",
    "            print(colored(f'F1 Score improved to {f1}. Saving model...', 'green','on_red'))\n",
    "        # Store metrics\n",
    "        losses.append(loss)\n",
    "        aucs.append(auc)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        roc_aucs.append(roc_auc)\n",
    "        iter += 1\n",
    "    print('losses: ', losses)\n",
    "    print('aucs: ', aucs)\n",
    "    print('precisions: ', precisions)\n",
    "    print('recalls: ', recalls)\n",
    "    print('f1s: ', f1s)\n",
    "    print('roc_aucs: ', roc_aucs)\n",
    "    print(f'Average Loss: {sum(losses) / len(losses)}')\n",
    "    print(f'Average AUC: {sum(aucs) / len(aucs)}')\n",
    "    print(f'Average Precision: {sum(precisions) / len(precisions)}')\n",
    "    print(f'Average Recall: {sum(recalls) / len(recalls)}')\n",
    "    print(f'Average F1 Score: {sum(f1s) / len(f1s)}')\n",
    "    print(f'Average ROC AUC Score: {sum(roc_aucs) / len(roc_aucs)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[9924  102]\n",
      " [ 434 1269]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10033    61]\n",
      " [  245  1390]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10046    34]\n",
      " [  196  1453]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10089    14]\n",
      " [  102  1524]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[9979    0]\n",
      " [  61 1689]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9637699175332626, 0.9906487579444355, 0.9961786003061018, 0.9994885398720363, 0.9998267218301291]\n",
      "precisions:  [0.925601750547046, 0.957960027567195, 0.9771351714862139, 0.9908972691807543, 1.0]\n",
      "recalls:  [0.7451556077510276, 0.8501529051987767, 0.8811400848999393, 0.9372693726937269, 0.9651428571428572]\n",
      "f1s:  [0.8541034098604163, 0.889936436158867, 0.9107387501317314, 0.9238955781390205, 0.931998088549999]\n",
      "roc_aucs:  [0.8863126118773488, 0.9131695432080466, 0.9286196371547589, 0.9394239714228965, 0.9455240448618619]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9899825074971931\n",
      "Average Precision: 0.9703188437562418\n",
      "Average Recall: 0.8757721655372656\n",
      "Average F1 Score: 0.902134452568007\n",
      "Average ROC AUC Score: 0.9226099617049826\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10046     9]\n",
      " [   57  1617]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10051     4]\n",
      " [   18  1656]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10063     2]\n",
      " [    4  1660]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[9989    0]\n",
      " [   2 1738]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10117     1]\n",
      " [    0  1611]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9998100649533895, 0.9999713641875302, 0.9999975519698882, 0.9999998849308953, 1.0]\n",
      "precisions:  [0.9944649446494465, 0.9975903614457832, 0.9987966305655837, 1.0, 0.999379652605459]\n",
      "recalls:  [0.9659498207885304, 0.989247311827957, 0.9975961538461539, 0.9988505747126437, 1.0]\n",
      "f1s:  [0.9367780399950316, 0.9422557569010658, 0.9450644306060793, 0.9462599556613843, 0.9483804838048381]\n",
      "roc_aucs:  [0.949529083794832, 0.9541461802510895, 0.9568841797466802, 0.9580152767335395, 0.9597116513567954]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9999557732083406\n",
      "Average Precision: 0.9980463178532546\n",
      "Average Recall: 0.9903287722350569\n",
      "Average F1 Score: 0.9437477333936798\n",
      "Average ROC AUC Score: 0.9556572743765873\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10025     0]\n",
      " [    0  1704]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10065     0]\n",
      " [    1  1663]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10030     0]\n",
      " [    0  1699]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10067     0]\n",
      " [    1  1661]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10095     0]\n",
      " [    0  1634]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "precisions:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "recalls:  [1.0, 0.9993990384615384, 1.0, 0.9993983152827918, 1.0]\n",
      "f1s:  [0.9491011834077229, 0.9497083205403746, 0.9502945508100147, 0.950945094509451, 0.9512125301598986]\n",
      "roc_aucs:  [0.9606000683789993, 0.9610457403118011, 0.9616346013609522, 0.9619736905550664, 0.9622964430022548]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 1.0\n",
      "Average Precision: 1.0\n",
      "Average Recall: 0.999759470748866\n",
      "Average F1 Score: 0.9502523358854923\n",
      "Average ROC AUC Score: 0.9615101087218149\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10055     0]\n",
      " [    0  1674]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10063     0]\n",
      " [    0  1666]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10030     0]\n",
      " [    0  1699]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10054     0]\n",
      " [    0  1675]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10080     0]\n",
      " [    0  1649]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "precisions:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "recalls:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "f1s:  [0.9512883435582822, 0.9520522769042271, 0.952071718842533, 0.9520287663956196, 0.9525501245457144]\n",
      "roc_aucs:  [0.9622930518325115, 0.9632713279078585, 0.963274641689878, 0.9630715723340079, 0.9636171994414591]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 1.0\n",
      "Average Precision: 1.0\n",
      "Average Recall: 1.0\n",
      "Average F1 Score: 0.9519982460492754\n",
      "Average ROC AUC Score: 0.963105558641143\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10054     0]\n",
      " [    0  1675]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10078     0]\n",
      " [    0  1651]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10070     0]\n",
      " [    0  1659]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10013     0]\n",
      " [    0  1716]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10067     0]\n",
      " [    0  1662]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "precisions:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "recalls:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "f1s:  [0.9524237350430841, 0.9527597601272794, 0.9531195787497194, 0.952841152364059, 0.9531689423272875]\n",
      "roc_aucs:  [0.9635140852602381, 0.9641098060365997, 0.9640895364058641, 0.9643195029565087, 0.964685489345397]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 1.0\n",
      "Average Precision: 1.0\n",
      "Average Recall: 1.0\n",
      "Average F1 Score: 0.9528626337222859\n",
      "Average ROC AUC Score: 0.9641436840009214\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10089     0]\n",
      " [    0  1640]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10098     0]\n",
      " [    0  1631]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10034     0]\n",
      " [    0  1695]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10013     0]\n",
      " [    0  1716]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10048     0]\n",
      " [    0  1681]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "precisions:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "recalls:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "f1s:  [0.9533598450086673, 0.9533433845401573, 0.9530896413936515, 0.9534987977340343, 0.9534836316197154]\n",
      "roc_aucs:  [0.9645221853737599, 0.9648784625797615, 0.9644924387233089, 0.9649049728359168, 0.9647718024533496]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 1.0\n",
      "Average Precision: 1.0\n",
      "Average Recall: 1.0\n",
      "Average F1 Score: 0.9533550600592451\n",
      "Average ROC AUC Score: 0.9647139723932193\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10092     0]\n",
      " [    0  1637]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10075     0]\n",
      " [    1  1653]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[9999    0]\n",
      " [   0 1730]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10064     0]\n",
      " [    0  1665]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10052     0]\n",
      " [    0  1677]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "precisions:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "recalls:  [1.0, 0.9993954050785974, 1.0, 1.0, 1.0]\n",
      "f1s:  [0.9536234838446641, 0.9530198605277109, 0.9534499969442011, 0.9539740429086612, 0.9539194915254238]\n",
      "roc_aucs:  [0.9648282915231272, 0.9643826195903252, 0.964978262978963, 0.965230884289677, 0.9652542355392605]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 1.0\n",
      "Average Precision: 1.0\n",
      "Average Recall: 0.9998790810157194\n",
      "Average F1 Score: 0.9535973751501322\n",
      "Average ROC AUC Score: 0.9649348587842705\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10025     0]\n",
      " [    0  1704]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "XGBClassifier(base_score='1.9275978E-1', booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n",
      "None\n",
      "iter: 0, epochs: 10, splits: 5\n",
      "{}\n",
      "\u001b[41m\u001b[32mF1 Score improved to 0.9544511773557282. Saving model...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [11:59:00] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10054     0]\n",
      " [    0  1675]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class XGBoostClassifierModel:\n",
    "    def __init__(self, eval_metric='logloss', model_path='xgb_model.json', **kwargs):\n",
    "        self.model = xgb.XGBClassifier(objective='binary:logistic', eval_metric=eval_metric, **kwargs)\n",
    "        self.model_path = model_path\n",
    "        self.THRESHOLD = 0.5\n",
    "        self.load_model()\n",
    "        self.f1 = 0 # Placeholder for F1 score\n",
    "\n",
    "    def fit(self, X_train, y_train, verbose=0, **kwargs):\n",
    "        self.model.fit(X_train, y_train, verbose=verbose, xgb_model=self.model_path if self.model_exists() else None)\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(colored('predicting using xgboost model', 'green'))\n",
    "        return self.model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        y_pred_proba = self.predict_proba(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        # Assuming loss is not directly available, using log loss as a proxy\n",
    "        loss = -1  # Placeholder, as XGBoost does not directly provide loss in this context\n",
    "        return [loss, auc, precision, recall]\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model)\n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "    def save_model(self):\n",
    "        self.model.save_model(self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_exists():\n",
    "            self.model.load_model(self.model_path)\n",
    "\n",
    "    def model_exists(self):\n",
    "        try:\n",
    "            with open(self.model_path, 'r'):\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "    def save(self, path):\n",
    "        self.model.save_model(path)\n",
    "\n",
    "# Example usage\n",
    "# Assuming xult and yult are your feature matrix and target vector\n",
    "# Split the data into training and test sets\n",
    "\n",
    "for i in range(10):\n",
    "    if i%2==0:\n",
    "        xgb_model = XGBoostClassifierModel(model_path=f'xgb_model.json')\n",
    "    else:\n",
    "        xgb_model = XGBoostClassifierModel(eval_metric='auc', model_path=f'xgb_model.json')\n",
    "    ktrain(xgb_model, xult, yult, epochs=10, batch_size=32, splits=5, random_state=i)\n",
    "    ens.add_model(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - auc: 0.7336 - loss: 0.4177 - precision: 0.5270 - recall: 0.1454 - val_auc: 0.8854 - val_loss: 0.2737 - val_precision: 0.7754 - val_recall: 0.3834\n",
      "Epoch 2/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.8713 - loss: 0.2733 - precision: 0.7422 - recall: 0.4298 - val_auc: 0.9010 - val_loss: 0.2435 - val_precision: 0.7937 - val_recall: 0.5147\n",
      "Epoch 3/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.8933 - loss: 0.2475 - precision: 0.7988 - recall: 0.4868 - val_auc: 0.9017 - val_loss: 0.2441 - val_precision: 0.8660 - val_recall: 0.4244\n",
      "Epoch 4/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.8992 - loss: 0.2365 - precision: 0.8006 - recall: 0.5088 - val_auc: 0.8894 - val_loss: 0.2514 - val_precision: 0.8767 - val_recall: 0.4303\n",
      "Epoch 5/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.8986 - loss: 0.2322 - precision: 0.8052 - recall: 0.5393 - val_auc: 0.9112 - val_loss: 0.2257 - val_precision: 0.7846 - val_recall: 0.5581\n",
      "Epoch 6/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9117 - loss: 0.2192 - precision: 0.8169 - recall: 0.5492 - val_auc: 0.9044 - val_loss: 0.2524 - val_precision: 0.8571 - val_recall: 0.3941\n",
      "Epoch 7/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9067 - loss: 0.2281 - precision: 0.8045 - recall: 0.5431 - val_auc: 0.9100 - val_loss: 0.2390 - val_precision: 0.7269 - val_recall: 0.6071\n",
      "Epoch 8/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9094 - loss: 0.2180 - precision: 0.8292 - recall: 0.5598 - val_auc: 0.9096 - val_loss: 0.2340 - val_precision: 0.9293 - val_recall: 0.4080\n",
      "Epoch 9/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9129 - loss: 0.2223 - precision: 0.8155 - recall: 0.5569 - val_auc: 0.9172 - val_loss: 0.2162 - val_precision: 0.8608 - val_recall: 0.5366\n",
      "Epoch 10/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9150 - loss: 0.2164 - precision: 0.8120 - recall: 0.5694 - val_auc: 0.9149 - val_loss: 0.2202 - val_precision: 0.7428 - val_recall: 0.6485\n",
      "Epoch 11/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9171 - loss: 0.2178 - precision: 0.8048 - recall: 0.5835 - val_auc: 0.9056 - val_loss: 0.2405 - val_precision: 0.8681 - val_recall: 0.4900\n",
      "Epoch 12/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9007 - loss: 0.2372 - precision: 0.7831 - recall: 0.5370 - val_auc: 0.9212 - val_loss: 0.2146 - val_precision: 0.8090 - val_recall: 0.6055\n",
      "Epoch 13/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9170 - loss: 0.2162 - precision: 0.8121 - recall: 0.5757 - val_auc: 0.9192 - val_loss: 0.2197 - val_precision: 0.8497 - val_recall: 0.5243\n",
      "Epoch 14/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9170 - loss: 0.2122 - precision: 0.8249 - recall: 0.6015 - val_auc: 0.9238 - val_loss: 0.2082 - val_precision: 0.8322 - val_recall: 0.5924\n",
      "Epoch 15/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc: 0.9198 - loss: 0.2136 - precision: 0.8149 - recall: 0.5961 - val_auc: 0.9245 - val_loss: 0.2050 - val_precision: 0.8763 - val_recall: 0.5697\n",
      "\u001b[1m1833/1833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc: 0.9263 - loss: 0.2004 - precision: 0.8757 - recall: 0.5815\n",
      "\u001b[32mPredicting with encoding_dim 512...\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 880us/step\n",
      "losses:  [0.20190709829330444]\n",
      "aucs:  [0.9262207746505737]\n",
      "precisions:  [0.8764674067497253]\n",
      "recalls:  [0.580294132232666]\n",
      "f1s:  [0.6986439216062101]\n",
      "roc_aucs:  [0.9270239796775868]\n",
      "Average Loss: 0.20190709829330444\n",
      "Average AUC: 0.9262207746505737\n",
      "Average Precision: 0.8764674067497253\n",
      "Average Recall: 0.580294132232666\n",
      "Average F1 Score: 0.6986439216062101\n",
      "Average ROC AUC Score: 0.9270239796775868\n",
      "Epoch 1/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - auc_1: 0.6970 - loss: 0.4096 - precision_1: 0.4170 - recall_1: 0.0916 - val_auc_1: 0.8860 - val_loss: 0.2627 - val_precision_1: 0.7830 - val_recall_1: 0.4323\n",
      "Epoch 2/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.8745 - loss: 0.2740 - precision_1: 0.7591 - recall_1: 0.4050 - val_auc_1: 0.9009 - val_loss: 0.2527 - val_precision_1: 0.8129 - val_recall_1: 0.4323\n",
      "Epoch 3/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.8889 - loss: 0.2582 - precision_1: 0.7579 - recall_1: 0.4423 - val_auc_1: 0.9072 - val_loss: 0.2404 - val_precision_1: 0.8689 - val_recall_1: 0.4144\n",
      "Epoch 4/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - auc_1: 0.8942 - loss: 0.2494 - precision_1: 0.8030 - recall_1: 0.4558 - val_auc_1: 0.9071 - val_loss: 0.2401 - val_precision_1: 0.7810 - val_recall_1: 0.5494\n",
      "Epoch 5/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.9048 - loss: 0.2361 - precision_1: 0.7844 - recall_1: 0.5382 - val_auc_1: 0.9084 - val_loss: 0.2417 - val_precision_1: 0.6167 - val_recall_1: 0.6668\n",
      "Epoch 6/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - auc_1: 0.9066 - loss: 0.2299 - precision_1: 0.7889 - recall_1: 0.5482 - val_auc_1: 0.9173 - val_loss: 0.2165 - val_precision_1: 0.8058 - val_recall_1: 0.5832\n",
      "Epoch 7/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.9079 - loss: 0.2284 - precision_1: 0.8089 - recall_1: 0.5476 - val_auc_1: 0.9060 - val_loss: 0.2446 - val_precision_1: 0.7026 - val_recall_1: 0.6206\n",
      "Epoch 8/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.9059 - loss: 0.2295 - precision_1: 0.8150 - recall_1: 0.5578 - val_auc_1: 0.9189 - val_loss: 0.2155 - val_precision_1: 0.8494 - val_recall_1: 0.5207\n",
      "Epoch 9/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.9109 - loss: 0.2231 - precision_1: 0.8290 - recall_1: 0.5433 - val_auc_1: 0.9201 - val_loss: 0.2122 - val_precision_1: 0.7754 - val_recall_1: 0.6541\n",
      "Epoch 10/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - auc_1: 0.9117 - loss: 0.2206 - precision_1: 0.7782 - recall_1: 0.5763 - val_auc_1: 0.9167 - val_loss: 0.2273 - val_precision_1: 0.8987 - val_recall_1: 0.4522\n",
      "Epoch 11/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - auc_1: 0.9137 - loss: 0.2179 - precision_1: 0.8126 - recall_1: 0.5537 - val_auc_1: 0.9180 - val_loss: 0.2165 - val_precision_1: 0.8166 - val_recall_1: 0.5832\n",
      "Epoch 12/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.9139 - loss: 0.2143 - precision_1: 0.8119 - recall_1: 0.5652 - val_auc_1: 0.9215 - val_loss: 0.2102 - val_precision_1: 0.8297 - val_recall_1: 0.5856\n",
      "Epoch 13/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.9148 - loss: 0.2174 - precision_1: 0.8164 - recall_1: 0.5825 - val_auc_1: 0.9207 - val_loss: 0.2239 - val_precision_1: 0.7190 - val_recall_1: 0.6915\n",
      "Epoch 14/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.9147 - loss: 0.2177 - precision_1: 0.8124 - recall_1: 0.5610 - val_auc_1: 0.9227 - val_loss: 0.2073 - val_precision_1: 0.8016 - val_recall_1: 0.6290\n",
      "Epoch 15/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_1: 0.9226 - loss: 0.2083 - precision_1: 0.8148 - recall_1: 0.5829 - val_auc_1: 0.9243 - val_loss: 0.2091 - val_precision_1: 0.7998 - val_recall_1: 0.6505\n",
      "\u001b[1m1833/1833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 916us/step - auc_1: 0.9259 - loss: 0.2050 - precision_1: 0.8013 - recall_1: 0.6557\n",
      "\u001b[32mPredicting with encoding_dim 256...\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 744us/step\n",
      "losses:  [0.20613528788089752]\n",
      "aucs:  [0.9259380102157593]\n",
      "precisions:  [0.8022218942642212]\n",
      "recalls:  [0.6562238335609436]\n",
      "f1s:  [0.7214069823470147]\n",
      "roc_aucs:  [0.9266337920970201]\n",
      "Average Loss: 0.20613528788089752\n",
      "Average AUC: 0.9259380102157593\n",
      "Average Precision: 0.8022218942642212\n",
      "Average Recall: 0.6562238335609436\n",
      "Average F1 Score: 0.7214069823470147\n",
      "Average ROC AUC Score: 0.9266337920970201\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from colorama import init\n",
    "from termcolor import colored\n",
    "# Define the L2 regularizers\n",
    "\n",
    "\n",
    "k = .0001\n",
    "kernel_regularizer = l2(0.0001*k)\n",
    "bias_regularizer = l2(0.00001*k)\n",
    "\n",
    "# Define the CNN model with different regularization strengths for kernel and bias\n",
    "class AutoencoderModel:\n",
    "    def __init__(self, input_dim, encoding_dim=512):\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.autoencoder = self.build_model()\n",
    "        self.THRESHOLD = 0.5\n",
    "    def build_model(self):\n",
    "        # Encoder\n",
    "        input_layer = Input(shape=(self.input_dim,))\n",
    "        encoded = Dense(self.encoding_dim, activation='relu')(input_layer)\n",
    "        encoded = Dense(self.encoding_dim // 2, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        encoded = Dense(self.encoding_dim // 4, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        encoded = Dense(self.encoding_dim // 8, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        encoded = Dense(self.encoding_dim // 16, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        encoded = Dense(self.encoding_dim // 32, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "    \n",
    "        # Decoder\n",
    "        decoded = Dense(self.encoding_dim // 32, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        decoded = Dense(self.encoding_dim // 16, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        decoded = Dense(self.encoding_dim // 8, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        decoded = Dense(self.encoding_dim // 4, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(decoded)\n",
    "        decoded = Dense(self.encoding_dim // 2, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(decoded)\n",
    "        decoded = Dense(1, activation='sigmoid')(decoded)\n",
    "    \n",
    "        # Autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "        # Compile the model with Binary Crossentropy loss\n",
    "        autoencoder.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=[AUC(), Precision(), Recall()])    \n",
    "        return autoencoder\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=10, verbose=0, batch_size=32, validation_split=0.3, validation_data=None):\n",
    "        if validation_data is not None:\n",
    "            self.autoencoder.fit(X_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, validation_data=validation_data, validation_split=validation_split)\n",
    "            return \n",
    "        self.autoencoder.fit(X_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, validation_split=validation_split)\n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "    def predict(self, X_test):\n",
    "        print(colored(f'Predicting with encoding_dim {self.encoding_dim}...', 'green'))\n",
    "        return self.autoencoder.predict(X_test)\n",
    "    \n",
    "    def summary(self):\n",
    "        self.autoencoder.summary()\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        return self.autoencoder.evaluate(X_test, y_test)\n",
    "    def save(self, path):\n",
    "        self.autoencoder.save(path)\n",
    "\n",
    "dim = [512, 256, 128, 64, 256]\n",
    "for i in range(2):\n",
    "    automodel = AutoencoderModel(X_train.shape[1], encoding_dim=dim[i])\n",
    "    ktrain(automodel, xult, yult, splits=1, epochs=15, batch_size=32, random_state=i)\n",
    "    ens.add_model(automodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - auc_2: 0.6922 - loss: 0.4486 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_auc_2: 0.8374 - val_loss: 0.3090 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.8242 - loss: 0.3182 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_auc_2: 0.8222 - val_loss: 0.3203 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.8442 - loss: 0.3000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_auc_2: 0.8863 - val_loss: 0.2895 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.8719 - loss: 0.2883 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_auc_2: 0.9041 - val_loss: 0.2619 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.8915 - loss: 0.2637 - precision_2: 0.3182 - recall_2: 0.0618 - val_auc_2: 0.8986 - val_loss: 0.2737 - val_precision_2: 0.6798 - val_recall_2: 0.6576\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.8917 - loss: 0.2627 - precision_2: 0.6817 - recall_2: 0.5659 - val_auc_2: 0.9096 - val_loss: 0.2448 - val_precision_2: 0.8031 - val_recall_2: 0.5301\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.8892 - loss: 0.2639 - precision_2: 0.6722 - recall_2: 0.5750 - val_auc_2: 0.9087 - val_loss: 0.2457 - val_precision_2: 0.7642 - val_recall_2: 0.5860\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9043 - loss: 0.2435 - precision_2: 0.7300 - recall_2: 0.5821 - val_auc_2: 0.8608 - val_loss: 0.3179 - val_precision_2: 0.4356 - val_recall_2: 0.7792\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.8963 - loss: 0.2476 - precision_2: 0.7035 - recall_2: 0.5908 - val_auc_2: 0.9063 - val_loss: 0.2426 - val_precision_2: 0.7221 - val_recall_2: 0.6265\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_2: 0.9071 - loss: 0.2322 - precision_2: 0.7588 - recall_2: 0.5977 - val_auc_2: 0.9120 - val_loss: 0.2396 - val_precision_2: 0.8289 - val_recall_2: 0.5217\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_2: 0.9132 - loss: 0.2313 - precision_2: 0.7869 - recall_2: 0.6010 - val_auc_2: 0.9045 - val_loss: 0.2486 - val_precision_2: 0.7090 - val_recall_2: 0.6285\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9094 - loss: 0.2345 - precision_2: 0.7570 - recall_2: 0.5875 - val_auc_2: 0.9150 - val_loss: 0.2256 - val_precision_2: 0.7730 - val_recall_2: 0.6344\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9087 - loss: 0.2285 - precision_2: 0.7730 - recall_2: 0.5937 - val_auc_2: 0.9129 - val_loss: 0.2364 - val_precision_2: 0.7567 - val_recall_2: 0.6117\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9085 - loss: 0.2298 - precision_2: 0.7703 - recall_2: 0.5754 - val_auc_2: 0.9166 - val_loss: 0.2243 - val_precision_2: 0.8142 - val_recall_2: 0.5781\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9110 - loss: 0.2221 - precision_2: 0.7924 - recall_2: 0.5958 - val_auc_2: 0.9159 - val_loss: 0.2248 - val_precision_2: 0.7829 - val_recall_2: 0.6057\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - auc_2: 0.9177 - loss: 0.2228 - precision_2: 0.7855 - recall_2: 0.6135\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 692us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9146 - loss: 0.2244 - precision_2: 0.7980 - recall_2: 0.5864 - val_auc_2: 0.9112 - val_loss: 0.2271 - val_precision_2: 0.8011 - val_recall_2: 0.5743\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9096 - loss: 0.2288 - precision_2: 0.7829 - recall_2: 0.5954 - val_auc_2: 0.9148 - val_loss: 0.2244 - val_precision_2: 0.7847 - val_recall_2: 0.6155\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9198 - loss: 0.2158 - precision_2: 0.7970 - recall_2: 0.5982 - val_auc_2: 0.9177 - val_loss: 0.2175 - val_precision_2: 0.8144 - val_recall_2: 0.5996\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9140 - loss: 0.2202 - precision_2: 0.8032 - recall_2: 0.6020 - val_auc_2: 0.9159 - val_loss: 0.2207 - val_precision_2: 0.8175 - val_recall_2: 0.5564\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9174 - loss: 0.2167 - precision_2: 0.8044 - recall_2: 0.5889 - val_auc_2: 0.9175 - val_loss: 0.2203 - val_precision_2: 0.8595 - val_recall_2: 0.5410\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9195 - loss: 0.2165 - precision_2: 0.8049 - recall_2: 0.6052 - val_auc_2: 0.9146 - val_loss: 0.2234 - val_precision_2: 0.7751 - val_recall_2: 0.6130\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9212 - loss: 0.2157 - precision_2: 0.8171 - recall_2: 0.5876 - val_auc_2: 0.9177 - val_loss: 0.2157 - val_precision_2: 0.8330 - val_recall_2: 0.5772\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9180 - loss: 0.2130 - precision_2: 0.8193 - recall_2: 0.6033 - val_auc_2: 0.9113 - val_loss: 0.2319 - val_precision_2: 0.8696 - val_recall_2: 0.5067\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9199 - loss: 0.2160 - precision_2: 0.8137 - recall_2: 0.6022 - val_auc_2: 0.9177 - val_loss: 0.2163 - val_precision_2: 0.8391 - val_recall_2: 0.5753\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9180 - loss: 0.2139 - precision_2: 0.8072 - recall_2: 0.5969 - val_auc_2: 0.9209 - val_loss: 0.2097 - val_precision_2: 0.8382 - val_recall_2: 0.6021\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9213 - loss: 0.2106 - precision_2: 0.8317 - recall_2: 0.6032 - val_auc_2: 0.9185 - val_loss: 0.2266 - val_precision_2: 0.7266 - val_recall_2: 0.6905\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9210 - loss: 0.2126 - precision_2: 0.8226 - recall_2: 0.6153 - val_auc_2: 0.9185 - val_loss: 0.2184 - val_precision_2: 0.8339 - val_recall_2: 0.5688\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9181 - loss: 0.2118 - precision_2: 0.8372 - recall_2: 0.5957 - val_auc_2: 0.9186 - val_loss: 0.2139 - val_precision_2: 0.8568 - val_recall_2: 0.5708\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9249 - loss: 0.2026 - precision_2: 0.8358 - recall_2: 0.6235 - val_auc_2: 0.9206 - val_loss: 0.2112 - val_precision_2: 0.8187 - val_recall_2: 0.6100\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9232 - loss: 0.2033 - precision_2: 0.8312 - recall_2: 0.6216 - val_auc_2: 0.9192 - val_loss: 0.2121 - val_precision_2: 0.8501 - val_recall_2: 0.5777\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - auc_2: 0.9217 - loss: 0.2053 - precision_2: 0.8517 - recall_2: 0.5812\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 651us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9223 - loss: 0.2068 - precision_2: 0.8278 - recall_2: 0.6039 - val_auc_2: 0.9261 - val_loss: 0.2084 - val_precision_2: 0.8593 - val_recall_2: 0.5512\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9229 - loss: 0.2108 - precision_2: 0.8357 - recall_2: 0.5995 - val_auc_2: 0.9218 - val_loss: 0.2159 - val_precision_2: 0.8604 - val_recall_2: 0.5316\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9220 - loss: 0.2082 - precision_2: 0.8239 - recall_2: 0.6016 - val_auc_2: 0.9255 - val_loss: 0.2036 - val_precision_2: 0.8563 - val_recall_2: 0.5737\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9213 - loss: 0.2074 - precision_2: 0.8303 - recall_2: 0.6111 - val_auc_2: 0.9287 - val_loss: 0.1989 - val_precision_2: 0.8513 - val_recall_2: 0.6058\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9268 - loss: 0.2026 - precision_2: 0.8289 - recall_2: 0.6307 - val_auc_2: 0.9285 - val_loss: 0.1991 - val_precision_2: 0.8515 - val_recall_2: 0.6008\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9272 - loss: 0.1989 - precision_2: 0.8445 - recall_2: 0.6360 - val_auc_2: 0.9286 - val_loss: 0.1982 - val_precision_2: 0.8526 - val_recall_2: 0.6003\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9242 - loss: 0.2076 - precision_2: 0.8305 - recall_2: 0.6225 - val_auc_2: 0.9245 - val_loss: 0.2113 - val_precision_2: 0.8658 - val_recall_2: 0.5211\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9281 - loss: 0.1992 - precision_2: 0.8425 - recall_2: 0.6323 - val_auc_2: 0.9258 - val_loss: 0.2060 - val_precision_2: 0.8276 - val_recall_2: 0.5993\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9252 - loss: 0.2046 - precision_2: 0.8297 - recall_2: 0.6256 - val_auc_2: 0.9245 - val_loss: 0.2097 - val_precision_2: 0.7792 - val_recall_2: 0.6354\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9231 - loss: 0.2044 - precision_2: 0.8269 - recall_2: 0.6262 - val_auc_2: 0.9284 - val_loss: 0.2063 - val_precision_2: 0.7703 - val_recall_2: 0.6861\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9276 - loss: 0.2004 - precision_2: 0.8328 - recall_2: 0.6295 - val_auc_2: 0.9292 - val_loss: 0.1976 - val_precision_2: 0.8281 - val_recall_2: 0.6259\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9263 - loss: 0.2014 - precision_2: 0.8370 - recall_2: 0.6264 - val_auc_2: 0.9295 - val_loss: 0.1970 - val_precision_2: 0.8512 - val_recall_2: 0.6138\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9267 - loss: 0.2021 - precision_2: 0.8285 - recall_2: 0.6184 - val_auc_2: 0.9292 - val_loss: 0.1982 - val_precision_2: 0.8415 - val_recall_2: 0.6179\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9238 - loss: 0.2041 - precision_2: 0.8335 - recall_2: 0.6303 - val_auc_2: 0.9264 - val_loss: 0.2109 - val_precision_2: 0.8246 - val_recall_2: 0.6128\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9274 - loss: 0.1974 - precision_2: 0.8440 - recall_2: 0.6284 - val_auc_2: 0.9252 - val_loss: 0.2103 - val_precision_2: 0.7711 - val_recall_2: 0.6841\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - auc_2: 0.9174 - loss: 0.2173 - precision_2: 0.7646 - recall_2: 0.6699\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 651us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9261 - loss: 0.2012 - precision_2: 0.8412 - recall_2: 0.6298 - val_auc_2: 0.9234 - val_loss: 0.2091 - val_precision_2: 0.7824 - val_recall_2: 0.6706\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9244 - loss: 0.2015 - precision_2: 0.8278 - recall_2: 0.6139 - val_auc_2: 0.9272 - val_loss: 0.2015 - val_precision_2: 0.8804 - val_recall_2: 0.5817\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9247 - loss: 0.2035 - precision_2: 0.8480 - recall_2: 0.6189 - val_auc_2: 0.9239 - val_loss: 0.2088 - val_precision_2: 0.8601 - val_recall_2: 0.5523\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9286 - loss: 0.2023 - precision_2: 0.8470 - recall_2: 0.6151 - val_auc_2: 0.9250 - val_loss: 0.2093 - val_precision_2: 0.8787 - val_recall_2: 0.5547\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9250 - loss: 0.2033 - precision_2: 0.8441 - recall_2: 0.6244 - val_auc_2: 0.9235 - val_loss: 0.2116 - val_precision_2: 0.8609 - val_recall_2: 0.5744\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9279 - loss: 0.1973 - precision_2: 0.8491 - recall_2: 0.6252 - val_auc_2: 0.9277 - val_loss: 0.1999 - val_precision_2: 0.8343 - val_recall_2: 0.6402\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9277 - loss: 0.2008 - precision_2: 0.8388 - recall_2: 0.6213 - val_auc_2: 0.9267 - val_loss: 0.2017 - val_precision_2: 0.8522 - val_recall_2: 0.6028\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9323 - loss: 0.1909 - precision_2: 0.8366 - recall_2: 0.6364 - val_auc_2: 0.9260 - val_loss: 0.2010 - val_precision_2: 0.8455 - val_recall_2: 0.6205\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9338 - loss: 0.1900 - precision_2: 0.8454 - recall_2: 0.6420 - val_auc_2: 0.9263 - val_loss: 0.2039 - val_precision_2: 0.7818 - val_recall_2: 0.6878\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9316 - loss: 0.1903 - precision_2: 0.8384 - recall_2: 0.6413 - val_auc_2: 0.9285 - val_loss: 0.2006 - val_precision_2: 0.8368 - val_recall_2: 0.6568\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9335 - loss: 0.1890 - precision_2: 0.8415 - recall_2: 0.6481 - val_auc_2: 0.9275 - val_loss: 0.2003 - val_precision_2: 0.8272 - val_recall_2: 0.6367\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9290 - loss: 0.1975 - precision_2: 0.8325 - recall_2: 0.6455 - val_auc_2: 0.9294 - val_loss: 0.2011 - val_precision_2: 0.8144 - val_recall_2: 0.6657\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_2: 0.9269 - loss: 0.1968 - precision_2: 0.8326 - recall_2: 0.6501 - val_auc_2: 0.9245 - val_loss: 0.2170 - val_precision_2: 0.7600 - val_recall_2: 0.6652\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9238 - loss: 0.2031 - precision_2: 0.8451 - recall_2: 0.5981 - val_auc_2: 0.9240 - val_loss: 0.2152 - val_precision_2: 0.8659 - val_recall_2: 0.5292\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9310 - loss: 0.1977 - precision_2: 0.8311 - recall_2: 0.6299 - val_auc_2: 0.9286 - val_loss: 0.2074 - val_precision_2: 0.8663 - val_recall_2: 0.5852\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - auc_2: 0.9314 - loss: 0.2016 - precision_2: 0.8617 - recall_2: 0.6021\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 648us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9329 - loss: 0.1929 - precision_2: 0.8473 - recall_2: 0.6351 - val_auc_2: 0.9284 - val_loss: 0.1953 - val_precision_2: 0.8233 - val_recall_2: 0.6505\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9297 - loss: 0.1956 - precision_2: 0.8275 - recall_2: 0.6371 - val_auc_2: 0.9307 - val_loss: 0.1942 - val_precision_2: 0.8423 - val_recall_2: 0.6242\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9349 - loss: 0.1887 - precision_2: 0.8419 - recall_2: 0.6359 - val_auc_2: 0.9296 - val_loss: 0.1960 - val_precision_2: 0.8110 - val_recall_2: 0.6662\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9303 - loss: 0.1887 - precision_2: 0.8352 - recall_2: 0.6507 - val_auc_2: 0.9279 - val_loss: 0.1978 - val_precision_2: 0.8060 - val_recall_2: 0.6535\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9324 - loss: 0.1886 - precision_2: 0.8412 - recall_2: 0.6362 - val_auc_2: 0.9297 - val_loss: 0.1962 - val_precision_2: 0.8231 - val_recall_2: 0.6308\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9373 - loss: 0.1862 - precision_2: 0.8391 - recall_2: 0.6463 - val_auc_2: 0.9284 - val_loss: 0.1981 - val_precision_2: 0.8176 - val_recall_2: 0.6394\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9351 - loss: 0.1886 - precision_2: 0.8386 - recall_2: 0.6290 - val_auc_2: 0.9318 - val_loss: 0.1912 - val_precision_2: 0.8163 - val_recall_2: 0.6677\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9341 - loss: 0.1860 - precision_2: 0.8438 - recall_2: 0.6476 - val_auc_2: 0.9306 - val_loss: 0.1924 - val_precision_2: 0.8751 - val_recall_2: 0.5918\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9385 - loss: 0.1820 - precision_2: 0.8438 - recall_2: 0.6451 - val_auc_2: 0.9308 - val_loss: 0.1975 - val_precision_2: 0.8552 - val_recall_2: 0.5913\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9354 - loss: 0.1904 - precision_2: 0.8274 - recall_2: 0.6470 - val_auc_2: 0.9303 - val_loss: 0.1959 - val_precision_2: 0.8498 - val_recall_2: 0.6323\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9364 - loss: 0.1865 - precision_2: 0.8322 - recall_2: 0.6556 - val_auc_2: 0.9343 - val_loss: 0.1888 - val_precision_2: 0.8527 - val_recall_2: 0.6414\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9374 - loss: 0.1830 - precision_2: 0.8491 - recall_2: 0.6590 - val_auc_2: 0.9339 - val_loss: 0.1883 - val_precision_2: 0.8331 - val_recall_2: 0.6490\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9357 - loss: 0.1850 - precision_2: 0.8384 - recall_2: 0.6587 - val_auc_2: 0.9302 - val_loss: 0.1952 - val_precision_2: 0.8576 - val_recall_2: 0.5969\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9346 - loss: 0.1910 - precision_2: 0.8415 - recall_2: 0.6365 - val_auc_2: 0.9300 - val_loss: 0.1978 - val_precision_2: 0.8386 - val_recall_2: 0.6125\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_2: 0.9378 - loss: 0.1807 - precision_2: 0.8450 - recall_2: 0.6584 - val_auc_2: 0.9323 - val_loss: 0.1974 - val_precision_2: 0.8075 - val_recall_2: 0.6535\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - auc_2: 0.9198 - loss: 0.2113 - precision_2: 0.7902 - recall_2: 0.6127\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 635us/step\n",
      "losses:  [0.22128671407699585, 0.20392736792564392, 0.2211766242980957, 0.1964556723833084, 0.2110828459262848]\n",
      "aucs:  [0.9178787469863892, 0.9254797101020813, 0.912755012512207, 0.9339151978492737, 0.9249261617660522]\n",
      "precisions:  [0.7893174886703491, 0.8499550819396973, 0.7572613954544067, 0.8617305755615234, 0.8205317854881287]\n",
      "recalls:  [0.6247798204421997, 0.5785932540893555, 0.6640388369560242, 0.5940959453582764, 0.6348571181297302]\n",
      "f1s:  [0.6923437814168584, 0.6962931259956725, 0.7290334911971457, 0.7089684509039348, 0.7288761189081666]\n",
      "roc_aucs:  [0.919343153372481, 0.9244273056432122, 0.9245685280530831, 0.9325549459122646, 0.9337906019157628]\n",
      "Average Loss: 0.21078584492206573\n",
      "Average AUC: 0.9229909658432007\n",
      "Average Precision: 0.8157592654228211\n",
      "Average Recall: 0.6192729949951172\n",
      "Average F1 Score: 0.7111029936843555\n",
      "Average ROC AUC Score: 0.9269369069793607\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - auc_3: 0.5923 - loss: 1.4285 - precision_3: 0.2794 - recall_3: 0.3027 - val_auc_3: 0.6843 - val_loss: 0.4594 - val_precision_3: 0.8361 - val_recall_3: 0.2302\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.6820 - loss: 0.4411 - precision_3: 0.7505 - recall_3: 0.2245 - val_auc_3: 0.7102 - val_loss: 0.3662 - val_precision_3: 0.8351 - val_recall_3: 0.2844\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.7812 - loss: 0.3457 - precision_3: 0.7864 - recall_3: 0.3640 - val_auc_3: 0.8059 - val_loss: 0.3058 - val_precision_3: 0.8525 - val_recall_3: 0.3826\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.7900 - loss: 0.3090 - precision_3: 0.7887 - recall_3: 0.4061 - val_auc_3: 0.8041 - val_loss: 0.2822 - val_precision_3: 0.8029 - val_recall_3: 0.4985\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.8446 - loss: 0.2818 - precision_3: 0.7993 - recall_3: 0.4724 - val_auc_3: 0.8913 - val_loss: 0.2539 - val_precision_3: 0.8160 - val_recall_3: 0.5206\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.8760 - loss: 0.2596 - precision_3: 0.8072 - recall_3: 0.5067 - val_auc_3: 0.8927 - val_loss: 0.2445 - val_precision_3: 0.8502 - val_recall_3: 0.4754\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.8858 - loss: 0.2504 - precision_3: 0.8009 - recall_3: 0.5145 - val_auc_3: 0.8997 - val_loss: 0.2496 - val_precision_3: 0.7287 - val_recall_3: 0.6249\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.8912 - loss: 0.2542 - precision_3: 0.7980 - recall_3: 0.5221 - val_auc_3: 0.9000 - val_loss: 0.2355 - val_precision_3: 0.7822 - val_recall_3: 0.5637\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.8978 - loss: 0.2387 - precision_3: 0.8128 - recall_3: 0.5596 - val_auc_3: 0.9046 - val_loss: 0.2327 - val_precision_3: 0.8180 - val_recall_3: 0.5366\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.8991 - loss: 0.2370 - precision_3: 0.8165 - recall_3: 0.5428 - val_auc_3: 0.9073 - val_loss: 0.2287 - val_precision_3: 0.8374 - val_recall_3: 0.5346\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9025 - loss: 0.2329 - precision_3: 0.8118 - recall_3: 0.5498 - val_auc_3: 0.9117 - val_loss: 0.2281 - val_precision_3: 0.7496 - val_recall_3: 0.6439\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.8980 - loss: 0.2348 - precision_3: 0.7999 - recall_3: 0.5338 - val_auc_3: 0.9073 - val_loss: 0.2284 - val_precision_3: 0.8581 - val_recall_3: 0.5065\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9066 - loss: 0.2326 - precision_3: 0.8031 - recall_3: 0.5498 - val_auc_3: 0.9121 - val_loss: 0.2225 - val_precision_3: 0.8604 - val_recall_3: 0.5100\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9151 - loss: 0.2235 - precision_3: 0.8103 - recall_3: 0.5525 - val_auc_3: 0.9093 - val_loss: 0.2324 - val_precision_3: 0.8896 - val_recall_3: 0.4403\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9129 - loss: 0.2205 - precision_3: 0.8252 - recall_3: 0.5767 - val_auc_3: 0.9079 - val_loss: 0.2320 - val_precision_3: 0.8695 - val_recall_3: 0.4845\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - auc_3: 0.9100 - loss: 0.2210 - precision_3: 0.8757 - recall_3: 0.5107\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 646us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9049 - loss: 0.2247 - precision_3: 0.8207 - recall_3: 0.5580 - val_auc_3: 0.9137 - val_loss: 0.2196 - val_precision_3: 0.8048 - val_recall_3: 0.5707\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_3: 0.9100 - loss: 0.2215 - precision_3: 0.8238 - recall_3: 0.5722 - val_auc_3: 0.9104 - val_loss: 0.2191 - val_precision_3: 0.7951 - val_recall_3: 0.5918\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9120 - loss: 0.2180 - precision_3: 0.8141 - recall_3: 0.5760 - val_auc_3: 0.9113 - val_loss: 0.2260 - val_precision_3: 0.8347 - val_recall_3: 0.4990\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9139 - loss: 0.2176 - precision_3: 0.8257 - recall_3: 0.5847 - val_auc_3: 0.9163 - val_loss: 0.2125 - val_precision_3: 0.8668 - val_recall_3: 0.5416\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9129 - loss: 0.2204 - precision_3: 0.8236 - recall_3: 0.5694 - val_auc_3: 0.9162 - val_loss: 0.2176 - val_precision_3: 0.8927 - val_recall_3: 0.5090\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9183 - loss: 0.2134 - precision_3: 0.8264 - recall_3: 0.5782 - val_auc_3: 0.9165 - val_loss: 0.2125 - val_precision_3: 0.8430 - val_recall_3: 0.5707\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_3: 0.9128 - loss: 0.2221 - precision_3: 0.8259 - recall_3: 0.5761 - val_auc_3: 0.9190 - val_loss: 0.2096 - val_precision_3: 0.8280 - val_recall_3: 0.5893\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9176 - loss: 0.2121 - precision_3: 0.8326 - recall_3: 0.5883 - val_auc_3: 0.9165 - val_loss: 0.2107 - val_precision_3: 0.8231 - val_recall_3: 0.5998\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9202 - loss: 0.2092 - precision_3: 0.8319 - recall_3: 0.6084 - val_auc_3: 0.9160 - val_loss: 0.2111 - val_precision_3: 0.8054 - val_recall_3: 0.6103\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9198 - loss: 0.2077 - precision_3: 0.8316 - recall_3: 0.6029 - val_auc_3: 0.8989 - val_loss: 0.2365 - val_precision_3: 0.8626 - val_recall_3: 0.4880\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9189 - loss: 0.2131 - precision_3: 0.8226 - recall_3: 0.5892 - val_auc_3: 0.9206 - val_loss: 0.2062 - val_precision_3: 0.8615 - val_recall_3: 0.5707\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9198 - loss: 0.2041 - precision_3: 0.8399 - recall_3: 0.6161 - val_auc_3: 0.9175 - val_loss: 0.2123 - val_precision_3: 0.8100 - val_recall_3: 0.6008\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9157 - loss: 0.2145 - precision_3: 0.8134 - recall_3: 0.5817 - val_auc_3: 0.9194 - val_loss: 0.2071 - val_precision_3: 0.8293 - val_recall_3: 0.6068\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9223 - loss: 0.2051 - precision_3: 0.8466 - recall_3: 0.6148 - val_auc_3: 0.9219 - val_loss: 0.2010 - val_precision_3: 0.8447 - val_recall_3: 0.6108\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9229 - loss: 0.2035 - precision_3: 0.8282 - recall_3: 0.6156 - val_auc_3: 0.9231 - val_loss: 0.2047 - val_precision_3: 0.8115 - val_recall_3: 0.6349\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - auc_3: 0.9193 - loss: 0.2131 - precision_3: 0.8006 - recall_3: 0.6259\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 638us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9179 - loss: 0.2106 - precision_3: 0.8301 - recall_3: 0.5920 - val_auc_3: 0.9215 - val_loss: 0.2088 - val_precision_3: 0.8476 - val_recall_3: 0.5866\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9173 - loss: 0.2109 - precision_3: 0.8333 - recall_3: 0.5929 - val_auc_3: 0.9214 - val_loss: 0.2056 - val_precision_3: 0.8392 - val_recall_3: 0.6067\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_3: 0.9178 - loss: 0.2099 - precision_3: 0.8304 - recall_3: 0.6004 - val_auc_3: 0.9194 - val_loss: 0.2084 - val_precision_3: 0.8207 - val_recall_3: 0.6287\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9231 - loss: 0.2053 - precision_3: 0.8354 - recall_3: 0.6176 - val_auc_3: 0.9228 - val_loss: 0.2092 - val_precision_3: 0.7878 - val_recall_3: 0.6626\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9249 - loss: 0.1994 - precision_3: 0.8304 - recall_3: 0.6181 - val_auc_3: 0.9248 - val_loss: 0.2058 - val_precision_3: 0.8197 - val_recall_3: 0.6130\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9253 - loss: 0.2029 - precision_3: 0.8304 - recall_3: 0.5943 - val_auc_3: 0.9191 - val_loss: 0.2124 - val_precision_3: 0.8393 - val_recall_3: 0.5817\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_3: 0.9208 - loss: 0.2056 - precision_3: 0.8314 - recall_3: 0.6099 - val_auc_3: 0.9247 - val_loss: 0.2062 - val_precision_3: 0.8531 - val_recall_3: 0.5895\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9198 - loss: 0.2039 - precision_3: 0.8372 - recall_3: 0.5907 - val_auc_3: 0.9237 - val_loss: 0.2055 - val_precision_3: 0.8265 - val_recall_3: 0.6282\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9238 - loss: 0.2013 - precision_3: 0.8370 - recall_3: 0.6007 - val_auc_3: 0.9243 - val_loss: 0.2057 - val_precision_3: 0.8452 - val_recall_3: 0.5920\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9248 - loss: 0.2047 - precision_3: 0.8384 - recall_3: 0.6099 - val_auc_3: 0.9246 - val_loss: 0.2122 - val_precision_3: 0.9188 - val_recall_3: 0.4993\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9229 - loss: 0.2047 - precision_3: 0.8409 - recall_3: 0.5993 - val_auc_3: 0.9265 - val_loss: 0.2013 - val_precision_3: 0.8335 - val_recall_3: 0.6435\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9243 - loss: 0.2010 - precision_3: 0.8416 - recall_3: 0.6184 - val_auc_3: 0.9253 - val_loss: 0.2041 - val_precision_3: 0.8124 - val_recall_3: 0.6479\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9224 - loss: 0.2052 - precision_3: 0.8337 - recall_3: 0.6123 - val_auc_3: 0.9248 - val_loss: 0.2093 - val_precision_3: 0.8778 - val_recall_3: 0.5390\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9269 - loss: 0.1993 - precision_3: 0.8369 - recall_3: 0.6153 - val_auc_3: 0.9264 - val_loss: 0.2013 - val_precision_3: 0.8555 - val_recall_3: 0.6184\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9255 - loss: 0.2021 - precision_3: 0.8415 - recall_3: 0.6153 - val_auc_3: 0.9260 - val_loss: 0.2025 - val_precision_3: 0.8195 - val_recall_3: 0.6415\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - auc_3: 0.9327 - loss: 0.1956 - precision_3: 0.8067 - recall_3: 0.6804\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 652us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9291 - loss: 0.1928 - precision_3: 0.8387 - recall_3: 0.6259 - val_auc_3: 0.9236 - val_loss: 0.2119 - val_precision_3: 0.7590 - val_recall_3: 0.6479\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9306 - loss: 0.1950 - precision_3: 0.8406 - recall_3: 0.6300 - val_auc_3: 0.9274 - val_loss: 0.2033 - val_precision_3: 0.8961 - val_recall_3: 0.5413\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9311 - loss: 0.1955 - precision_3: 0.8396 - recall_3: 0.6146 - val_auc_3: 0.9250 - val_loss: 0.2032 - val_precision_3: 0.8648 - val_recall_3: 0.5832\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_3: 0.9322 - loss: 0.1898 - precision_3: 0.8408 - recall_3: 0.6318 - val_auc_3: 0.9273 - val_loss: 0.1964 - val_precision_3: 0.8433 - val_recall_3: 0.6325\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9316 - loss: 0.1911 - precision_3: 0.8501 - recall_3: 0.6437 - val_auc_3: 0.9259 - val_loss: 0.2000 - val_precision_3: 0.8494 - val_recall_3: 0.6066\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9317 - loss: 0.1943 - precision_3: 0.8572 - recall_3: 0.6289 - val_auc_3: 0.9236 - val_loss: 0.2054 - val_precision_3: 0.8917 - val_recall_3: 0.5578\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9339 - loss: 0.1891 - precision_3: 0.8481 - recall_3: 0.6396 - val_auc_3: 0.9134 - val_loss: 0.2163 - val_precision_3: 0.7909 - val_recall_3: 0.6384\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9298 - loss: 0.1978 - precision_3: 0.8383 - recall_3: 0.6205 - val_auc_3: 0.9292 - val_loss: 0.2001 - val_precision_3: 0.9215 - val_recall_3: 0.5319\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9288 - loss: 0.1906 - precision_3: 0.8543 - recall_3: 0.6109 - val_auc_3: 0.9266 - val_loss: 0.1987 - val_precision_3: 0.8703 - val_recall_3: 0.5916\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9301 - loss: 0.1926 - precision_3: 0.8450 - recall_3: 0.6324 - val_auc_3: 0.9271 - val_loss: 0.1998 - val_precision_3: 0.8025 - val_recall_3: 0.6678\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9317 - loss: 0.1912 - precision_3: 0.8372 - recall_3: 0.6339 - val_auc_3: 0.9281 - val_loss: 0.1979 - val_precision_3: 0.8342 - val_recall_3: 0.6340\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9352 - loss: 0.1889 - precision_3: 0.8442 - recall_3: 0.6390 - val_auc_3: 0.9185 - val_loss: 0.2359 - val_precision_3: 0.6728 - val_recall_3: 0.6962\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9335 - loss: 0.1873 - precision_3: 0.8409 - recall_3: 0.6388 - val_auc_3: 0.9269 - val_loss: 0.1973 - val_precision_3: 0.8764 - val_recall_3: 0.5966\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9402 - loss: 0.1791 - precision_3: 0.8528 - recall_3: 0.6667 - val_auc_3: 0.9273 - val_loss: 0.1987 - val_precision_3: 0.8698 - val_recall_3: 0.5956\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9332 - loss: 0.1900 - precision_3: 0.8515 - recall_3: 0.6319 - val_auc_3: 0.9257 - val_loss: 0.2004 - val_precision_3: 0.8592 - val_recall_3: 0.5867\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - auc_3: 0.9156 - loss: 0.2209 - precision_3: 0.8526 - recall_3: 0.5635\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 643us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9305 - loss: 0.1958 - precision_3: 0.8447 - recall_3: 0.6376 - val_auc_3: 0.9223 - val_loss: 0.2060 - val_precision_3: 0.8346 - val_recall_3: 0.6109\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9288 - loss: 0.1983 - precision_3: 0.8367 - recall_3: 0.6235 - val_auc_3: 0.9272 - val_loss: 0.1974 - val_precision_3: 0.8512 - val_recall_3: 0.6268\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9311 - loss: 0.1947 - precision_3: 0.8524 - recall_3: 0.6231 - val_auc_3: 0.9266 - val_loss: 0.2058 - val_precision_3: 0.7686 - val_recall_3: 0.6836\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9293 - loss: 0.1960 - precision_3: 0.8457 - recall_3: 0.6229 - val_auc_3: 0.9289 - val_loss: 0.1964 - val_precision_3: 0.8265 - val_recall_3: 0.6457\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9324 - loss: 0.1903 - precision_3: 0.8460 - recall_3: 0.6407 - val_auc_3: 0.9250 - val_loss: 0.2097 - val_precision_3: 0.8626 - val_recall_3: 0.5725\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9306 - loss: 0.1952 - precision_3: 0.8555 - recall_3: 0.6188 - val_auc_3: 0.9300 - val_loss: 0.1944 - val_precision_3: 0.8561 - val_recall_3: 0.6253\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9275 - loss: 0.1967 - precision_3: 0.8491 - recall_3: 0.6325 - val_auc_3: 0.9279 - val_loss: 0.1964 - val_precision_3: 0.8338 - val_recall_3: 0.6373\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_3: 0.9355 - loss: 0.1880 - precision_3: 0.8614 - recall_3: 0.6454 - val_auc_3: 0.9295 - val_loss: 0.1963 - val_precision_3: 0.8596 - val_recall_3: 0.6163\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9321 - loss: 0.1946 - precision_3: 0.8481 - recall_3: 0.6324 - val_auc_3: 0.9294 - val_loss: 0.1972 - val_precision_3: 0.8532 - val_recall_3: 0.6198\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_3: 0.9364 - loss: 0.1864 - precision_3: 0.8544 - recall_3: 0.6515 - val_auc_3: 0.9304 - val_loss: 0.1963 - val_precision_3: 0.8480 - val_recall_3: 0.6228\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9373 - loss: 0.1860 - precision_3: 0.8598 - recall_3: 0.6555 - val_auc_3: 0.9292 - val_loss: 0.2044 - val_precision_3: 0.8950 - val_recall_3: 0.5306\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9335 - loss: 0.1901 - precision_3: 0.8582 - recall_3: 0.6280 - val_auc_3: 0.9277 - val_loss: 0.2015 - val_precision_3: 0.8662 - val_recall_3: 0.5904\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9356 - loss: 0.1874 - precision_3: 0.8551 - recall_3: 0.6423 - val_auc_3: 0.9299 - val_loss: 0.1993 - val_precision_3: 0.8187 - val_recall_3: 0.6547\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9350 - loss: 0.1901 - precision_3: 0.8516 - recall_3: 0.6332 - val_auc_3: 0.9305 - val_loss: 0.1959 - val_precision_3: 0.8854 - val_recall_3: 0.5889\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_3: 0.9386 - loss: 0.1803 - precision_3: 0.8662 - recall_3: 0.6591 - val_auc_3: 0.9317 - val_loss: 0.1973 - val_precision_3: 0.7986 - val_recall_3: 0.6856\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - auc_3: 0.9291 - loss: 0.1944 - precision_3: 0.7649 - recall_3: 0.6836\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 643us/step\n",
      "losses:  [0.23064634203910828, 0.2092396318912506, 0.19604037702083588, 0.21491739153862, 0.19309955835342407]\n",
      "aucs:  [0.9113864302635193, 0.920840859413147, 0.930456280708313, 0.9186224341392517, 0.9308146238327026]\n",
      "precisions:  [0.8748674392700195, 0.8127398490905762, 0.8160412311553955, 0.8446683287620544, 0.7837837934494019]\n",
      "recalls:  [0.4928315281867981, 0.6326164603233337, 0.6664663553237915, 0.5781609416007996, 0.7020484209060669]\n",
      "f1s:  [0.628202169397646, 0.7209343954175244, 0.728584110075455, 0.7030715123094959, 0.7474417914874685]\n",
      "roc_aucs:  [0.9110710398800106, 0.9258327675100142, 0.929131500858177, 0.92878798151452, 0.9336458880628247]\n",
      "Average Loss: 0.20878866016864778\n",
      "Average AUC: 0.9224241256713868\n",
      "Average Precision: 0.8264201283454895\n",
      "Average Recall: 0.6144247412681579\n",
      "Average F1 Score: 0.705646795737518\n",
      "Average ROC AUC Score: 0.9256938355651092\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from colorama import init\n",
    "from termcolor import colored\n",
    "\n",
    "# Define the L2 regularizers\n",
    "k = .0001\n",
    "kernel_regularizer = l2(0.0001*k)\n",
    "bias_regularizer = l2(0.00001*k)\n",
    "\n",
    "# Define the Dense model with different regularization strengths for kernel and bias\n",
    "class DenseModel:\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = self.build_model()\n",
    "        self.THRESHOLD = 0.5\n",
    "\n",
    "    def build_model(self):\n",
    "        input_layer = Input(shape=(self.input_dim,))\n",
    "        x = Dense(self.hidden_dim, activation='relu')(input_layer)\n",
    "        x = Dense(self.hidden_dim // 2, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        x = Dense(self.hidden_dim // 4, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        x = Dense(self.hidden_dim // 8, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        x = Dense(self.hidden_dim // 16, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        x = Dense(self.hidden_dim // 32, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # Compile the model with Binary Crossentropy loss\n",
    "        model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=[AUC(), Precision(), Recall()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=10, verbose=0, batch_size=32, validation_split=0.3, validation_data=None):\n",
    "        if validation_data is not None:\n",
    "            self.model.fit(X_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, validation_data=validation_data, validation_split=validation_split)\n",
    "            return \n",
    "        self.model.fit(X_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(colored(f'predicting using dense model of hidden {self.hidden_dim}', 'green'))\n",
    "        return self.model.predict(X_test)\n",
    "    \n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "        \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "# Example usage\n",
    "# Assuming xult and yult are your feature matrix and target vector\n",
    "for i in range(2):\n",
    "    dense_model = DenseModel(input_dim=xult.shape[1])\n",
    "    ktrain(dense_model, xult, yult,epochs=15,batch_size=32,splits=5,random_state=i)\n",
    "    ens.add_model(dense_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9941  148]\n",
      " [ 472 1168]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9984  114]\n",
      " [ 483 1148]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9953   81]\n",
      " [ 541 1154]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9918   95]\n",
      " [ 519 1197]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9967   81]\n",
      " [ 487 1194]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9219965780166277, 0.9422302150148654, 0.9452250254738608, 0.9537328454361311, 0.956263711697238]\n",
      "precisions:  [0.8875379939209727, 0.9096671949286846, 0.934412955465587, 0.9264705882352942, 0.9364705882352942]\n",
      "recalls:  [0.7121951219512195, 0.703862660944206, 0.6808259587020649, 0.6975524475524476, 0.710291493158834]\n",
      "f1s:  [0.7777777777777778, 0.7905192492122208, 0.7925875085792725, 0.7992758542656709, 0.8015284435199421]\n",
      "roc_aucs:  [0.8430414724780482, 0.8407570186200817, 0.8413146626520296, 0.8475906125913002, 0.8489774898954453]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9438896751277446\n",
      "Average Precision: 0.9189118641571665\n",
      "Average Recall: 0.7009455364617543\n",
      "Average F1 Score: 0.7923377666709769\n",
      "Average ROC AUC Score: 0.844336251247381\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9941  148]\n",
      " [ 472 1168]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9984  114]\n",
      " [ 483 1148]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9953   81]\n",
      " [ 541 1154]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9918   95]\n",
      " [ 519 1197]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9967   81]\n",
      " [ 487 1194]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9219965780166277, 0.9422302150148654, 0.9452250254738608, 0.9537328454361311, 0.956263711697238]\n",
      "precisions:  [0.8875379939209727, 0.9096671949286846, 0.934412955465587, 0.9264705882352942, 0.9364705882352942]\n",
      "recalls:  [0.7121951219512195, 0.703862660944206, 0.6808259587020649, 0.6975524475524476, 0.710291493158834]\n",
      "f1s:  [0.7777777777777778, 0.7905192492122208, 0.7925875085792725, 0.7992758542656709, 0.8015284435199421]\n",
      "roc_aucs:  [0.8430414724780482, 0.8407570186200817, 0.8413146626520296, 0.8475906125913002, 0.8489774898954453]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9438896751277446\n",
      "Average Precision: 0.9189118641571665\n",
      "Average Recall: 0.7009455364617543\n",
      "Average F1 Score: 0.7923377666709769\n",
      "Average ROC AUC Score: 0.844336251247381\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class XGBRFClassifierModel:\n",
    "    def __init__(self, objective='binary:logistic', eval_metric='auc', n_estimators=600, max_depth=5, subsample=0.9,\n",
    "                 colsample_bynode=0.9, reg_alpha=0.1, reg_lambda=1.0, min_child_weight=1, random_state=42, model_path='xgbrf_model.json', **kwargs):\n",
    "        self.model = xgb.XGBRFClassifier(objective=objective, eval_metric=eval_metric, n_estimators=n_estimators,\n",
    "                                         max_depth=max_depth, subsample=subsample, colsample_bynode=colsample_bynode, reg_alpha=reg_alpha,\n",
    "                                         reg_lambda=reg_lambda, min_child_weight=min_child_weight, random_state=random_state, **kwargs)\n",
    "        self.model_path = model_path\n",
    "        self.THRESHOLD = 0.5\n",
    "        self.load_model()\n",
    "\n",
    "    def fit(self, X_train, y_train, verbose=0, **kwargs):\n",
    "        self.model.fit(X_train, y_train, verbose=verbose, xgb_model=self.model_path if self.model_exists() else None)\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(colored('predicting using xgbrf model', 'green'))\n",
    "        return self.model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        y_pred_proba = self.predict_proba(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        # Assuming loss is not directly available, using log loss as a proxy\n",
    "        loss = -1  # Placeholder, as XGBoost does not directly provide loss in this context\n",
    "        return [loss, auc, precision, recall]\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save_model(self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_exists():\n",
    "            self.model.load_model(self.model_path)\n",
    "            \n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "\n",
    "    def model_exists(self):\n",
    "        try:\n",
    "            with open(self.model_path, 'r'):\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "    def save(self, path):\n",
    "        self.model.save_model(path)\n",
    "\n",
    "for i in range(2):\n",
    "    if i%2==0:\n",
    "        xgbrf_model = XGBRFClassifierModel(model_path=f'xgbrf_model{i}.json')\n",
    "    else:\n",
    "        xgbrf_model = XGBRFClassifierModel(eval_metric='logloss', model_path=f'xgbrf_model{i}.json')\n",
    "    ktrain(xgbrf_model, xult, yult, epochs=10, batch_size=32, splits=5, random_state=5)\n",
    "    ens.add_model(xgbrf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6660, number of negative: 40256\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141956 -> initscore=-1.799140\n",
      "[LightGBM] [Info] Start training from score -1.799140\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9933   93]\n",
      " [ 466 1237]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6728, number of negative: 40188\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143405 -> initscore=-1.787291\n",
      "[LightGBM] [Info] Start training from score -1.787291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[10011    83]\n",
      " [  430  1205]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6714, number of negative: 40202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143107 -> initscore=-1.789722\n",
      "[LightGBM] [Info] Start training from score -1.789722\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9993   87]\n",
      " [ 477 1172]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6737, number of negative: 40179\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143597 -> initscore=-1.785730\n",
      "[LightGBM] [Info] Start training from score -1.785730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[10015    88]\n",
      " [  446  1180]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6613, number of negative: 40303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.140954 -> initscore=-1.807388\n",
      "[LightGBM] [Info] Start training from score -1.807388\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9897   82]\n",
      " [ 527 1223]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9633493726645425, 0.9645975536380045, 0.9565594107058631, 0.9620984426215636, 0.9561801497430318]\n",
      "precisions:  [0.9300751879699248, 0.9355590062111802, 0.9308975377283558, 0.9305993690851735, 0.9371647509578545]\n",
      "recalls:  [0.7263652378156195, 0.7370030581039755, 0.7107337780473014, 0.7257072570725708, 0.6988571428571428]\n",
      "f1s:  [0.8260830688067242, 0.8253137373330649, 0.8254103125978266, 0.8239427164913851, 0.823023516237402]\n",
      "roc_aucs:  [0.8648026603121425, 0.8642071716989521, 0.8643769484591808, 0.8634123918382541, 0.8626838102302206]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.960556985874601\n",
      "Average Precision: 0.9328591703904978\n",
      "Average Recall: 0.719733294779322\n",
      "Average F1 Score: 0.8247546702932805\n",
      "Average ROC AUC Score: 0.8638965965077501\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9977   78]\n",
      " [ 464 1210]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9971   84]\n",
      " [ 476 1198]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6699, number of negative: 40217\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142787 -> initscore=-1.792332\n",
      "[LightGBM] [Info] Start training from score -1.792332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9961  104]\n",
      " [ 466 1198]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6623, number of negative: 40293\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141167 -> initscore=-1.805629\n",
      "[LightGBM] [Info] Start training from score -1.805629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9901   88]\n",
      " [ 494 1246]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6752, number of negative: 40164\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143917 -> initscore=-1.783132\n",
      "[LightGBM] [Info] Start training from score -1.783132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[10041    77]\n",
      " [  440  1171]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9630844572295623, 0.9615678879662454, 0.9609952078317856, 0.9572158972570977, 0.9587822110026578]\n",
      "precisions:  [0.9394409937888198, 0.9344773790951638, 0.9201228878648233, 0.9340329835082459, 0.938301282051282]\n",
      "recalls:  [0.7228195937873357, 0.7156511350059738, 0.7199519230769231, 0.7160919540229885, 0.7268777157045313]\n",
      "f1s:  [0.8252422828427853, 0.8238323406923111, 0.8249687444186462, 0.8246808510638298, 0.8257897323622716]\n",
      "roc_aucs:  [0.8633712334749926, 0.8631793436687615, 0.8645871870931563, 0.8635449431190307, 0.8648262437248974]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9603291322574699\n",
      "Average Precision: 0.933275105261667\n",
      "Average Recall: 0.7202784643195504\n",
      "Average F1 Score: 0.8249027902759689\n",
      "Average ROC AUC Score: 0.8639017902161676\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class LGBMClassifierModel:\n",
    "    def __init__(self, eval_metric='logloss', model_path='lgbm_model.txt', **kwargs):\n",
    "        self.model = lgb.LGBMClassifier(objective='binary', metric=eval_metric, )\n",
    "        self.model_path = model_path\n",
    "        self.THRESHOLD = 0.5\n",
    "        self.init_model = None\n",
    "        self.load_model()\n",
    "\n",
    "    def fit(self, X_train, y_train, verbose=0, **kwargs):\n",
    "        self.model.fit(X_train, y_train,   init_model=self.init_model)\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        ret = self.model.predict(X_test).astype(float)\n",
    "        # All ret values multiplied by 1.1\n",
    "        # ret *= 1.1\n",
    "        print(colored('predicting using lgbm model', 'green'))\n",
    "        return ret.reshape(-1, 1)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        y_pred_proba = self.predict_proba(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        # Assuming loss is not directly available, using log loss as a proxy\n",
    "        loss = -1  # Placeholder, as LightGBM does not directly provide loss in this context\n",
    "        return [loss, auc, precision, recall]\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.booster_.save_model(self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_exists():\n",
    "            self.init_model = self.model_path\n",
    "\n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "\n",
    "    def model_exists(self):\n",
    "        try:\n",
    "            with open(self.model_path, 'r'):\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.booster_.save_model(path)\n",
    "\n",
    "# Example usage\n",
    "# Assuming xult and yult are your feature matrix and target vector\n",
    "# Split the data into training and test sets\n",
    "\n",
    "for i in range(2):\n",
    "    # Initialize the LightGBM classifier model\n",
    "    if i % 2 == 0:\n",
    "        lgbm_model = LGBMClassifierModel(model_path=f'lgbm_model{i}.txt')\n",
    "    else:\n",
    "        lgbm_model = LGBMClassifierModel(eval_metric='auc', model_path=f'lgbm_model{i}.txt')\n",
    "    # Fit the model on training data\n",
    "    ktrain(lgbm_model, xult, yult, epochs=10, batch_size=32, splits=5, random_state=i)\n",
    "    ens.add_model(lgbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6660, number of negative: 40256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141956 -> initscore=-1.799140\n",
      "[LightGBM] [Info] Start training from score -1.799140\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9894  132]\n",
      " [ 490 1213]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6728, number of negative: 40188\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143405 -> initscore=-1.787291\n",
      "[LightGBM] [Info] Start training from score -1.787291\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9992  102]\n",
      " [ 470 1165]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6714, number of negative: 40202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143107 -> initscore=-1.789722\n",
      "[LightGBM] [Info] Start training from score -1.789722\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9984   96]\n",
      " [ 497 1152]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6737, number of negative: 40179\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143597 -> initscore=-1.785730\n",
      "[LightGBM] [Info] Start training from score -1.785730\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9973  130]\n",
      " [ 463 1163]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6613, number of negative: 40303\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.140954 -> initscore=-1.807388\n",
      "[LightGBM] [Info] Start training from score -1.807388\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9865  114]\n",
      " [ 554 1196]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9251168043533086, 0.9277543688714464, 0.9181703437388702, 0.9288854625159139, 0.9173807853635492]\n",
      "precisions:  [0.9018587360594795, 0.9194948697711128, 0.9230769230769231, 0.8994586233565351, 0.9129770992366413]\n",
      "recalls:  [0.7122724603640634, 0.7125382262996942, 0.698605215281989, 0.7152521525215252, 0.6834285714285714]\n",
      "f1s:  [0.7978656622724419, 0.7955169464824233, 0.7969897706277319, 0.7962230215827338, 0.7959041609592886]\n",
      "roc_aucs:  [0.8493374043244657, 0.8471078838445983, 0.847559486851925, 0.8477467611749361, 0.8478969647939433]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9234615529686175\n",
      "Average Precision: 0.9113732503001384\n",
      "Average Recall: 0.7044193251791687\n",
      "Average F1 Score: 0.7964999123849239\n",
      "Average ROC AUC Score: 0.8479297001979736\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9940  115]\n",
      " [ 491 1183]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9948  107]\n",
      " [ 511 1163]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6699, number of negative: 40217\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142787 -> initscore=-1.792332\n",
      "[LightGBM] [Info] Start training from score -1.792332\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9946  119]\n",
      " [ 486 1178]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6623, number of negative: 40293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141167 -> initscore=-1.805629\n",
      "[LightGBM] [Info] Start training from score -1.805629\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9882  107]\n",
      " [ 534 1206]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6752, number of negative: 40164\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143917 -> initscore=-1.783132\n",
      "[LightGBM] [Info] Start training from score -1.783132\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[10025    93]\n",
      " [  478  1133]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.923420529976408, 0.9283117287416224, 0.9274783020940808, 0.9196739977193304, 0.9208254453439483]\n",
      "precisions:  [0.911402157164869, 0.915748031496063, 0.9082498072474943, 0.9185072353389185, 0.9241435562805873]\n",
      "recalls:  [0.7066905615292712, 0.6947431302270012, 0.7079326923076923, 0.6931034482758621, 0.7032898820608318]\n",
      "f1s:  [0.7956703722039695, 0.7974694943491377, 0.7958552497911823, 0.7962241196746795, 0.7975640013533326]\n",
      "roc_aucs:  [0.8472109980258193, 0.8480220372457192, 0.846468000473851, 0.8471938100139319, 0.8475755914356792]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9239420007750778\n",
      "Average Precision: 0.9156101575055864\n",
      "Average Recall: 0.7011519428801317\n",
      "Average F1 Score: 0.7965566474744603\n",
      "Average ROC AUC Score: 0.8472940874390001\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6659, number of negative: 40257\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141935 -> initscore=-1.799315\n",
      "[LightGBM] [Info] Start training from score -1.799315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9903  122]\n",
      " [ 508 1196]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6699, number of negative: 40217\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142787 -> initscore=-1.792332\n",
      "[LightGBM] [Info] Start training from score -1.792332\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9949  116]\n",
      " [ 492 1172]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6664, number of negative: 40252\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142041 -> initscore=-1.798440\n",
      "[LightGBM] [Info] Start training from score -1.798440\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9910  120]\n",
      " [ 515 1184]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6701, number of negative: 40215\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142830 -> initscore=-1.791983\n",
      "[LightGBM] [Info] Start training from score -1.791983\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9933  134]\n",
      " [ 483 1179]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6729, number of negative: 40187\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143427 -> initscore=-1.787117\n",
      "[LightGBM] [Info] Start training from score -1.787117\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[10023    72]\n",
      " [  491  1143]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.921897808296161, 0.9223558587928464, 0.9241637066434597, 0.9276047832112092, 0.9253949475090677]\n",
      "precisions:  [0.9074355083459787, 0.9099378881987578, 0.9079754601226994, 0.897943640517898, 0.9407407407407408]\n",
      "recalls:  [0.7018779342723005, 0.7043269230769231, 0.6968805179517363, 0.7093862815884476, 0.6995104039167687]\n",
      "f1s:  [0.7952920076224639, 0.7973076749333695, 0.7942460942776102, 0.797507648011517, 0.7959579555194659]\n",
      "roc_aucs:  [0.8480074804164411, 0.8470795162830717, 0.8463796117874599, 0.8482684953187373, 0.8467410461907481]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9242834208905488\n",
      "Average Precision: 0.9128066475852149\n",
      "Average Recall: 0.7023964121612353\n",
      "Average F1 Score: 0.7960622760728853\n",
      "Average ROC AUC Score: 0.8472952299992915\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9959   96]\n",
      " [ 512 1162]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6697, number of negative: 40219\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142744 -> initscore=-1.792680\n",
      "[LightGBM] [Info] Start training from score -1.792680\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9939  124]\n",
      " [ 468 1198]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6664, number of negative: 40252\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142041 -> initscore=-1.798440\n",
      "[LightGBM] [Info] Start training from score -1.798440\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9917  113]\n",
      " [ 499 1200]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6688, number of negative: 40228\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142553 -> initscore=-1.794248\n",
      "[LightGBM] [Info] Start training from score -1.794248\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9935  119]\n",
      " [ 499 1176]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6714, number of negative: 40202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143107 -> initscore=-1.789722\n",
      "[LightGBM] [Info] Start training from score -1.789722\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9972  108]\n",
      " [ 508 1141]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9193020228646863, 0.930896069050695, 0.9311536549856024, 0.9192812543607803, 0.9195411240097413]\n",
      "precisions:  [0.9236883942766295, 0.9062027231467473, 0.913937547600914, 0.9081081081081082, 0.9135308246597278]\n",
      "recalls:  [0.6941457586618877, 0.7190876350540216, 0.7062978222483814, 0.702089552238806, 0.6919345057610673]\n",
      "f1s:  [0.7972084829595502, 0.7948781309670897, 0.7954024022673085, 0.795228986159559, 0.797626567999278]\n",
      "roc_aucs:  [0.8470463010751537, 0.8472583970145008, 0.8471912701091504, 0.8469649268914203, 0.8475422214523135]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9240348250543011\n",
      "Average Precision: 0.9130935195584253\n",
      "Average Recall: 0.7027110547928329\n",
      "Average F1 Score: 0.796068914070557\n",
      "Average ROC AUC Score: 0.8472006233085076\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6688, number of negative: 40228\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142553 -> initscore=-1.794248\n",
      "[LightGBM] [Info] Start training from score -1.794248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9948  106]\n",
      " [ 512 1163]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6712, number of negative: 40204\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143064 -> initscore=-1.790070\n",
      "[LightGBM] [Info] Start training from score -1.790070\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9988   90]\n",
      " [ 487 1164]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6704, number of negative: 40212\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142894 -> initscore=-1.791461\n",
      "[LightGBM] [Info] Start training from score -1.791461\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9941  129]\n",
      " [ 486 1173]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6647, number of negative: 40269\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141679 -> initscore=-1.801416\n",
      "[LightGBM] [Info] Start training from score -1.801416\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9891  122]\n",
      " [ 521 1195]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6701, number of negative: 40215\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142830 -> initscore=-1.791983\n",
      "[LightGBM] [Info] Start training from score -1.791983\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9951  116]\n",
      " [ 494 1168]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9272353470364508, 0.9264152992485385, 0.9218677515379086, 0.9240055235885657, 0.9212227832846045]\n",
      "precisions:  [0.9164696611505122, 0.9282296650717703, 0.9009216589861752, 0.9073652239939256, 0.9096573208722741]\n",
      "recalls:  [0.6943283582089552, 0.7050272562083586, 0.7070524412296564, 0.6963869463869464, 0.7027677496991577]\n",
      "f1s:  [0.7971867180631635, 0.7949541076999593, 0.7955664470873895, 0.7959101123595506, 0.7943933657833063]\n",
      "roc_aucs:  [0.8475359808268937, 0.8456294449573861, 0.8474009670290594, 0.8477037593964074, 0.8461829378324571]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9241493409392136\n",
      "Average Precision: 0.9125287060149315\n",
      "Average Recall: 0.7011125503466149\n",
      "Average F1 Score: 0.7956021501986739\n",
      "Average ROC AUC Score: 0.8468906180084407\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class LGBMRandomForestClassifierModel:\n",
    "    def __init__(self, eval_metric='logloss', model_path='lgbm_rf_model.txt', bagging_freq=1, bagging_fraction=0.8, feature_fraction=0.8, **kwargs):\n",
    "        self.model_path = model_path\n",
    "        self.model = lgb.LGBMClassifier(boosting_type='rf', objective='binary', metric=eval_metric,\n",
    "                                        bagging_freq=bagging_freq, bagging_fraction=bagging_fraction,\n",
    "                                        feature_fraction=feature_fraction, **kwargs)\n",
    "        self.THRESHOLD = 0.5\n",
    "        self.load_model()  # Load the saved model if available\n",
    "\n",
    "    def fit(self, X_train, y_train, verbose=0, **kwargs):\n",
    "        # Train model from scratch\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.save_model()  # Save model state after training\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(colored('predicting using lgbm_rf model', 'green'))\n",
    "        ret = self.model.predict(X_test).astype(float)\n",
    "        return ret.reshape(-1, 1)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        y_pred_proba = self.predict_proba(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        loss = -1  # Placeholder for loss\n",
    "        return [loss, auc, precision, recall]\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.booster_.save_model(self.model_path)\n",
    "\n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_exists():\n",
    "            # Load the booster and convert to LGBMClassifier\n",
    "            booster = lgb.Booster(model_file=self.model_path)\n",
    "            self.model._Booster = booster  # Inject the booster into the LGBMClassifier\n",
    "\n",
    "    def model_exists(self):\n",
    "        try:\n",
    "            with open(self.model_path, 'r'):\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "# Assuming xult and yult are your feature matrix and target vector\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# yyyyyyy.shape \n",
    "for i in range(5):\n",
    "    # Initialize the LightGBM classifier model\n",
    "    if i % 2 == 0:\n",
    "        lgbm_rf_model = LGBMRandomForestClassifierModel(model_path=f'lgbm_rf_model{i}.txt')\n",
    "    else:\n",
    "        lgbm_rf_model = LGBMRandomForestClassifierModel(eval_metric='auc', model_path=f'lgbm_rf_model{i}.txt')\n",
    "    # Fit the model on training data\n",
    "    ktrain(lgbm_rf_model, xult, yult, epochs=1, batch_size=32, splits=5, random_state=i)\n",
    "    ens.add_model(lgbm_rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.29%\n",
      "Percentage of predictions less than or equal to 0.5: 88.71%\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of predictions greater than 0.5: 11.41%\n",
      "Percentage of predictions less than or equal to 0.5: 88.59%\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.48%\n",
      "Percentage of predictions less than or equal to 0.5: 88.52%\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.56%\n",
      "Percentage of predictions less than or equal to 0.5: 88.44%\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.56%\n",
      "Percentage of predictions less than or equal to 0.5: 88.44%\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.63%\n",
      "Percentage of predictions less than or equal to 0.5: 88.37%\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.66%\n",
      "Percentage of predictions less than or equal to 0.5: 88.34%\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.69%\n",
      "Percentage of predictions less than or equal to 0.5: 88.31%\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.72%\n",
      "Percentage of predictions less than or equal to 0.5: 88.28%\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.73%\n",
      "Percentage of predictions less than or equal to 0.5: 88.27%\n",
      "\u001b[32mPredicting with encoding_dim 512...\u001b[0m\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 890us/step\n",
      "Percentage of predictions greater than 0.5: 9.59%\n",
      "Percentage of predictions less than or equal to 0.5: 90.41%\n",
      "\u001b[32mPredicting with encoding_dim 256...\u001b[0m\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step\n",
      "Percentage of predictions greater than 0.5: 11.76%\n",
      "Percentage of predictions less than or equal to 0.5: 88.24%\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step\n",
      "Percentage of predictions greater than 0.5: 11.40%\n",
      "Percentage of predictions less than or equal to 0.5: 88.60%\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 662us/step\n",
      "Percentage of predictions greater than 0.5: 12.19%\n",
      "Percentage of predictions less than or equal to 0.5: 87.81%\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 10.46%\n",
      "Percentage of predictions less than or equal to 0.5: 89.54%\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 10.46%\n",
      "Percentage of predictions less than or equal to 0.5: 89.54%\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 10.56%\n",
      "Percentage of predictions less than or equal to 0.5: 89.44%\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 10.64%\n",
      "Percentage of predictions less than or equal to 0.5: 89.36%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 10.80%\n",
      "Percentage of predictions less than or equal to 0.5: 89.20%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 10.62%\n",
      "Percentage of predictions less than or equal to 0.5: 89.38%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 10.66%\n",
      "Percentage of predictions less than or equal to 0.5: 89.34%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 10.63%\n",
      "Percentage of predictions less than or equal to 0.5: 89.37%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 10.73%\n",
      "Percentage of predictions less than or equal to 0.5: 89.27%\n"
     ]
    }
   ],
   "source": [
    "ens.save(testx, 'finals.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
