{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load saved bm.keras and optimize it and then also train on test data and train data then upload final \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#default ml libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sklearn as sk\n",
    "#tf libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import kfold from sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test.csv file\n",
    "DEV = True\n",
    "loaddata = pd.read_csv('train.csv')\n",
    "testdata = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  person_age  person_income person_home_ownership  person_emp_length  \\\n",
      "0   0          37          35000                  RENT                0.0   \n",
      "1   1          22          56000                   OWN                6.0   \n",
      "2   2          29          28800                   OWN                8.0   \n",
      "3   3          30          70000                  RENT               14.0   \n",
      "4   4          22          60000                  RENT                2.0   \n",
      "\n",
      "  loan_intent loan_grade  loan_amnt  loan_int_rate  loan_percent_income  \\\n",
      "0   EDUCATION          B       6000          11.49                 0.17   \n",
      "1     MEDICAL          C       4000          13.35                 0.07   \n",
      "2    PERSONAL          A       6000           8.90                 0.21   \n",
      "3     VENTURE          B      12000          11.11                 0.17   \n",
      "4     MEDICAL          A       6000           6.92                 0.10   \n",
      "\n",
      "  cb_person_default_on_file  cb_person_cred_hist_length  loan_status  \n",
      "0                         N                          14            0  \n",
      "1                         N                           2            0  \n",
      "2                         N                          10            0  \n",
      "3                         N                           5            0  \n",
      "4                         N                           3            0  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      id  person_age  person_income person_home_ownership  person_emp_length  \\\n",
      "0  58645          23          69000                  RENT                3.0   \n",
      "1  58646          26          96000              MORTGAGE                6.0   \n",
      "2  58647          26          30000                  RENT                5.0   \n",
      "3  58648          33          50000                  RENT                4.0   \n",
      "4  58649          26         102000              MORTGAGE                8.0   \n",
      "\n",
      "         loan_intent loan_grade  loan_amnt  loan_int_rate  \\\n",
      "0    HOMEIMPROVEMENT          F      25000          15.76   \n",
      "1           PERSONAL          C      10000          12.68   \n",
      "2            VENTURE          E       4000          17.19   \n",
      "3  DEBTCONSOLIDATION          A       7000           8.90   \n",
      "4    HOMEIMPROVEMENT          D      15000          16.32   \n",
      "\n",
      "   loan_percent_income cb_person_default_on_file  cb_person_cred_hist_length  \n",
      "0                 0.36                         N                           2  \n",
      "1                 0.10                         Y                           4  \n",
      "2                 0.13                         Y                           2  \n",
      "3                 0.14                         N                           7  \n",
      "4                 0.15                         Y                           4  \n"
     ]
    }
   ],
   "source": [
    "train = loaddata.copy()\n",
    "print(train.head())\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "test = testdata.copy()\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120294/4078911324.py:68: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['person_home_ownership'] = test['person_home_ownership'].replace({'RENT': 0, 'MORTGAGE': 1, 'OWN': 2, 'OTHER': 3}).astype('category')\n",
      "/tmp/ipykernel_120294/4078911324.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['loan_intent'] = test['loan_intent'].replace({'EDUCATION': 0, 'MEDICAL': 1, 'PERSONAL': 2, 'VENTURE': 3, 'DEBTCONSOLIDATION': 4, 'HOMEIMPROVEMENT': 5}).astype('category')\n",
      "/tmp/ipykernel_120294/4078911324.py:70: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['loan_grade'] = test['loan_grade'].replace({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}).astype('category')\n",
      "/tmp/ipykernel_120294/4078911324.py:71: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['cb_person_default_on_file'] = test['cb_person_default_on_file'].replace({'N': 0, 'Y': 1}).astype('category')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, ..., 3136, 3137, 3138, 3139, 3140]\n",
       "Length: 3141\n",
       "Categories (3141, int64): [0, 1, 2, 3, ..., 3137, 3138, 3139, 3140]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature engineering\n",
    "test[\"loantoincome\"] = ((test[\"loan_amnt\"] / test[\"person_income\"])).astype('Float64')\n",
    "test[\"loan_percent_incometoincome\"] = ((test[\"loan_percent_income\"] / test[\"person_income\"])).astype('Float64')\n",
    "test['person_age_to_person_income'] = (test['person_age'] / test['person_income']).astype(str).astype('Float64')\n",
    "test['person_emp_length_to_person_age'] = (test['person_emp_length'] / test['person_age']).astype('Float64')\n",
    "test['loan_int_rate_to_loan_amnt'] = (test['loan_int_rate'] / test['loan_amnt']).astype('Float64')\n",
    "\n",
    "\n",
    "\n",
    "test['income_to_age'] = test['person_income'] / test['person_age']\n",
    "test['loan_to_income'] = test['loan_amnt'] / test['person_income']\n",
    "test['rate_to_loan'] = test['loan_int_rate'] / test['loan_amnt']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test['log_income'] = (np.log1p(test['person_income'])).astype('Float64')\n",
    "test['age_credit_history_interaction'] = (test['person_age'] * test['cb_person_cred_hist_length']).astype('float64')\n",
    "test['high_loan_to_income'] = (test['loan_percent_income'] > 0.5).astype('float64')\n",
    "test['is_new_credit_user'] = (test['cb_person_cred_hist_length'] < 2).astype('float64')\n",
    "test['high_interest_rate'] = (test['loan_int_rate'] > test['loan_int_rate'].mean()).astype('float64')\n",
    "test['loan_to_employment'] = test['loan_amnt'] / (test['person_emp_length'] + 1)\n",
    "test['rate_to_grade'] = test.groupby('loan_grade')['loan_int_rate'].transform('mean')\n",
    "\n",
    "\n",
    "def categorize_age(age):\n",
    "    if age <= 25:\n",
    "        return 0\n",
    "    elif age <= 35:\n",
    "        return 1\n",
    "    elif age <= 45:\n",
    "        return 2\n",
    "    elif age <= 55:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# test['income_category'] = pd.qcut(test['person_income'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "# test['intent_grade_interaction'] = test['loan_intent'].astype(str) + '_' + test['loan_grade'].astype(str)\n",
    "# test['home_ownership_intent'] = test['person_home_ownership'].astype(str) + '_' + test['loan_intent'].astype(str)\n",
    "\n",
    "# Function to calculate and store quantiles\n",
    "\n",
    "\n",
    "\n",
    "test['age_category'] = test['person_age'].apply(categorize_age).astype('category')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test['person_home_ownership'] = test['person_home_ownership'].replace({'RENT': 0, 'MORTGAGE': 1, 'OWN': 2, 'OTHER': 3}).astype('category')\n",
    "test['loan_intent'] = test['loan_intent'].replace({'EDUCATION': 0, 'MEDICAL': 1, 'PERSONAL': 2, 'VENTURE': 3, 'DEBTCONSOLIDATION': 4, 'HOMEIMPROVEMENT': 5}).astype('category')\n",
    "test['loan_grade'] = test['loan_grade'].replace({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}).astype('category')\n",
    "test['cb_person_default_on_file'] = test['cb_person_default_on_file'].replace({'N': 0, 'Y': 1}).astype('category')\n",
    "test[\"person_home_ownership_income\"] = pd.Series(pd.factorize((test[\"person_home_ownership\"].astype(str) + test[\"person_income\"].astype(str)).to_numpy())[0]).astype('category')\n",
    "\n",
    "test['person_home_ownership_income'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120294/699656101.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['person_home_ownership'] = train['person_home_ownership'].replace({'RENT': 0, 'MORTGAGE': 1, 'OWN': 2, 'OTHER': 3}).astype('category')\n",
      "/tmp/ipykernel_120294/699656101.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['loan_intent'] = train['loan_intent'].replace({'EDUCATION': 0, 'MEDICAL': 1, 'PERSONAL': 2, 'VENTURE': 3, 'DEBTCONSOLIDATION': 4, 'HOMEIMPROVEMENT': 5}).astype('category')\n",
      "/tmp/ipykernel_120294/699656101.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['loan_grade'] = train['loan_grade'].replace({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}).astype('category')\n",
      "/tmp/ipykernel_120294/699656101.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['cb_person_default_on_file'] = train['cb_person_default_on_file'].replace({'N': 0, 'Y': 1}).astype('category')\n",
      "/tmp/ipykernel_120294/699656101.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  train['rate_to_grade'] = train.groupby('loan_grade')['loan_int_rate'].transform('mean').astype('float64')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01, 0.12, 0.20, 0.31, 0.10, ..., 0.16, 0.56, 0.26, 0.06, 0.46]\n",
       "Length: 42\n",
       "Categories (42, float64): [0.00, 0.01, 0.02, 0.03, ..., 0.53, 0.54, 0.55, 0.56]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature engineering\n",
    "train[\"loantoincome\"] = ((train[\"loan_amnt\"] / train[\"person_income\"])).astype('Float64')\n",
    "train[\"loan_percent_incometoincome\"] = ((train[\"loan_percent_income\"] / train[\"person_income\"])).astype('Float64')\n",
    "train['person_age_to_person_income'] = (train['person_age'] / train['person_income']).astype(str).astype('Float64')\n",
    "train['person_emp_length_to_person_age'] = (train['person_emp_length'] / train['person_age']).astype('Float64')\n",
    "train['loan_int_rate_to_loan_amnt'] = (train['loan_int_rate'] / train['loan_amnt']).astype('Float64')\n",
    "\n",
    "\n",
    "train['income_to_age'] = train['person_income'] / train['person_age']\n",
    "train['rate_to_loan'] = train['loan_int_rate'] / train['loan_amnt']\n",
    "train['person_home_ownership'] = train['person_home_ownership'].replace({'RENT': 0, 'MORTGAGE': 1, 'OWN': 2, 'OTHER': 3}).astype('category')\n",
    "train['loan_intent'] = train['loan_intent'].replace({'EDUCATION': 0, 'MEDICAL': 1, 'PERSONAL': 2, 'VENTURE': 3, 'DEBTCONSOLIDATION': 4, 'HOMEIMPROVEMENT': 5}).astype('category')\n",
    "train['loan_grade'] = train['loan_grade'].replace({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}).astype('category')\n",
    "train['cb_person_default_on_file'] = train['cb_person_default_on_file'].replace({'N': 0, 'Y': 1}).astype('category')\n",
    "train[\"person_home_ownership_income\"] = pd.Series(pd.factorize((train[\"person_home_ownership\"].astype(str) + train[\"person_income\"].astype(str)).to_numpy())[0]).astype('category')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train['log_income'] = np.log1p(train['person_income']).astype('float64')\n",
    "train['age_credit_history_interaction'] = (train['person_age'] * train['cb_person_cred_hist_length']).astype('float64')\n",
    "train['high_loan_to_income'] = (train['loan_percent_income'] > 0.5).astype('float64')\n",
    "train['is_new_credit_user'] = (train['cb_person_cred_hist_length'] < 2).astype('float64')\n",
    "train['high_interest_rate'] = (train['loan_int_rate'] > train['loan_int_rate'].mean()).astype('float64')\n",
    "train['loan_to_employment'] = (train['loan_amnt'] / (train['person_emp_length'] + 1)).astype('float64')\n",
    "train['rate_to_grade'] = train.groupby('loan_grade')['loan_int_rate'].transform('mean').astype('float64')\n",
    "train['age_category'] = train['person_age'].apply(categorize_age).astype('category')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df['risk_score'] = df['loan_percent_income'] * df['loan_int_rate'] * (5 - df['loan_grade'].map({'A':5, 'B':4, 'C':3, 'D':2, 'E':1, 'F':0, 'G':0}))\n",
    "# df['age_to_credit_history'] = df['person_age'] / (df['cb_person_cred_hist_length'] + 1)\n",
    "# df['income_home_mismatch'] = ((df['person_income'] > df['person_income'].quantile(0.8)) & (df['person_home_ownership'] == 'RENT')).astype(int)\n",
    "# df['default_grade_interaction'] = df['cb_person_default_on_file'].astype(str) + '_' + df['loan_grade'].astype(str)\n",
    "# df['normalized_loan_amount'] = df.groupby('loan_intent')['loan_amnt'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "# df['income_to_loan'] = df['person_income'] / df['loan_amnt']\n",
    "# df['log_loan_amnt'] = np.log1p(df['loan_amnt'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate and store quantiles in a dictionary\n",
    "def calculate_quantiles(data, column):\n",
    "    quantiles = {}\n",
    "    for q in [0.2, 0.4, 0.6, 0.8]:\n",
    "        quantiles[f'q{int(q*100)}'] = data[column].quantile(q)\n",
    "    return quantiles\n",
    "\n",
    "# Calculate quantiles for the train dataset\n",
    "income_quantiles = calculate_quantiles(train, 'person_income')\n",
    "\n",
    "# Function to categorize income using cached quantiles\n",
    "def categorize_income(income, quantiles):\n",
    "    if income <= quantiles['q20']:\n",
    "        return 0\n",
    "    elif income <= quantiles['q40']:\n",
    "        return 0.1\n",
    "    elif income <= quantiles['q60']:\n",
    "        return 0.2\n",
    "    elif income <= quantiles['q80']:\n",
    "        return 0.3\n",
    "    else:\n",
    "        return 0.4\n",
    "\n",
    "# Apply the categorize_income function to create the income_category column\n",
    "train['income_category'] = train['person_income'].apply(categorize_income, args=(income_quantiles,)).astype('category')\n",
    "test['income_category'] = test['person_income'].apply(categorize_income, args=(income_quantiles,)).astype('category')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add the transformed columns\n",
    "\n",
    "\n",
    "train['loan_intent_grade'] = ((train['loan_intent'].astype('float64') * 10 + train['loan_grade'].astype('float64'))/100).astype('category')\n",
    "test['loan_intent_grade'] = ((test['loan_intent'].astype('float64') * 10 + test['loan_grade'].astype('float64'))/100).astype('category')\n",
    " \n",
    "\n",
    "# more feature engineering\n",
    "\n",
    "train['loan_intent_grade'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all files that start with lgbm and xgb \n",
    "\n",
    "for item in os.listdir():\n",
    "    if item.startswith(\"lgbm\") or item.startswith(\"xgb\"):\n",
    "        os.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_status\n",
      "0    50295\n",
      "1     8350\n",
      "Name: count, dtype: int64\n",
      "loan_status\n",
      "0    0.857618\n",
      "1    0.142382\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Plot histograms for numerical columns\n",
    "numerical_columns = ['person_age','loantoincome','person_emp_length_to_person_age',\n",
    "                     'loan_int_rate_to_loan_amnt','loan_percent_incometoincome',\n",
    "                     'person_age_to_person_income','person_income', 'loan_amnt',\"person_emp_length\" ,\n",
    "                     'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length','income_to_age','rate_to_loan','log_income',\n",
    "                     'age_credit_history_interaction','high_loan_to_income','is_new_credit_user','high_interest_rate','loan_to_employment','rate_to_grade',\n",
    "                     ]\n",
    "train[numerical_columns].hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot count plots for categorical columns\n",
    "categorical_columns = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file', 'loan_status',\n",
    "    \"person_home_ownership_income\",\n",
    "    'age_category',\n",
    "    'loan_intent_grade',\n",
    "    'income_category',\n",
    "    # 'intent_grade_interaction','home_ownership_intent'\n",
    "]\n",
    "numerical_features = numerical_columns\n",
    "categorical_features = categorical_columns\n",
    "if  not  DEV:\n",
    "    fig, axes = plt.subplots(math.ceil(len(categorical_columns)/2)+1, 2, figsize=(15, 15))  # Adjusted to 3x2 grid\n",
    "    for ax, col in zip(axes.flatten(), categorical_columns):\n",
    "        sns.countplot(data=train, x=col, ax=ax)\n",
    "        ax.set_title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "features = numerical_columns + categorical_columns \n",
    "categorical_columns.remove('loan_status')\n",
    "features.remove('loan_status')\n",
    "# Define the preprocessing for numerical and categorical features\n",
    "#print how many 'load_status' 0 and 1 and find the ratio\n",
    "print(train['loan_status'].value_counts())\n",
    "print(train['loan_status'].value_counts(normalize=True))\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58645, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_home_ownership</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_intent</th>\n",
       "      <th>loan_grade</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>loantoincome</th>\n",
       "      <th>loan_percent_incometoincome</th>\n",
       "      <th>person_age_to_person_income</th>\n",
       "      <th>person_emp_length_to_person_age</th>\n",
       "      <th>loan_int_rate_to_loan_amnt</th>\n",
       "      <th>income_to_age</th>\n",
       "      <th>rate_to_loan</th>\n",
       "      <th>person_home_ownership_income</th>\n",
       "      <th>log_income</th>\n",
       "      <th>age_credit_history_interaction</th>\n",
       "      <th>high_loan_to_income</th>\n",
       "      <th>is_new_credit_user</th>\n",
       "      <th>high_interest_rate</th>\n",
       "      <th>loan_to_employment</th>\n",
       "      <th>rate_to_grade</th>\n",
       "      <th>age_category</th>\n",
       "      <th>income_category</th>\n",
       "      <th>loan_intent_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>35000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6000</td>\n",
       "      <td>11.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>945.945946</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0</td>\n",
       "      <td>10.463132</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>11.034733</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>56000</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4000</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>2545.454545</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>1</td>\n",
       "      <td>10.933125</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>571.428571</td>\n",
       "      <td>13.510343</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>28800</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>8.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>993.103448</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>2</td>\n",
       "      <td>10.268165</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>666.666667</td>\n",
       "      <td>7.335176</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>70000</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12000</td>\n",
       "      <td>11.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>2333.333333</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>3</td>\n",
       "      <td>11.156265</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>11.034733</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  person_age  person_income person_home_ownership  person_emp_length  \\\n",
       "0   0          37          35000                     0                0.0   \n",
       "1   1          22          56000                     2                6.0   \n",
       "2   2          29          28800                     2                8.0   \n",
       "3   3          30          70000                     0               14.0   \n",
       "\n",
       "  loan_intent loan_grade  loan_amnt  loan_int_rate  loan_percent_income  \\\n",
       "0           0          1       6000          11.49                 0.17   \n",
       "1           1          2       4000          13.35                 0.07   \n",
       "2           2          0       6000           8.90                 0.21   \n",
       "3           3          1      12000          11.11                 0.17   \n",
       "\n",
       "  cb_person_default_on_file  cb_person_cred_hist_length  loan_status  \\\n",
       "0                         0                          14            0   \n",
       "1                         0                           2            0   \n",
       "2                         0                          10            0   \n",
       "3                         0                           5            0   \n",
       "\n",
       "   loantoincome  loan_percent_incometoincome  person_age_to_person_income  \\\n",
       "0      0.171429                     0.000005                     0.001057   \n",
       "1      0.071429                     0.000001                     0.000393   \n",
       "2      0.208333                     0.000007                     0.001007   \n",
       "3      0.171429                     0.000002                     0.000429   \n",
       "\n",
       "   person_emp_length_to_person_age  loan_int_rate_to_loan_amnt  income_to_age  \\\n",
       "0                              0.0                    0.001915     945.945946   \n",
       "1                         0.272727                    0.003337    2545.454545   \n",
       "2                         0.275862                    0.001483     993.103448   \n",
       "3                         0.466667                    0.000926    2333.333333   \n",
       "\n",
       "   rate_to_loan person_home_ownership_income  log_income  \\\n",
       "0      0.001915                            0   10.463132   \n",
       "1      0.003337                            1   10.933125   \n",
       "2      0.001483                            2   10.268165   \n",
       "3      0.000926                            3   11.156265   \n",
       "\n",
       "   age_credit_history_interaction  high_loan_to_income  is_new_credit_user  \\\n",
       "0                           518.0                  0.0                 0.0   \n",
       "1                            44.0                  0.0                 0.0   \n",
       "2                           290.0                  0.0                 0.0   \n",
       "3                           150.0                  0.0                 0.0   \n",
       "\n",
       "   high_interest_rate  loan_to_employment  rate_to_grade age_category  \\\n",
       "0                 1.0         6000.000000      11.034733            2   \n",
       "1                 1.0          571.428571      13.510343            0   \n",
       "2                 0.0          666.666667       7.335176            1   \n",
       "3                 1.0          800.000000      11.034733            1   \n",
       "\n",
       "  income_category loan_intent_grade  \n",
       "0             0.0              0.01  \n",
       "1             0.2              0.12  \n",
       "2             0.0              0.20  \n",
       "3             0.3              0.31  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train.head(4))\n",
    "#print all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(train.shape)\n",
    "train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person_age',\n",
       " 'loantoincome',\n",
       " 'person_emp_length_to_person_age',\n",
       " 'loan_int_rate_to_loan_amnt',\n",
       " 'loan_percent_incometoincome',\n",
       " 'person_age_to_person_income',\n",
       " 'person_income',\n",
       " 'loan_amnt',\n",
       " 'person_emp_length',\n",
       " 'loan_int_rate',\n",
       " 'loan_percent_income',\n",
       " 'cb_person_cred_hist_length',\n",
       " 'income_to_age',\n",
       " 'rate_to_loan',\n",
       " 'log_income',\n",
       " 'age_credit_history_interaction',\n",
       " 'high_loan_to_income',\n",
       " 'is_new_credit_user',\n",
       " 'high_interest_rate',\n",
       " 'loan_to_employment',\n",
       " 'rate_to_grade',\n",
       " 'person_home_ownership',\n",
       " 'loan_intent',\n",
       " 'loan_grade',\n",
       " 'cb_person_default_on_file',\n",
       " 'person_home_ownership_income',\n",
       " 'age_category',\n",
       " 'loan_intent_grade',\n",
       " 'income_category']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((58645, 29), (58645, 1), (39098, 29), (39098,))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "target = ['loan_status']  # Replace with the actual target column name\n",
    "\n",
    "# Preprocess the data\n",
    "X = train[features]\n",
    "y = train[target]\n",
    "ids = train['id']\n",
    "testx = test[features]\n",
    "test_ids = test['id']\n",
    "\n",
    "X.shape , y.shape , testx.shape,  test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140748, 29)\n",
      "(35187, 29)\n",
      "(140748, 1)\n",
      "(35187, 1)\n",
      "(140748,)\n",
      "(35187,)\n",
      "(39098, 29)\n",
      "(39098,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(X, y, ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    # ('onehot', OneHotEncoder(handle_unknown='error'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "testx = preprocessor.transform(testx)\n",
    "\n",
    "# Function to add noise\n",
    "def add_noise(data, noise_level=0.01):\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "# Add noise to the numerical features\n",
    "X_train_noisy = X_train.copy()\n",
    "X_test_noisy = X_test.copy()\n",
    "X_train_noisy[:, :len(numerical_features)] = add_noise(X_train[:, :len(numerical_features)])\n",
    "X_test_noisy[:, :len(numerical_features)] = add_noise(X_test[:, :len(numerical_features)])\n",
    "\n",
    "\n",
    "X_train_less_noise = X_train.copy()\n",
    "X_test_less_noise = X_test.copy()\n",
    "X_train_less_noise[:, :len(numerical_features)] = add_noise(X_train[:, :len(numerical_features)],noise_level=0.001)\n",
    "X_test_less_noise[:, :len(numerical_features)] = add_noise(X_test[:, :len(numerical_features)],noise_level=0.001)\n",
    "\n",
    "# Concatenate the original data with the noisy data vertically\n",
    "X_train_combined = np.vstack((X_train, X_train_noisy, X_train_less_noise))\n",
    "X_test_combined = np.vstack((X_test, X_test_noisy, X_test_less_noise))\n",
    "\n",
    "# Concatenate the target variable as well\n",
    "y_train_combined = np.vstack((y_train, y_train, y_train))\n",
    "y_test_combined = np.vstack((y_test, y_test,y_test))\n",
    "\n",
    "# Concatenate the ids as well\n",
    "ids_train_combined = np.hstack((ids_train, ids_train, ids_train))\n",
    "ids_test_combined = np.hstack((ids_test, ids_test, ids_test))\n",
    "\n",
    "# Update the original variables\n",
    "X_train = X_train_combined\n",
    "X_test = X_test_combined\n",
    "y_train = y_train_combined\n",
    "y_test = y_test_combined\n",
    "ids_train = ids_train_combined\n",
    "ids_test = ids_test_combined\n",
    "xult , yult  , idsult= np.vstack((X_train, X_test)), np.vstack((y_train, y_test)) , np.hstack((ids_train, ids_test))\n",
    "print(X_train.shape)  # Should output (46916 + 46916, 26)\n",
    "print(X_test.shape)   # Should output (11729 + 11729, 26)\n",
    "print(y_train.shape)  # Should output (46916 + 46916, 1)\n",
    "print(y_test.shape)   # Should output (11729 + 11729, 1)\n",
    "print(ids_train.shape)  # Should output (46916 + 46916,)\n",
    "print(ids_test.shape)   # Should output (11729 + 11729,)\n",
    "print(testx.shape)   # Should output (11729 + 11729,)\n",
    "print(test_ids.shape)   # Should output (11729 + 11729,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmax(value=False):\n",
    "    #see if fmax.txt exists and if it does then read the value and return it\n",
    "    #if it does not exist then return 0\n",
    "    #if value exists then save it to fmax.txt\n",
    "    if(value):\n",
    "        try:\n",
    "            with open(\"fmax.txt\", \"w\") as f:\n",
    "                f.write(str(value))\n",
    "            return 0    \n",
    "        except:\n",
    "            return 0\n",
    "    try:\n",
    "        with open(\"fmax.txt\", \"r\") as f:\n",
    "            return float(f.read())\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ensemble:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        \n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            pred = model.predict(X_test)\n",
    "            predictions.append(pred)\n",
    "            count_greater_than_0_5 = (pred > model.THRESHOLD).sum()\n",
    "            count_less_than_or_equal_0_5 = (pred <= model.THRESHOLD).sum()\n",
    "            print(f'Percentage of predictions greater than {model.THRESHOLD}: {count_greater_than_0_5 / len(pred) * 100:.2f}%')\n",
    "            passc = count_greater_than_0_5 / len(pred) * 100\n",
    "            if passc < 5:\n",
    "                predictions.pop()\n",
    "                continue\n",
    "            print(f'Percentage of predictions less than or equal to {model.THRESHOLD}: {count_less_than_or_equal_0_5 / len(pred) * 100:.2f}%')\n",
    "        \n",
    "        # Stack predictions to form a 2D array\n",
    "        stacked_predictions = np.hstack(predictions)\n",
    "        \n",
    "        # Average the predictions across models\n",
    "        y_pred = np.mean(stacked_predictions, axis=1)\n",
    "        \n",
    "        # Apply threshold\n",
    "        # y_pred = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        # Flatten the predictions to form a 1D array\n",
    "        y_pred = y_pred.flatten()\n",
    "        \n",
    "        # Assuming test_ids is defined elsewhere in your code\n",
    "        ids = test_ids\n",
    "        predictions_df = pd.DataFrame({'id': ids, 'loan_status': y_pred})\n",
    "        return predictions_df\n",
    "    \n",
    "    def save(self, testx, path=\"ftt.csv\"):\n",
    "        df = self.predict(testx)\n",
    "        df.to_csv(path, index=False)\n",
    "        csvfile =  pd.read_csv(path)\n",
    "        csvfile\n",
    "        \n",
    "    def rmamodel(self):\n",
    "        self.models = []\n",
    "\n",
    "ens = ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ktrain( model , xult , yult,splits=5,epochs=15,batch_size=32,random_state=42):\n",
    "    \n",
    "    if splits==1:\n",
    "        class temp:\n",
    "            def split(self, X):\n",
    "                n_samples = len(X)\n",
    "                indices = list(range(n_samples))\n",
    "                yield indices, indices  # Use the same indices for train and test\n",
    "\n",
    "        kf = temp()\n",
    "    else:\n",
    "        kf = KFold(n_splits=splits, shuffle=True, random_state=random_state)\n",
    "        obj = kf \n",
    "    losses, aucs, precisions, recalls, f1s, roc_aucs = [], [], [], [], [], []\n",
    "    iter = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        xt, xv = xult[train_index], xult[test_index]\n",
    "        yt, yv = yult[train_index], yult[test_index]\n",
    "        \n",
    "        # Initialize and compile the model\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(xt, yt, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        results = model.evaluate(xv, yv)\n",
    "        loss, auc, precision, recall = results[0], results[1], results[2], results[3]\n",
    "        \n",
    "        # Predict probabilities for the test set\n",
    "        y_pred_prob = model.predict(xult)\n",
    "        y_pred = (y_pred_prob > model.THRESHOLD).astype(int)\n",
    "        \n",
    "        # Calculate F1 score and ROC AUC score\n",
    "        f1 = f1_score(yult, y_pred)\n",
    "        #count which iteration is this\n",
    "        count = train_index[0]\n",
    "        roc_auc = roc_auc_score(yult, y_pred_prob)\n",
    "        if f1 > fmax():\n",
    "            model.f1max(iter,epochs,splits)\n",
    "            fmax(f1)\n",
    "            model.save('best.keras')\n",
    "            print(colored(f'F1 Score improved to {f1}. Saving model...', 'green','on_red'))\n",
    "        # Store metrics\n",
    "        losses.append(loss)\n",
    "        aucs.append(auc)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        roc_aucs.append(roc_auc)\n",
    "        iter += 1\n",
    "    print('losses: ', losses)\n",
    "    print('aucs: ', aucs)\n",
    "    print('precisions: ', precisions)\n",
    "    print('recalls: ', recalls)\n",
    "    print('f1s: ', f1s)\n",
    "    print('roc_aucs: ', roc_aucs)\n",
    "    print(f'Average Loss: {sum(losses) / len(losses)}')\n",
    "    print(f'Average AUC: {sum(aucs) / len(aucs)}')\n",
    "    print(f'Average Precision: {sum(precisions) / len(precisions)}')\n",
    "    print(f'Average Recall: {sum(recalls) / len(recalls)}')\n",
    "    print(f'Average F1 Score: {sum(f1s) / len(f1s)}')\n",
    "    print(f'Average ROC AUC Score: {sum(roc_aucs) / len(roc_aucs)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[9924  102]\n",
      " [ 439 1264]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10035    59]\n",
      " [  259  1376]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10040    40]\n",
      " [  198  1451]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[10095     8]\n",
      " [  104  1522]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "[[9977    2]\n",
      " [  60 1690]]\n",
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9635698212246516, 0.9904694647075898, 0.9955431743143993, 0.9992129345721846, 0.9997770747140423]\n",
      "precisions:  [0.9253294289897511, 0.9588850174216028, 0.9731723675385647, 0.9947712418300654, 0.9988179669030733]\n",
      "recalls:  [0.7422196124486201, 0.8415902140672783, 0.8799272286234081, 0.9360393603936039, 0.9657142857142857]\n",
      "f1s:  [0.8517373670941576, 0.8814458558888865, 0.9020024658815526, 0.919002919002919, 0.9253424944488667]\n",
      "roc_aucs:  [0.883970113257926, 0.9049614757957989, 0.9209015590451466, 0.9344666249566677, 0.938866700439463]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9897144939065734\n",
      "Average Precision: 0.9701952045366115\n",
      "Average Recall: 0.8730981402494393\n",
      "Average F1 Score: 0.8959062204632764\n",
      "Average ROC AUC Score: 0.9166332946990006\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class XGBoostClassifierModel:\n",
    "    def __init__(self, eval_metric='logloss', model_path='xgb_model.json', **kwargs):\n",
    "        self.model = xgb.XGBClassifier(objective='binary:logistic', eval_metric=eval_metric, **kwargs)\n",
    "        self.model_path = model_path\n",
    "        self.THRESHOLD = 0.5\n",
    "        self.load_model()\n",
    "        self.f1 = 0 # Placeholder for F1 score\n",
    "\n",
    "    def fit(self, X_train, y_train, verbose=0, **kwargs):\n",
    "        self.model.fit(X_train, y_train, verbose=verbose, xgb_model=self.model_path if self.model_exists() else None)\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(colored('predicting using xgboost model', 'green'))\n",
    "        return self.model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        y_pred_proba = self.predict_proba(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        # Assuming loss is not directly available, using log loss as a proxy\n",
    "        loss = -1  # Placeholder, as XGBoost does not directly provide loss in this context\n",
    "        return [loss, auc, precision, recall]\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model)\n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "    def save_model(self):\n",
    "        self.model.save_model(self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_exists():\n",
    "            self.model.load_model(self.model_path)\n",
    "\n",
    "    def model_exists(self):\n",
    "        try:\n",
    "            with open(self.model_path, 'r'):\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "    def save(self, path):\n",
    "        self.model.save_model(path)\n",
    "\n",
    "# Example usage\n",
    "# Assuming xult and yult are your feature matrix and target vector\n",
    "# Split the data into training and test sets\n",
    "\n",
    "for i in range(1):\n",
    "    if i%2==0:\n",
    "        xgb_model = XGBoostClassifierModel(model_path=f'xgb_modelss.json')\n",
    "    else:\n",
    "        xgb_model = XGBoostClassifierModel(eval_metric='auc', model_path=f'xgb_model.json')\n",
    "    ktrain(xgb_model, xult, yult, epochs=10, batch_size=32, splits=5, random_state=i)\n",
    "    ens.add_model(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - auc_14: 0.7001 - loss: 0.4335 - precision_14: 0.3551 - recall_14: 0.0619 - val_auc_14: 0.8803 - val_loss: 0.2694 - val_precision_14: 0.8659 - val_recall_14: 0.2647\n",
      "Epoch 2/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - auc_14: 0.8688 - loss: 0.2741 - precision_14: 0.7598 - recall_14: 0.3823 - val_auc_14: 0.9005 - val_loss: 0.2810 - val_precision_14: 0.5969 - val_recall_14: 0.7174\n",
      "Epoch 3/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc_14: 0.8910 - loss: 0.2572 - precision_14: 0.7659 - recall_14: 0.4695 - val_auc_14: 0.9035 - val_loss: 0.2599 - val_precision_14: 0.8466 - val_recall_14: 0.3603\n",
      "Epoch 4/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc_14: 0.8985 - loss: 0.2443 - precision_14: 0.7874 - recall_14: 0.5131 - val_auc_14: 0.9076 - val_loss: 0.2465 - val_precision_14: 0.8712 - val_recall_14: 0.3985\n",
      "Epoch 5/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - auc_14: 0.9083 - loss: 0.2306 - precision_14: 0.8027 - recall_14: 0.5357 - val_auc_14: 0.8970 - val_loss: 0.2519 - val_precision_14: 0.8340 - val_recall_14: 0.4479\n",
      "Epoch 6/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc_14: 0.8995 - loss: 0.2461 - precision_14: 0.7783 - recall_14: 0.5066 - val_auc_14: 0.9112 - val_loss: 0.2383 - val_precision_14: 0.8311 - val_recall_14: 0.4447\n",
      "Epoch 7/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc_14: 0.9016 - loss: 0.2390 - precision_14: 0.7783 - recall_14: 0.5171 - val_auc_14: 0.9028 - val_loss: 0.2400 - val_precision_14: 0.7529 - val_recall_14: 0.4889\n",
      "Epoch 8/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - auc_14: 0.9077 - loss: 0.2273 - precision_14: 0.7910 - recall_14: 0.5491 - val_auc_14: 0.9137 - val_loss: 0.2226 - val_precision_14: 0.8374 - val_recall_14: 0.5701\n",
      "Epoch 9/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc_14: 0.9114 - loss: 0.2257 - precision_14: 0.7962 - recall_14: 0.5691 - val_auc_14: 0.9161 - val_loss: 0.2283 - val_precision_14: 0.9047 - val_recall_14: 0.4232\n",
      "Epoch 10/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc_14: 0.9117 - loss: 0.2188 - precision_14: 0.8152 - recall_14: 0.5432 - val_auc_14: 0.9194 - val_loss: 0.2157 - val_precision_14: 0.8344 - val_recall_14: 0.5478\n",
      "Epoch 11/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - auc_14: 0.9139 - loss: 0.2222 - precision_14: 0.7833 - recall_14: 0.5844 - val_auc_14: 0.9190 - val_loss: 0.2133 - val_precision_14: 0.8038 - val_recall_14: 0.6051\n",
      "Epoch 12/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc_14: 0.9157 - loss: 0.2188 - precision_14: 0.8036 - recall_14: 0.5739 - val_auc_14: 0.9181 - val_loss: 0.2234 - val_precision_14: 0.8299 - val_recall_14: 0.5032\n",
      "Epoch 13/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - auc_14: 0.9171 - loss: 0.2170 - precision_14: 0.7918 - recall_14: 0.5906 - val_auc_14: 0.9172 - val_loss: 0.2358 - val_precision_14: 0.8879 - val_recall_14: 0.4383\n",
      "Epoch 14/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc_14: 0.9207 - loss: 0.2139 - precision_14: 0.8171 - recall_14: 0.5700 - val_auc_14: 0.9196 - val_loss: 0.2182 - val_precision_14: 0.8677 - val_recall_14: 0.4829\n",
      "Epoch 15/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - auc_14: 0.9198 - loss: 0.2131 - precision_14: 0.8340 - recall_14: 0.5646 - val_auc_14: 0.9268 - val_loss: 0.2034 - val_precision_14: 0.7771 - val_recall_14: 0.6704\n",
      "\u001b[1m1833/1833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_14: 0.9284 - loss: 0.1997 - precision_14: 0.7812 - recall_14: 0.6756\n",
      "\u001b[32mPredicting with encoding_dim 512...\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 892us/step\n",
      "losses:  [0.2007705420255661]\n",
      "aucs:  [0.9285780787467957]\n",
      "precisions:  [0.7813888788223267]\n",
      "recalls:  [0.6727250814437866]\n",
      "f1s:  [0.7232973216778711]\n",
      "roc_aucs:  [0.9290371383686549]\n",
      "Average Loss: 0.2007705420255661\n",
      "Average AUC: 0.9285780787467957\n",
      "Average Precision: 0.7813888788223267\n",
      "Average Recall: 0.6727250814437866\n",
      "Average F1 Score: 0.7232973216778711\n",
      "Average ROC AUC Score: 0.9290371383686549\n",
      "Epoch 1/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - auc_15: 0.6314 - loss: 0.4221 - precision_15: 0.4530 - recall_15: 0.0840 - val_auc_15: 0.7692 - val_loss: 0.3409 - val_precision_15: 0.6897 - val_recall_15: 0.2663\n",
      "Epoch 2/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.8060 - loss: 0.3159 - precision_15: 0.7519 - recall_15: 0.2589 - val_auc_15: 0.8592 - val_loss: 0.2790 - val_precision_15: 0.7593 - val_recall_15: 0.4132\n",
      "Epoch 3/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.8643 - loss: 0.2757 - precision_15: 0.7624 - recall_15: 0.3990 - val_auc_15: 0.8773 - val_loss: 0.2609 - val_precision_15: 0.7331 - val_recall_15: 0.5052\n",
      "Epoch 4/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.8720 - loss: 0.2612 - precision_15: 0.7699 - recall_15: 0.4625 - val_auc_15: 0.8886 - val_loss: 0.2495 - val_precision_15: 0.7876 - val_recall_15: 0.4873\n",
      "Epoch 5/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.8799 - loss: 0.2607 - precision_15: 0.7633 - recall_15: 0.4666 - val_auc_15: 0.9030 - val_loss: 0.2366 - val_precision_15: 0.7218 - val_recall_15: 0.5971\n",
      "Epoch 6/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.8959 - loss: 0.2434 - precision_15: 0.7668 - recall_15: 0.5092 - val_auc_15: 0.9046 - val_loss: 0.2394 - val_precision_15: 0.8127 - val_recall_15: 0.4873\n",
      "Epoch 7/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.8938 - loss: 0.2496 - precision_15: 0.8061 - recall_15: 0.4859 - val_auc_15: 0.9016 - val_loss: 0.2445 - val_precision_15: 0.6847 - val_recall_15: 0.6051\n",
      "Epoch 8/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.8953 - loss: 0.2408 - precision_15: 0.7692 - recall_15: 0.5095 - val_auc_15: 0.9097 - val_loss: 0.2309 - val_precision_15: 0.7455 - val_recall_15: 0.5669\n",
      "Epoch 9/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.9059 - loss: 0.2333 - precision_15: 0.7910 - recall_15: 0.5403 - val_auc_15: 0.9126 - val_loss: 0.2263 - val_precision_15: 0.8462 - val_recall_15: 0.5080\n",
      "Epoch 10/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.9112 - loss: 0.2220 - precision_15: 0.8080 - recall_15: 0.5523 - val_auc_15: 0.9104 - val_loss: 0.2511 - val_precision_15: 0.6273 - val_recall_15: 0.7062\n",
      "Epoch 11/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.9143 - loss: 0.2208 - precision_15: 0.8023 - recall_15: 0.5679 - val_auc_15: 0.9180 - val_loss: 0.2193 - val_precision_15: 0.8660 - val_recall_15: 0.5171\n",
      "Epoch 12/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.9141 - loss: 0.2232 - precision_15: 0.8127 - recall_15: 0.5634 - val_auc_15: 0.9162 - val_loss: 0.2324 - val_precision_15: 0.6668 - val_recall_15: 0.7178\n",
      "Epoch 13/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.9154 - loss: 0.2188 - precision_15: 0.8053 - recall_15: 0.5955 - val_auc_15: 0.9206 - val_loss: 0.2115 - val_precision_15: 0.8597 - val_recall_15: 0.5537\n",
      "Epoch 14/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.9080 - loss: 0.2276 - precision_15: 0.8238 - recall_15: 0.5494 - val_auc_15: 0.9197 - val_loss: 0.2101 - val_precision_15: 0.8347 - val_recall_15: 0.5828\n",
      "Epoch 15/15\n",
      "\u001b[1m1283/1283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_15: 0.9168 - loss: 0.2147 - precision_15: 0.8215 - recall_15: 0.5785 - val_auc_15: 0.9194 - val_loss: 0.2152 - val_precision_15: 0.7919 - val_recall_15: 0.6091\n",
      "\u001b[1m1833/1833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 966us/step - auc_15: 0.9201 - loss: 0.2123 - precision_15: 0.7976 - recall_15: 0.6112\n",
      "\u001b[32mPredicting with encoding_dim 256...\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 755us/step\n",
      "losses:  [0.21330036222934723]\n",
      "aucs:  [0.9203411340713501]\n",
      "precisions:  [0.800407350063324]\n",
      "recalls:  [0.6109051704406738]\n",
      "f1s:  [0.6922937689488212]\n",
      "roc_aucs:  [0.9209253829551356]\n",
      "Average Loss: 0.21330036222934723\n",
      "Average AUC: 0.9203411340713501\n",
      "Average Precision: 0.800407350063324\n",
      "Average Recall: 0.6109051704406738\n",
      "Average F1 Score: 0.6922937689488212\n",
      "Average ROC AUC Score: 0.9209253829551356\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from colorama import init\n",
    "from termcolor import colored\n",
    "# Define the L2 regularizers\n",
    "\n",
    "\n",
    "k = .0001\n",
    "kernel_regularizer = l2(0.0001*k)\n",
    "bias_regularizer = l2(0.00001*k)\n",
    "\n",
    "# Define the CNN model with different regularization strengths for kernel and bias\n",
    "class AutoencoderModel:\n",
    "    def __init__(self, input_dim, encoding_dim=512):\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.autoencoder = self.build_model()\n",
    "        self.THRESHOLD = 0.5\n",
    "    def build_model(self):\n",
    "        # Encoder\n",
    "        input_layer = Input(shape=(self.input_dim,))\n",
    "        encoded = Dense(self.encoding_dim, activation='relu')(input_layer)\n",
    "        encoded = Dense(self.encoding_dim // 2, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        encoded = Dense(self.encoding_dim // 4, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        encoded = Dense(self.encoding_dim // 8, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        encoded = Dense(self.encoding_dim // 16, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        encoded = Dense(self.encoding_dim // 32, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "    \n",
    "        # Decoder\n",
    "        decoded = Dense(self.encoding_dim // 32, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        decoded = Dense(self.encoding_dim // 16, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        decoded = Dense(self.encoding_dim // 8, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(encoded)\n",
    "        decoded = Dense(self.encoding_dim // 4, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(decoded)\n",
    "        decoded = Dense(self.encoding_dim // 2, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(decoded)\n",
    "        decoded = Dense(1, activation='sigmoid')(decoded)\n",
    "    \n",
    "        # Autoencoder\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "        # Compile the model with Binary Crossentropy loss\n",
    "        autoencoder.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=[AUC(), Precision(), Recall()])    \n",
    "        return autoencoder\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=10, verbose=0, batch_size=32, validation_split=0.3, validation_data=None):\n",
    "        if validation_data is not None:\n",
    "            self.autoencoder.fit(X_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, validation_data=validation_data, validation_split=validation_split)\n",
    "            return \n",
    "        self.autoencoder.fit(X_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, validation_split=validation_split)\n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "    def predict(self, X_test):\n",
    "        print(colored(f'Predicting with encoding_dim {self.encoding_dim}...', 'green'))\n",
    "        return self.autoencoder.predict(X_test)\n",
    "    \n",
    "    def summary(self):\n",
    "        self.autoencoder.summary()\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        return self.autoencoder.evaluate(X_test, y_test)\n",
    "    def save(self, path):\n",
    "        self.autoencoder.save(path)\n",
    "\n",
    "dim = [512, 256, 128, 64, 256]\n",
    "for i in range(2):\n",
    "    automodel = AutoencoderModel(X_train.shape[1], encoding_dim=dim[i])\n",
    "    ktrain(automodel, xult, yult, splits=1, epochs=15, batch_size=32, random_state=i)\n",
    "    ens.add_model(automodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - auc_16: 0.5646 - loss: 0.5959 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_auc_16: 0.6620 - val_loss: 0.3768 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.7089 - loss: 0.3695 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_auc_16: 0.7992 - val_loss: 0.3372 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.7152 - loss: 0.3648 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_auc_16: 0.8259 - val_loss: 0.3292 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8107 - loss: 0.3282 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_auc_16: 0.8028 - val_loss: 0.3258 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8066 - loss: 0.3375 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_auc_16: 0.8162 - val_loss: 0.3337 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - auc_16: 0.8173 - loss: 0.3188 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_auc_16: 0.8262 - val_loss: 0.3300 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8437 - loss: 0.3004 - precision_16: 0.0000e+00 - recall_16: 0.0000e+00 - val_auc_16: 0.8710 - val_loss: 0.2814 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8512 - loss: 0.2966 - precision_16: 0.5659 - recall_16: 0.2595 - val_auc_16: 0.8814 - val_loss: 0.2761 - val_precision_16: 0.0000e+00 - val_recall_16: 0.0000e+00\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8665 - loss: 0.2821 - precision_16: 0.4086 - recall_16: 0.1545 - val_auc_16: 0.8220 - val_loss: 0.3488 - val_precision_16: 0.3538 - val_recall_16: 0.8295\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8576 - loss: 0.2895 - precision_16: 0.4848 - recall_16: 0.5806 - val_auc_16: 0.8914 - val_loss: 0.2631 - val_precision_16: 0.6523 - val_recall_16: 0.6581\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8659 - loss: 0.2841 - precision_16: 0.5008 - recall_16: 0.5284 - val_auc_16: 0.8671 - val_loss: 0.2926 - val_precision_16: 0.4757 - val_recall_16: 0.7885\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8712 - loss: 0.2797 - precision_16: 0.5599 - recall_16: 0.6063 - val_auc_16: 0.8950 - val_loss: 0.2576 - val_precision_16: 0.6480 - val_recall_16: 0.6803\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8744 - loss: 0.2818 - precision_16: 0.5475 - recall_16: 0.6104 - val_auc_16: 0.8994 - val_loss: 0.2568 - val_precision_16: 0.7132 - val_recall_16: 0.5983\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.8965 - loss: 0.2544 - precision_16: 0.7570 - recall_16: 0.5393 - val_auc_16: 0.8959 - val_loss: 0.2594 - val_precision_16: 0.6394 - val_recall_16: 0.6981\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9050 - loss: 0.2430 - precision_16: 0.7557 - recall_16: 0.5600 - val_auc_16: 0.9004 - val_loss: 0.2483 - val_precision_16: 0.7443 - val_recall_16: 0.5968\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - auc_16: 0.9004 - loss: 0.2489 - precision_16: 0.7608 - recall_16: 0.5980\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 668us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9000 - loss: 0.2507 - precision_16: 0.7695 - recall_16: 0.5476 - val_auc_16: 0.9083 - val_loss: 0.2385 - val_precision_16: 0.8003 - val_recall_16: 0.5534\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9080 - loss: 0.2366 - precision_16: 0.7883 - recall_16: 0.5395 - val_auc_16: 0.9070 - val_loss: 0.2377 - val_precision_16: 0.7516 - val_recall_16: 0.6269\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9064 - loss: 0.2411 - precision_16: 0.7609 - recall_16: 0.5611 - val_auc_16: 0.9095 - val_loss: 0.2361 - val_precision_16: 0.7456 - val_recall_16: 0.6041\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9114 - loss: 0.2286 - precision_16: 0.7975 - recall_16: 0.5576 - val_auc_16: 0.9094 - val_loss: 0.2320 - val_precision_16: 0.8105 - val_recall_16: 0.5460\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9115 - loss: 0.2302 - precision_16: 0.8156 - recall_16: 0.5587 - val_auc_16: 0.9150 - val_loss: 0.2290 - val_precision_16: 0.8520 - val_recall_16: 0.4948\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_16: 0.9129 - loss: 0.2286 - precision_16: 0.8356 - recall_16: 0.5365 - val_auc_16: 0.9141 - val_loss: 0.2383 - val_precision_16: 0.8924 - val_recall_16: 0.4203\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9156 - loss: 0.2256 - precision_16: 0.8185 - recall_16: 0.5479 - val_auc_16: 0.9163 - val_loss: 0.2229 - val_precision_16: 0.8566 - val_recall_16: 0.5132\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9157 - loss: 0.2205 - precision_16: 0.8281 - recall_16: 0.5444 - val_auc_16: 0.9187 - val_loss: 0.2195 - val_precision_16: 0.8150 - val_recall_16: 0.5668\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9163 - loss: 0.2191 - precision_16: 0.8339 - recall_16: 0.5493 - val_auc_16: 0.9162 - val_loss: 0.2194 - val_precision_16: 0.8182 - val_recall_16: 0.6036\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9174 - loss: 0.2142 - precision_16: 0.8287 - recall_16: 0.5640 - val_auc_16: 0.9121 - val_loss: 0.2323 - val_precision_16: 0.8812 - val_recall_16: 0.4496\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9209 - loss: 0.2144 - precision_16: 0.8437 - recall_16: 0.5562 - val_auc_16: 0.9172 - val_loss: 0.2242 - val_precision_16: 0.7699 - val_recall_16: 0.6185\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9179 - loss: 0.2217 - precision_16: 0.8413 - recall_16: 0.5467 - val_auc_16: 0.9201 - val_loss: 0.2219 - val_precision_16: 0.8817 - val_recall_16: 0.4739\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_16: 0.9187 - loss: 0.2184 - precision_16: 0.8352 - recall_16: 0.5592 - val_auc_16: 0.9207 - val_loss: 0.2140 - val_precision_16: 0.8498 - val_recall_16: 0.5479\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9183 - loss: 0.2161 - precision_16: 0.8333 - recall_16: 0.5685 - val_auc_16: 0.9174 - val_loss: 0.2233 - val_precision_16: 0.7503 - val_recall_16: 0.6031\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9221 - loss: 0.2150 - precision_16: 0.8184 - recall_16: 0.5736 - val_auc_16: 0.9205 - val_loss: 0.2141 - val_precision_16: 0.7954 - val_recall_16: 0.6180\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - auc_16: 0.9198 - loss: 0.2103 - precision_16: 0.7880 - recall_16: 0.6344\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 650us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_16: 0.9244 - loss: 0.2099 - precision_16: 0.8231 - recall_16: 0.5868 - val_auc_16: 0.9264 - val_loss: 0.2108 - val_precision_16: 0.8391 - val_recall_16: 0.5677\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9248 - loss: 0.2074 - precision_16: 0.8394 - recall_16: 0.5878 - val_auc_16: 0.9252 - val_loss: 0.2055 - val_precision_16: 0.8298 - val_recall_16: 0.6013\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9270 - loss: 0.2068 - precision_16: 0.8342 - recall_16: 0.5796 - val_auc_16: 0.9230 - val_loss: 0.2112 - val_precision_16: 0.8214 - val_recall_16: 0.5582\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9263 - loss: 0.2087 - precision_16: 0.8416 - recall_16: 0.5842 - val_auc_16: 0.9281 - val_loss: 0.2055 - val_precision_16: 0.8859 - val_recall_16: 0.5216\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9259 - loss: 0.2089 - precision_16: 0.8350 - recall_16: 0.5741 - val_auc_16: 0.9288 - val_loss: 0.2036 - val_precision_16: 0.8594 - val_recall_16: 0.5702\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9228 - loss: 0.2122 - precision_16: 0.8302 - recall_16: 0.5851 - val_auc_16: 0.9256 - val_loss: 0.2068 - val_precision_16: 0.8268 - val_recall_16: 0.6058\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9265 - loss: 0.2063 - precision_16: 0.8087 - recall_16: 0.6139 - val_auc_16: 0.9219 - val_loss: 0.2211 - val_precision_16: 0.6740 - val_recall_16: 0.7247\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9270 - loss: 0.2045 - precision_16: 0.8071 - recall_16: 0.6174 - val_auc_16: 0.9285 - val_loss: 0.2088 - val_precision_16: 0.8537 - val_recall_16: 0.5471\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9288 - loss: 0.2033 - precision_16: 0.8228 - recall_16: 0.6328 - val_auc_16: 0.9267 - val_loss: 0.2053 - val_precision_16: 0.8624 - val_recall_16: 0.5502\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9282 - loss: 0.2017 - precision_16: 0.8226 - recall_16: 0.6254 - val_auc_16: 0.9259 - val_loss: 0.2167 - val_precision_16: 0.7783 - val_recall_16: 0.6163\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9294 - loss: 0.2016 - precision_16: 0.8322 - recall_16: 0.6392 - val_auc_16: 0.9271 - val_loss: 0.2038 - val_precision_16: 0.8523 - val_recall_16: 0.5873\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9301 - loss: 0.2018 - precision_16: 0.8336 - recall_16: 0.6294 - val_auc_16: 0.9277 - val_loss: 0.2021 - val_precision_16: 0.8324 - val_recall_16: 0.6053\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9273 - loss: 0.2038 - precision_16: 0.8160 - recall_16: 0.6271 - val_auc_16: 0.9239 - val_loss: 0.2114 - val_precision_16: 0.8414 - val_recall_16: 0.5612\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9268 - loss: 0.2016 - precision_16: 0.8196 - recall_16: 0.6251 - val_auc_16: 0.9291 - val_loss: 0.2083 - val_precision_16: 0.8689 - val_recall_16: 0.5652\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9311 - loss: 0.1966 - precision_16: 0.8311 - recall_16: 0.6438 - val_auc_16: 0.9262 - val_loss: 0.2131 - val_precision_16: 0.8444 - val_recall_16: 0.5171\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - auc_16: 0.9173 - loss: 0.2190 - precision_16: 0.8221 - recall_16: 0.5118\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 657us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_16: 0.9244 - loss: 0.2115 - precision_16: 0.8170 - recall_16: 0.5961 - val_auc_16: 0.9257 - val_loss: 0.2062 - val_precision_16: 0.8057 - val_recall_16: 0.6372\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9272 - loss: 0.2019 - precision_16: 0.8209 - recall_16: 0.6271 - val_auc_16: 0.9284 - val_loss: 0.2047 - val_precision_16: 0.8523 - val_recall_16: 0.6033\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_16: 0.9275 - loss: 0.2073 - precision_16: 0.8102 - recall_16: 0.6070 - val_auc_16: 0.9278 - val_loss: 0.2022 - val_precision_16: 0.8272 - val_recall_16: 0.6298\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9280 - loss: 0.2027 - precision_16: 0.8234 - recall_16: 0.6138 - val_auc_16: 0.9260 - val_loss: 0.2142 - val_precision_16: 0.7725 - val_recall_16: 0.6568\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9330 - loss: 0.1999 - precision_16: 0.8352 - recall_16: 0.6287 - val_auc_16: 0.9261 - val_loss: 0.2113 - val_precision_16: 0.7468 - val_recall_16: 0.7153\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9316 - loss: 0.1990 - precision_16: 0.8221 - recall_16: 0.6248 - val_auc_16: 0.9285 - val_loss: 0.2012 - val_precision_16: 0.8322 - val_recall_16: 0.6352\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9286 - loss: 0.2052 - precision_16: 0.8277 - recall_16: 0.6081 - val_auc_16: 0.9279 - val_loss: 0.2049 - val_precision_16: 0.7999 - val_recall_16: 0.6495\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9316 - loss: 0.1973 - precision_16: 0.8201 - recall_16: 0.6161 - val_auc_16: 0.9262 - val_loss: 0.2060 - val_precision_16: 0.8493 - val_recall_16: 0.5920\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9338 - loss: 0.1988 - precision_16: 0.8288 - recall_16: 0.6268 - val_auc_16: 0.9261 - val_loss: 0.2175 - val_precision_16: 0.8735 - val_recall_16: 0.4914\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_16: 0.9328 - loss: 0.1974 - precision_16: 0.8308 - recall_16: 0.6249 - val_auc_16: 0.9269 - val_loss: 0.2090 - val_precision_16: 0.7487 - val_recall_16: 0.6888\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9324 - loss: 0.2000 - precision_16: 0.8175 - recall_16: 0.6341 - val_auc_16: 0.9283 - val_loss: 0.2043 - val_precision_16: 0.8158 - val_recall_16: 0.6328\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9247 - loss: 0.2081 - precision_16: 0.8284 - recall_16: 0.6001 - val_auc_16: 0.9272 - val_loss: 0.2075 - val_precision_16: 0.8021 - val_recall_16: 0.6505\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9353 - loss: 0.1925 - precision_16: 0.8291 - recall_16: 0.6455 - val_auc_16: 0.9294 - val_loss: 0.1996 - val_precision_16: 0.8604 - val_recall_16: 0.6019\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9330 - loss: 0.1938 - precision_16: 0.8211 - recall_16: 0.6271 - val_auc_16: 0.9306 - val_loss: 0.2015 - val_precision_16: 0.8640 - val_recall_16: 0.5896\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9341 - loss: 0.1967 - precision_16: 0.8171 - recall_16: 0.6282 - val_auc_16: 0.9311 - val_loss: 0.2025 - val_precision_16: 0.8606 - val_recall_16: 0.5970\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - auc_16: 0.9295 - loss: 0.2014 - precision_16: 0.8672 - recall_16: 0.6225\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 660us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_16: 0.9349 - loss: 0.1944 - precision_16: 0.8324 - recall_16: 0.6477 - val_auc_16: 0.9270 - val_loss: 0.2048 - val_precision_16: 0.7747 - val_recall_16: 0.6191\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9367 - loss: 0.1907 - precision_16: 0.8266 - recall_16: 0.6402 - val_auc_16: 0.9297 - val_loss: 0.2074 - val_precision_16: 0.8829 - val_recall_16: 0.5185\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9374 - loss: 0.1888 - precision_16: 0.8282 - recall_16: 0.6114 - val_auc_16: 0.9311 - val_loss: 0.1965 - val_precision_16: 0.8245 - val_recall_16: 0.6535\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9350 - loss: 0.1917 - precision_16: 0.8402 - recall_16: 0.6362 - val_auc_16: 0.9311 - val_loss: 0.2033 - val_precision_16: 0.7906 - val_recall_16: 0.6550\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9376 - loss: 0.1882 - precision_16: 0.8434 - recall_16: 0.6214 - val_auc_16: 0.9322 - val_loss: 0.1934 - val_precision_16: 0.8324 - val_recall_16: 0.6555\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9377 - loss: 0.1894 - precision_16: 0.8146 - recall_16: 0.6460 - val_auc_16: 0.9290 - val_loss: 0.2305 - val_precision_16: 0.6531 - val_recall_16: 0.7360\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9315 - loss: 0.2002 - precision_16: 0.8332 - recall_16: 0.6020 - val_auc_16: 0.9307 - val_loss: 0.2048 - val_precision_16: 0.7539 - val_recall_16: 0.6925\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9391 - loss: 0.1915 - precision_16: 0.8306 - recall_16: 0.6447 - val_auc_16: 0.9328 - val_loss: 0.1939 - val_precision_16: 0.8700 - val_recall_16: 0.5923\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9385 - loss: 0.1874 - precision_16: 0.8405 - recall_16: 0.6286 - val_auc_16: 0.9339 - val_loss: 0.1975 - val_precision_16: 0.7672 - val_recall_16: 0.6884\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9381 - loss: 0.1877 - precision_16: 0.8182 - recall_16: 0.6433 - val_auc_16: 0.9342 - val_loss: 0.1906 - val_precision_16: 0.8587 - val_recall_16: 0.6302\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9389 - loss: 0.1865 - precision_16: 0.8436 - recall_16: 0.6442 - val_auc_16: 0.9344 - val_loss: 0.1933 - val_precision_16: 0.8610 - val_recall_16: 0.6045\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_16: 0.9416 - loss: 0.1840 - precision_16: 0.8379 - recall_16: 0.6482 - val_auc_16: 0.9332 - val_loss: 0.1967 - val_precision_16: 0.8370 - val_recall_16: 0.6206\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9385 - loss: 0.1890 - precision_16: 0.8344 - recall_16: 0.6390 - val_auc_16: 0.9322 - val_loss: 0.2017 - val_precision_16: 0.7549 - val_recall_16: 0.7011\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_16: 0.9424 - loss: 0.1824 - precision_16: 0.8321 - recall_16: 0.6556 - val_auc_16: 0.9323 - val_loss: 0.1970 - val_precision_16: 0.7708 - val_recall_16: 0.6975\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_16: 0.9395 - loss: 0.1896 - precision_16: 0.8121 - recall_16: 0.6511 - val_auc_16: 0.9347 - val_loss: 0.1930 - val_precision_16: 0.8651 - val_recall_16: 0.5938\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - auc_16: 0.9227 - loss: 0.2072 - precision_16: 0.8350 - recall_16: 0.5674\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 662us/step\n",
      "losses:  [0.24672053754329681, 0.20786647498607635, 0.21901686489582062, 0.19732630252838135, 0.21026529371738434]\n",
      "aucs:  [0.9024674296379089, 0.9243715405464172, 0.9167316555976868, 0.930705726146698, 0.9267702698707581]\n",
      "precisions:  [0.7699248194694519, 0.792438268661499, 0.8427299857139587, 0.8588744401931763, 0.8553406000137329]\n",
      "recalls:  [0.601291835308075, 0.628134548664093, 0.5166767835617065, 0.6100860834121704, 0.5811428427696228]\n",
      "f1s:  [0.6656428746299882, 0.7046741100396036, 0.6530884313435047, 0.7122504452151092, 0.7093280595187644]\n",
      "roc_aucs:  [0.9038536489667713, 0.9253319035786002, 0.9268172671457958, 0.9333824195066804, 0.935737317888331]\n",
      "Average Loss: 0.2162390947341919\n",
      "Average AUC: 0.9202093243598938\n",
      "Average Precision: 0.8238616228103638\n",
      "Average Recall: 0.5874664187431335\n",
      "Average F1 Score: 0.688996784149394\n",
      "Average ROC AUC Score: 0.9250245114172359\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - auc_17: 0.7310 - loss: 0.4304 - precision_17: 0.6917 - recall_17: 0.1079 - val_auc_17: 0.8265 - val_loss: 0.3770 - val_precision_17: 0.8642 - val_recall_17: 0.1660\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.8372 - loss: 0.3082 - precision_17: 0.8088 - recall_17: 0.2472 - val_auc_17: 0.8265 - val_loss: 0.3175 - val_precision_17: 0.7673 - val_recall_17: 0.2944\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.8512 - loss: 0.2957 - precision_17: 0.8122 - recall_17: 0.3051 - val_auc_17: 0.8586 - val_loss: 0.2856 - val_precision_17: 0.7938 - val_recall_17: 0.3862\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.8639 - loss: 0.2806 - precision_17: 0.7941 - recall_17: 0.3223 - val_auc_17: 0.8791 - val_loss: 0.2690 - val_precision_17: 0.7867 - val_recall_17: 0.2904\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_17: 0.8694 - loss: 0.2746 - precision_17: 0.8284 - recall_17: 0.2823 - val_auc_17: 0.8927 - val_loss: 0.2508 - val_precision_17: 0.8145 - val_recall_17: 0.4514\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.8852 - loss: 0.2545 - precision_17: 0.8016 - recall_17: 0.4158 - val_auc_17: 0.8936 - val_loss: 0.2459 - val_precision_17: 0.7911 - val_recall_17: 0.4920\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.8966 - loss: 0.2492 - precision_17: 0.7677 - recall_17: 0.4959 - val_auc_17: 0.9026 - val_loss: 0.2378 - val_precision_17: 0.7372 - val_recall_17: 0.6118\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.8908 - loss: 0.2494 - precision_17: 0.7869 - recall_17: 0.4663 - val_auc_17: 0.9111 - val_loss: 0.2304 - val_precision_17: 0.8455 - val_recall_17: 0.4639\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9043 - loss: 0.2388 - precision_17: 0.8082 - recall_17: 0.5261 - val_auc_17: 0.9044 - val_loss: 0.2362 - val_precision_17: 0.8480 - val_recall_17: 0.4870\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.8970 - loss: 0.2470 - precision_17: 0.7890 - recall_17: 0.5023 - val_auc_17: 0.9120 - val_loss: 0.2258 - val_precision_17: 0.8172 - val_recall_17: 0.5491\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9097 - loss: 0.2286 - precision_17: 0.8100 - recall_17: 0.5523 - val_auc_17: 0.9074 - val_loss: 0.2357 - val_precision_17: 0.7259 - val_recall_17: 0.6469\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9060 - loss: 0.2326 - precision_17: 0.8061 - recall_17: 0.5351 - val_auc_17: 0.9120 - val_loss: 0.2305 - val_precision_17: 0.8611 - val_recall_17: 0.4819\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9131 - loss: 0.2181 - precision_17: 0.8211 - recall_17: 0.5581 - val_auc_17: 0.9120 - val_loss: 0.2610 - val_precision_17: 0.6021 - val_recall_17: 0.7407\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9103 - loss: 0.2294 - precision_17: 0.7941 - recall_17: 0.5261 - val_auc_17: 0.9105 - val_loss: 0.2301 - val_precision_17: 0.8796 - val_recall_17: 0.4433\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9108 - loss: 0.2236 - precision_17: 0.8171 - recall_17: 0.5417 - val_auc_17: 0.9141 - val_loss: 0.2219 - val_precision_17: 0.7777 - val_recall_17: 0.5878\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - auc_17: 0.9125 - loss: 0.2162 - precision_17: 0.7928 - recall_17: 0.6076\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 678us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9139 - loss: 0.2216 - precision_17: 0.8011 - recall_17: 0.5732 - val_auc_17: 0.9182 - val_loss: 0.2119 - val_precision_17: 0.8263 - val_recall_17: 0.5822\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9156 - loss: 0.2204 - precision_17: 0.8263 - recall_17: 0.5591 - val_auc_17: 0.9162 - val_loss: 0.2242 - val_precision_17: 0.7077 - val_recall_17: 0.6810\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9158 - loss: 0.2162 - precision_17: 0.8284 - recall_17: 0.5818 - val_auc_17: 0.9176 - val_loss: 0.2144 - val_precision_17: 0.8218 - val_recall_17: 0.5827\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_17: 0.9176 - loss: 0.2110 - precision_17: 0.8292 - recall_17: 0.5929 - val_auc_17: 0.9203 - val_loss: 0.2168 - val_precision_17: 0.8309 - val_recall_17: 0.5346\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9125 - loss: 0.2266 - precision_17: 0.8108 - recall_17: 0.5557 - val_auc_17: 0.9195 - val_loss: 0.2178 - val_precision_17: 0.7460 - val_recall_17: 0.6349\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9215 - loss: 0.2112 - precision_17: 0.8111 - recall_17: 0.5830 - val_auc_17: 0.9207 - val_loss: 0.2232 - val_precision_17: 0.8916 - val_recall_17: 0.4413\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9213 - loss: 0.2106 - precision_17: 0.8287 - recall_17: 0.5688 - val_auc_17: 0.9221 - val_loss: 0.2113 - val_precision_17: 0.7737 - val_recall_17: 0.6515\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9159 - loss: 0.2181 - precision_17: 0.8163 - recall_17: 0.5612 - val_auc_17: 0.9194 - val_loss: 0.2172 - val_precision_17: 0.7217 - val_recall_17: 0.6735\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9167 - loss: 0.2144 - precision_17: 0.8128 - recall_17: 0.5625 - val_auc_17: 0.9172 - val_loss: 0.2169 - val_precision_17: 0.7617 - val_recall_17: 0.6284\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9212 - loss: 0.2092 - precision_17: 0.8317 - recall_17: 0.5761 - val_auc_17: 0.9198 - val_loss: 0.2113 - val_precision_17: 0.8164 - val_recall_17: 0.6043\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9218 - loss: 0.2119 - precision_17: 0.8275 - recall_17: 0.5968 - val_auc_17: 0.9196 - val_loss: 0.2196 - val_precision_17: 0.9077 - val_recall_17: 0.4438\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9217 - loss: 0.2083 - precision_17: 0.8324 - recall_17: 0.5562 - val_auc_17: 0.9200 - val_loss: 0.2107 - val_precision_17: 0.8013 - val_recall_17: 0.6128\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9245 - loss: 0.2049 - precision_17: 0.8349 - recall_17: 0.5927 - val_auc_17: 0.9198 - val_loss: 0.2102 - val_precision_17: 0.7948 - val_recall_17: 0.6409\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9221 - loss: 0.2101 - precision_17: 0.8231 - recall_17: 0.5982 - val_auc_17: 0.9232 - val_loss: 0.2070 - val_precision_17: 0.7978 - val_recall_17: 0.6194\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9255 - loss: 0.2025 - precision_17: 0.8381 - recall_17: 0.6028 - val_auc_17: 0.9254 - val_loss: 0.2060 - val_precision_17: 0.7514 - val_recall_17: 0.6640\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - auc_17: 0.9191 - loss: 0.2186 - precision_17: 0.7393 - recall_17: 0.6467\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 666us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_17: 0.9177 - loss: 0.2102 - precision_17: 0.8193 - recall_17: 0.5812 - val_auc_17: 0.9266 - val_loss: 0.2044 - val_precision_17: 0.8127 - val_recall_17: 0.6278\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9237 - loss: 0.2075 - precision_17: 0.8190 - recall_17: 0.5650 - val_auc_17: 0.9274 - val_loss: 0.2053 - val_precision_17: 0.8835 - val_recall_17: 0.5542\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9224 - loss: 0.2077 - precision_17: 0.8430 - recall_17: 0.5716 - val_auc_17: 0.9233 - val_loss: 0.2113 - val_precision_17: 0.8315 - val_recall_17: 0.6027\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9250 - loss: 0.2017 - precision_17: 0.8197 - recall_17: 0.5980 - val_auc_17: 0.9268 - val_loss: 0.2040 - val_precision_17: 0.7792 - val_recall_17: 0.6905\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9224 - loss: 0.2054 - precision_17: 0.7963 - recall_17: 0.6180 - val_auc_17: 0.9260 - val_loss: 0.2115 - val_precision_17: 0.8771 - val_recall_17: 0.5213\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9200 - loss: 0.2069 - precision_17: 0.8226 - recall_17: 0.5938 - val_auc_17: 0.9257 - val_loss: 0.2077 - val_precision_17: 0.8441 - val_recall_17: 0.5895\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9229 - loss: 0.2063 - precision_17: 0.8138 - recall_17: 0.5969 - val_auc_17: 0.9242 - val_loss: 0.2196 - val_precision_17: 0.9021 - val_recall_17: 0.4924\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9274 - loss: 0.2024 - precision_17: 0.8260 - recall_17: 0.6100 - val_auc_17: 0.9270 - val_loss: 0.2042 - val_precision_17: 0.8416 - val_recall_17: 0.5969\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9239 - loss: 0.2058 - precision_17: 0.8232 - recall_17: 0.6078 - val_auc_17: 0.9249 - val_loss: 0.2101 - val_precision_17: 0.8026 - val_recall_17: 0.6420\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9293 - loss: 0.1982 - precision_17: 0.8323 - recall_17: 0.6291 - val_auc_17: 0.9270 - val_loss: 0.2060 - val_precision_17: 0.8981 - val_recall_17: 0.5056\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9299 - loss: 0.1984 - precision_17: 0.8389 - recall_17: 0.6177 - val_auc_17: 0.9289 - val_loss: 0.2015 - val_precision_17: 0.7911 - val_recall_17: 0.6778\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9211 - loss: 0.2072 - precision_17: 0.8139 - recall_17: 0.5903 - val_auc_17: 0.9271 - val_loss: 0.2048 - val_precision_17: 0.7862 - val_recall_17: 0.6582\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9283 - loss: 0.1978 - precision_17: 0.8128 - recall_17: 0.6171 - val_auc_17: 0.9293 - val_loss: 0.2024 - val_precision_17: 0.8518 - val_recall_17: 0.5920\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9310 - loss: 0.1990 - precision_17: 0.8219 - recall_17: 0.6067 - val_auc_17: 0.9250 - val_loss: 0.2116 - val_precision_17: 0.8018 - val_recall_17: 0.6189\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9266 - loss: 0.2019 - precision_17: 0.8063 - recall_17: 0.6107 - val_auc_17: 0.9262 - val_loss: 0.2101 - val_precision_17: 0.8677 - val_recall_17: 0.5532\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - auc_17: 0.9319 - loss: 0.1997 - precision_17: 0.8653 - recall_17: 0.5900\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 663us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9310 - loss: 0.1948 - precision_17: 0.8357 - recall_17: 0.6240 - val_auc_17: 0.9257 - val_loss: 0.2048 - val_precision_17: 0.8095 - val_recall_17: 0.6305\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9270 - loss: 0.2036 - precision_17: 0.8266 - recall_17: 0.6045 - val_auc_17: 0.9286 - val_loss: 0.2064 - val_precision_17: 0.9123 - val_recall_17: 0.5025\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9338 - loss: 0.1879 - precision_17: 0.8433 - recall_17: 0.6350 - val_auc_17: 0.9296 - val_loss: 0.1995 - val_precision_17: 0.8121 - val_recall_17: 0.6285\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9317 - loss: 0.1976 - precision_17: 0.8335 - recall_17: 0.6159 - val_auc_17: 0.9309 - val_loss: 0.1979 - val_precision_17: 0.8584 - val_recall_17: 0.5767\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9302 - loss: 0.1945 - precision_17: 0.8304 - recall_17: 0.6347 - val_auc_17: 0.9298 - val_loss: 0.2054 - val_precision_17: 0.8931 - val_recall_17: 0.5533\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9331 - loss: 0.1889 - precision_17: 0.8335 - recall_17: 0.6255 - val_auc_17: 0.9308 - val_loss: 0.1977 - val_precision_17: 0.8409 - val_recall_17: 0.6240\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9369 - loss: 0.1858 - precision_17: 0.8296 - recall_17: 0.6556 - val_auc_17: 0.9287 - val_loss: 0.2080 - val_precision_17: 0.7398 - val_recall_17: 0.7052\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9359 - loss: 0.1879 - precision_17: 0.8274 - recall_17: 0.6356 - val_auc_17: 0.9300 - val_loss: 0.2034 - val_precision_17: 0.8897 - val_recall_17: 0.5304\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9344 - loss: 0.1880 - precision_17: 0.8325 - recall_17: 0.6309 - val_auc_17: 0.9293 - val_loss: 0.1993 - val_precision_17: 0.7705 - val_recall_17: 0.7057\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9357 - loss: 0.1898 - precision_17: 0.8054 - recall_17: 0.6471 - val_auc_17: 0.9317 - val_loss: 0.1952 - val_precision_17: 0.8419 - val_recall_17: 0.6205\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_17: 0.9367 - loss: 0.1882 - precision_17: 0.8257 - recall_17: 0.6277 - val_auc_17: 0.9283 - val_loss: 0.2009 - val_precision_17: 0.8416 - val_recall_17: 0.6165\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9363 - loss: 0.1871 - precision_17: 0.8337 - recall_17: 0.6392 - val_auc_17: 0.9327 - val_loss: 0.1939 - val_precision_17: 0.8551 - val_recall_17: 0.6056\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - auc_17: 0.9399 - loss: 0.1812 - precision_17: 0.8390 - recall_17: 0.6537 - val_auc_17: 0.9313 - val_loss: 0.1960 - val_precision_17: 0.8079 - val_recall_17: 0.6599\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9379 - loss: 0.1855 - precision_17: 0.8323 - recall_17: 0.6477 - val_auc_17: 0.9309 - val_loss: 0.2024 - val_precision_17: 0.8491 - val_recall_17: 0.5717\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9371 - loss: 0.1858 - precision_17: 0.8262 - recall_17: 0.6329 - val_auc_17: 0.9326 - val_loss: 0.1944 - val_precision_17: 0.8679 - val_recall_17: 0.6086\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - auc_17: 0.9262 - loss: 0.2159 - precision_17: 0.8325 - recall_17: 0.5754\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 657us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9289 - loss: 0.1986 - precision_17: 0.8284 - recall_17: 0.6323 - val_auc_17: 0.9294 - val_loss: 0.2037 - val_precision_17: 0.8339 - val_recall_17: 0.5805\n",
      "Epoch 2/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9343 - loss: 0.1918 - precision_17: 0.8298 - recall_17: 0.6329 - val_auc_17: 0.9319 - val_loss: 0.1966 - val_precision_17: 0.8024 - val_recall_17: 0.6577\n",
      "Epoch 3/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9344 - loss: 0.1923 - precision_17: 0.8282 - recall_17: 0.6523 - val_auc_17: 0.9260 - val_loss: 0.2149 - val_precision_17: 0.8543 - val_recall_17: 0.5286\n",
      "Epoch 4/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9325 - loss: 0.1985 - precision_17: 0.8268 - recall_17: 0.6047 - val_auc_17: 0.9325 - val_loss: 0.1976 - val_precision_17: 0.8625 - val_recall_17: 0.5780\n",
      "Epoch 5/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9394 - loss: 0.1816 - precision_17: 0.8282 - recall_17: 0.6613 - val_auc_17: 0.9314 - val_loss: 0.2002 - val_precision_17: 0.8114 - val_recall_17: 0.6323\n",
      "Epoch 6/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - auc_17: 0.9368 - loss: 0.1879 - precision_17: 0.8258 - recall_17: 0.6442 - val_auc_17: 0.9314 - val_loss: 0.2007 - val_precision_17: 0.8385 - val_recall_17: 0.6104\n",
      "Epoch 7/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9410 - loss: 0.1845 - precision_17: 0.8234 - recall_17: 0.6757 - val_auc_17: 0.9307 - val_loss: 0.2009 - val_precision_17: 0.8016 - val_recall_17: 0.6542\n",
      "Epoch 8/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9338 - loss: 0.1904 - precision_17: 0.8237 - recall_17: 0.6488 - val_auc_17: 0.9329 - val_loss: 0.1975 - val_precision_17: 0.8402 - val_recall_17: 0.6129\n",
      "Epoch 9/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9373 - loss: 0.1845 - precision_17: 0.8342 - recall_17: 0.6485 - val_auc_17: 0.9265 - val_loss: 0.2167 - val_precision_17: 0.7441 - val_recall_17: 0.6811\n",
      "Epoch 10/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9355 - loss: 0.1893 - precision_17: 0.8163 - recall_17: 0.6514 - val_auc_17: 0.9310 - val_loss: 0.2044 - val_precision_17: 0.8649 - val_recall_17: 0.5456\n",
      "Epoch 11/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9344 - loss: 0.1903 - precision_17: 0.8365 - recall_17: 0.6328 - val_auc_17: 0.9320 - val_loss: 0.1961 - val_precision_17: 0.8242 - val_recall_17: 0.6423\n",
      "Epoch 12/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9380 - loss: 0.1870 - precision_17: 0.8371 - recall_17: 0.6663 - val_auc_17: 0.9303 - val_loss: 0.2027 - val_precision_17: 0.8857 - val_recall_17: 0.5326\n",
      "Epoch 13/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - auc_17: 0.9390 - loss: 0.1857 - precision_17: 0.8361 - recall_17: 0.6461 - val_auc_17: 0.9286 - val_loss: 0.1999 - val_precision_17: 0.8587 - val_recall_17: 0.5874\n",
      "Epoch 14/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9404 - loss: 0.1850 - precision_17: 0.8526 - recall_17: 0.6410 - val_auc_17: 0.9335 - val_loss: 0.1967 - val_precision_17: 0.8785 - val_recall_17: 0.5835\n",
      "Epoch 15/15\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - auc_17: 0.9420 - loss: 0.1824 - precision_17: 0.8355 - recall_17: 0.6676 - val_auc_17: 0.9324 - val_loss: 0.2003 - val_precision_17: 0.7937 - val_recall_17: 0.6672\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - auc_17: 0.9281 - loss: 0.1958 - precision_17: 0.7586 - recall_17: 0.6499\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m5498/5498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 649us/step\n",
      "losses:  [0.22289098799228668, 0.21403789520263672, 0.20200568437576294, 0.2093782126903534, 0.19567449390888214]\n",
      "aucs:  [0.9121267199516296, 0.921222984790802, 0.9281063079833984, 0.9258806109428406, 0.9308094382286072]\n",
      "precisions:  [0.7910685539245605, 0.7556623220443726, 0.8658758997917175, 0.8462170958518982, 0.7739380598068237]\n",
      "recalls:  [0.5925925970077515, 0.657706081867218, 0.5703125, 0.5913792848587036, 0.6672874093055725]\n",
      "f1s:  [0.6776351970669111, 0.7127890634981474, 0.6848902699553311, 0.7177817065227505, 0.7305790808649671]\n",
      "roc_aucs:  [0.9163042045279491, 0.9267448963953657, 0.9285497156927157, 0.9340454187889587, 0.9343514181829534]\n",
      "Average Loss: 0.20879745483398438\n",
      "Average AUC: 0.9236292123794556\n",
      "Average Precision: 0.8065523862838745\n",
      "Average Recall: 0.6158555746078491\n",
      "Average F1 Score: 0.7047350635816214\n",
      "Average ROC AUC Score: 0.9279991307175885\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from colorama import init\n",
    "from termcolor import colored\n",
    "\n",
    "# Define the L2 regularizers\n",
    "k = .0001\n",
    "kernel_regularizer = l2(0.0001*k)\n",
    "bias_regularizer = l2(0.00001*k)\n",
    "\n",
    "# Define the Dense model with different regularization strengths for kernel and bias\n",
    "class DenseModel:\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = self.build_model()\n",
    "        self.THRESHOLD = 0.5\n",
    "\n",
    "    def build_model(self):\n",
    "        input_layer = Input(shape=(self.input_dim,))\n",
    "        x = Dense(self.hidden_dim, activation='relu')(input_layer)\n",
    "        x = Dense(self.hidden_dim // 2, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        x = Dense(self.hidden_dim // 4, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        x = Dense(self.hidden_dim // 8, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        x = Dense(self.hidden_dim // 16, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        x = Dense(self.hidden_dim // 32, activation='relu', kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)(x)\n",
    "        output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # Compile the model with Binary Crossentropy loss\n",
    "        model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=[AUC(), Precision(), Recall()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=10, verbose=0, batch_size=32, validation_split=0.3, validation_data=None):\n",
    "        if validation_data is not None:\n",
    "            self.model.fit(X_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, validation_data=validation_data, validation_split=validation_split)\n",
    "            return \n",
    "        self.model.fit(X_train, y_train, epochs=epochs, verbose=verbose, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(colored(f'predicting using dense model of hidden {self.hidden_dim}', 'green'))\n",
    "        return self.model.predict(X_test)\n",
    "    \n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "        \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "# Example usage\n",
    "# Assuming xult and yult are your feature matrix and target vector\n",
    "for i in range(2):\n",
    "    dense_model = DenseModel(input_dim=xult.shape[1])\n",
    "    ktrain(dense_model, xult, yult,epochs=15,batch_size=32,splits=5,random_state=i)\n",
    "    ens.add_model(dense_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9949  140]\n",
      " [ 480 1160]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9947  151]\n",
      " [ 481 1150]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9931  103]\n",
      " [ 534 1161]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9933   80]\n",
      " [ 533 1183]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9973   75]\n",
      " [ 495 1186]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9226907353819301, 0.9411424083224134, 0.9488352580577071, 0.9533394174985107, 0.9567850640542294]\n",
      "precisions:  [0.8923076923076924, 0.883935434281322, 0.9185126582278481, 0.936658749010293, 0.9405233941316415]\n",
      "recalls:  [0.7073170731707317, 0.7050889025137952, 0.6849557522123894, 0.6893939393939394, 0.7055324211778703]\n",
      "f1s:  [0.771162957003665, 0.7796679000090244, 0.7886908810592663, 0.7946946923977145, 0.7994528043775649]\n",
      "roc_aucs:  [0.8362923343094109, 0.8382792244321379, 0.8416548491643813, 0.8416091249254151, 0.8457051063396618]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9445585766629581\n",
      "Average Precision: 0.9143875855917594\n",
      "Average Recall: 0.6984576176937451\n",
      "Average F1 Score: 0.7867338469694471\n",
      "Average ROC AUC Score: 0.8407081278342015\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9949  140]\n",
      " [ 480 1160]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9947  151]\n",
      " [ 481 1150]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9931  103]\n",
      " [ 534 1161]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9933   80]\n",
      " [ 533 1183]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "[[9973   75]\n",
      " [ 495 1186]]\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9226907353819301, 0.9411424083224134, 0.9488352580577071, 0.9533394174985107, 0.9567850640542294]\n",
      "precisions:  [0.8923076923076924, 0.883935434281322, 0.9185126582278481, 0.936658749010293, 0.9405233941316415]\n",
      "recalls:  [0.7073170731707317, 0.7050889025137952, 0.6849557522123894, 0.6893939393939394, 0.7055324211778703]\n",
      "f1s:  [0.771162957003665, 0.7796679000090244, 0.7886908810592663, 0.7946946923977145, 0.7994528043775649]\n",
      "roc_aucs:  [0.8362923343094109, 0.8382792244321379, 0.8416548491643813, 0.8416091249254151, 0.8457051063396618]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9445585766629581\n",
      "Average Precision: 0.9143875855917594\n",
      "Average Recall: 0.6984576176937451\n",
      "Average F1 Score: 0.7867338469694471\n",
      "Average ROC AUC Score: 0.8407081278342015\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class XGBRFClassifierModel:\n",
    "    def __init__(self, objective='binary:logistic', eval_metric='auc', n_estimators=600, max_depth=5, subsample=0.9,\n",
    "                 colsample_bynode=0.9, reg_alpha=0.1, reg_lambda=1.0, min_child_weight=1, random_state=42, model_path='xgbrf_model.json', **kwargs):\n",
    "        self.model = xgb.XGBRFClassifier(objective=objective, eval_metric=eval_metric, n_estimators=n_estimators,\n",
    "                                         max_depth=max_depth, subsample=subsample, colsample_bynode=colsample_bynode, reg_alpha=reg_alpha,\n",
    "                                         reg_lambda=reg_lambda, min_child_weight=min_child_weight, random_state=random_state, **kwargs)\n",
    "        self.model_path = model_path\n",
    "        self.THRESHOLD = 0.5\n",
    "        self.load_model()\n",
    "\n",
    "    def fit(self, X_train, y_train, verbose=0, **kwargs):\n",
    "        self.model.fit(X_train, y_train, verbose=verbose, xgb_model=self.model_path if self.model_exists() else None)\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(colored('predicting using xgbrf model', 'green'))\n",
    "        return self.model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        y_pred_proba = self.predict_proba(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        # Assuming loss is not directly available, using log loss as a proxy\n",
    "        loss = -1  # Placeholder, as XGBoost does not directly provide loss in this context\n",
    "        return [loss, auc, precision, recall]\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save_model(self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_exists():\n",
    "            self.model.load_model(self.model_path)\n",
    "            \n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "\n",
    "    def model_exists(self):\n",
    "        try:\n",
    "            with open(self.model_path, 'r'):\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "    def save(self, path):\n",
    "        self.model.save_model(path)\n",
    "\n",
    "for i in range(2):\n",
    "    if i%2==0:\n",
    "        xgbrf_model = XGBRFClassifierModel(model_path=f'xgbrf_model{i}.json')\n",
    "    else:\n",
    "        xgbrf_model = XGBRFClassifierModel(eval_metric='logloss', model_path=f'xgbrf_model{i}.json')\n",
    "    ktrain(xgbrf_model, xult, yult, epochs=10, batch_size=32, splits=5, random_state=5)\n",
    "    ens.add_model(xgbrf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6660, number of negative: 40256\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141956 -> initscore=-1.799140\n",
      "[LightGBM] [Info] Start training from score -1.799140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9929   97]\n",
      " [ 469 1234]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6728, number of negative: 40188\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143405 -> initscore=-1.787291\n",
      "[LightGBM] [Info] Start training from score -1.787291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[10010    84]\n",
      " [  427  1208]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6714, number of negative: 40202\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143107 -> initscore=-1.789722\n",
      "[LightGBM] [Info] Start training from score -1.789722\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9993   87]\n",
      " [ 472 1177]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6737, number of negative: 40179\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143597 -> initscore=-1.785730\n",
      "[LightGBM] [Info] Start training from score -1.785730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[10006    97]\n",
      " [  446  1180]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6613, number of negative: 40303\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.140954 -> initscore=-1.807388\n",
      "[LightGBM] [Info] Start training from score -1.807388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9893   86]\n",
      " [ 533 1217]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9611812282779981, 0.964728736421976, 0.9565523718078298, 0.9630451186725071, 0.9556980516226934]\n",
      "precisions:  [0.927122464312547, 0.934984520123839, 0.9311708860759493, 0.9240407204385278, 0.9339984650805833]\n",
      "recalls:  [0.724603640634175, 0.7388379204892966, 0.7137659187386295, 0.7257072570725708, 0.6954285714285714]\n",
      "f1s:  [0.8252210413503617, 0.8275446697450312, 0.8251157174481787, 0.8262898220704128, 0.8236610639869296]\n",
      "roc_aucs:  [0.864693537668673, 0.8661635690741987, 0.864217345208182, 0.8652820891669292, 0.8632094772578315]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9602411013606009\n",
      "Average Precision: 0.9302634112062893\n",
      "Average Recall: 0.7196686616726486\n",
      "Average F1 Score: 0.8255664629201828\n",
      "Average ROC AUC Score: 0.8647132036751628\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9974   81]\n",
      " [ 448 1226]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9969   86]\n",
      " [ 478 1196]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6699, number of negative: 40217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142787 -> initscore=-1.792332\n",
      "[LightGBM] [Info] Start training from score -1.792332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9962  103]\n",
      " [ 453 1211]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6623, number of negative: 40293\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141167 -> initscore=-1.805629\n",
      "[LightGBM] [Info] Start training from score -1.805629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[9900   89]\n",
      " [ 501 1239]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 6752, number of negative: 40164\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143917 -> initscore=-1.783132\n",
      "[LightGBM] [Info] Start training from score -1.783132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "[[10049    69]\n",
      " [  439  1172]]\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9628266458017345, 0.9593223233981322, 0.9615356552600405, 0.9589526352551025, 0.9582916004554084]\n",
      "precisions:  [0.9380260137719969, 0.9329173166926678, 0.921613394216134, 0.9329819277108434, 0.9443996776792909]\n",
      "recalls:  [0.7323775388291517, 0.7144563918757467, 0.7277644230769231, 0.7120689655172414, 0.7274984481688392]\n",
      "f1s:  [0.8264547666346733, 0.827292206053031, 0.8263069891873815, 0.8277463876543761, 0.8260209774603883]\n",
      "roc_aucs:  [0.8654184959437919, 0.8658740318349285, 0.8656817550900784, 0.865940230087593, 0.8652490287344586]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9601857720340835\n",
      "Average Precision: 0.9339876660141865\n",
      "Average Recall: 0.7228331534935803\n",
      "Average F1 Score: 0.8267642653979701\n",
      "Average ROC AUC Score: 0.8656327083381701\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class LGBMClassifierModel:\n",
    "    def __init__(self, eval_metric='logloss', model_path='lgbm_model.txt', **kwargs):\n",
    "        self.model = lgb.LGBMClassifier(objective='binary', metric=eval_metric, )\n",
    "        self.model_path = model_path\n",
    "        self.THRESHOLD = 0.5\n",
    "        self.init_model = None\n",
    "        self.load_model()\n",
    "\n",
    "    def fit(self, X_train, y_train, verbose=0, **kwargs):\n",
    "        self.model.fit(X_train, y_train,   init_model=self.init_model)\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        ret = self.model.predict(X_test).astype(float)\n",
    "        # All ret values multiplied by 1.1\n",
    "        # ret *= 1.1\n",
    "        print(colored('predicting using lgbm model', 'green'))\n",
    "        return ret.reshape(-1, 1)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        y_pred_proba = self.predict_proba(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        # Assuming loss is not directly available, using log loss as a proxy\n",
    "        loss = -1  # Placeholder, as LightGBM does not directly provide loss in this context\n",
    "        return [loss, auc, precision, recall]\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.booster_.save_model(self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_exists():\n",
    "            self.init_model = self.model_path\n",
    "\n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "\n",
    "    def model_exists(self):\n",
    "        try:\n",
    "            with open(self.model_path, 'r'):\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.booster_.save_model(path)\n",
    "\n",
    "# Example usage\n",
    "# Assuming xult and yult are your feature matrix and target vector\n",
    "# Split the data into training and test sets\n",
    "\n",
    "for i in range(2):\n",
    "    # Initialize the LightGBM classifier model\n",
    "    if i % 2 == 0:\n",
    "        lgbm_model = LGBMClassifierModel(model_path=f'lgbm_model{i}.txt')\n",
    "    else:\n",
    "        lgbm_model = LGBMClassifierModel(eval_metric='auc', model_path=f'lgbm_model{i}.txt')\n",
    "    # Fit the model on training data\n",
    "    ktrain(lgbm_model, xult, yult, epochs=10, batch_size=32, splits=5, random_state=i)\n",
    "    ens.add_model(lgbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6660, number of negative: 40256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141956 -> initscore=-1.799140\n",
      "[LightGBM] [Info] Start training from score -1.799140\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9890  136]\n",
      " [ 489 1214]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6728, number of negative: 40188\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143405 -> initscore=-1.787291\n",
      "[LightGBM] [Info] Start training from score -1.787291\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9968  126]\n",
      " [ 463 1172]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6714, number of negative: 40202\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143107 -> initscore=-1.789722\n",
      "[LightGBM] [Info] Start training from score -1.789722\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9980  100]\n",
      " [ 495 1154]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6737, number of negative: 40179\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143597 -> initscore=-1.785730\n",
      "[LightGBM] [Info] Start training from score -1.785730\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9961  142]\n",
      " [ 454 1172]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6613, number of negative: 40303\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.140954 -> initscore=-1.807388\n",
      "[LightGBM] [Info] Start training from score -1.807388\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9838  141]\n",
      " [ 548 1202]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9195876686557405, 0.9278652531645953, 0.9190576359409742, 0.9274246174610613, 0.9181929766795984]\n",
      "precisions:  [0.8992592592592593, 0.9029275808936826, 0.9202551834130781, 0.8919330289193302, 0.8950111690245719]\n",
      "recalls:  [0.7128596594245449, 0.7168195718654434, 0.6998180715585203, 0.7207872078720787, 0.6868571428571428]\n",
      "f1s:  [0.7959589638139514, 0.7939974542774838, 0.7968683401754267, 0.7962191461721095, 0.7936719548504644]\n",
      "roc_aucs:  [0.8491627025301222, 0.8483784831172727, 0.8486588389976185, 0.8487327482424554, 0.8495413388354021]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9224256303803939\n",
      "Average Precision: 0.9018772443019845\n",
      "Average Recall: 0.707428330715546\n",
      "Average F1 Score: 0.7953431718578872\n",
      "Average ROC AUC Score: 0.848894822344574\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9929  126]\n",
      " [ 482 1192]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9930  125]\n",
      " [ 507 1167]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6699, number of negative: 40217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142787 -> initscore=-1.792332\n",
      "[LightGBM] [Info] Start training from score -1.792332\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9930  135]\n",
      " [ 473 1191]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6623, number of negative: 40293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141167 -> initscore=-1.805629\n",
      "[LightGBM] [Info] Start training from score -1.805629\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9879  110]\n",
      " [ 529 1211]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6752, number of negative: 40164\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143917 -> initscore=-1.783132\n",
      "[LightGBM] [Info] Start training from score -1.783132\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[10009   109]\n",
      " [  473  1138]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9266386130761101, 0.9277500628264974, 0.92668517019183, 0.9199784130359487, 0.9202106637641075]\n",
      "precisions:  [0.9044006069802731, 0.903250773993808, 0.8981900452488688, 0.9167297501892505, 0.9125902165196471]\n",
      "recalls:  [0.7120669056152927, 0.6971326164874552, 0.7157451923076923, 0.6959770114942528, 0.7063935443823712]\n",
      "f1s:  [0.793958379950772, 0.7953143163730663, 0.7940036300894078, 0.7954514870138608, 0.795904192153436]\n",
      "roc_aucs:  [0.8478189043922296, 0.8487735970548217, 0.8474690999748192, 0.8479773468273712, 0.8484798947685701]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9242525845788988\n",
      "Average Precision: 0.9070322785863695\n",
      "Average Recall: 0.7054630540574129\n",
      "Average F1 Score: 0.7949264011161086\n",
      "Average ROC AUC Score: 0.8481037686035624\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6659, number of negative: 40257\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141935 -> initscore=-1.799315\n",
      "[LightGBM] [Info] Start training from score -1.799315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9888  137]\n",
      " [ 502 1202]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6699, number of negative: 40217\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142787 -> initscore=-1.792332\n",
      "[LightGBM] [Info] Start training from score -1.792332\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9935  130]\n",
      " [ 485 1179]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6664, number of negative: 40252\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142041 -> initscore=-1.798440\n",
      "[LightGBM] [Info] Start training from score -1.798440\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9903  127]\n",
      " [ 505 1194]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6701, number of negative: 40215\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142830 -> initscore=-1.791983\n",
      "[LightGBM] [Info] Start training from score -1.791983\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9919  148]\n",
      " [ 481 1181]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6729, number of negative: 40187\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5677\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143427 -> initscore=-1.787117\n",
      "[LightGBM] [Info] Start training from score -1.787117\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[10002    93]\n",
      " [  484  1150]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9230028801236346, 0.9218212926076655, 0.9255712556268805, 0.9275552654017123, 0.9236358025926281]\n",
      "precisions:  [0.8976848394324123, 0.9006875477463713, 0.9038607115821348, 0.8886380737396539, 0.9251810136765889]\n",
      "recalls:  [0.7053990610328639, 0.7085336538461539, 0.7027663331371395, 0.710589651022864, 0.7037943696450428]\n",
      "f1s:  [0.7939411515381355, 0.796911127573125, 0.7942637254463287, 0.7938866577420794, 0.7945597709377237]\n",
      "roc_aucs:  [0.8486217047959951, 0.848502239977744, 0.8480251327546717, 0.8485518058226922, 0.8482147148192928]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9243172992705041\n",
      "Average Precision: 0.9032104372354322\n",
      "Average Recall: 0.7062166137368129\n",
      "Average F1 Score: 0.7947124866474784\n",
      "Average ROC AUC Score: 0.8483831196340791\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6689, number of negative: 40227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142574 -> initscore=-1.794074\n",
      "[LightGBM] [Info] Start training from score -1.794074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9949  106]\n",
      " [ 504 1170]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6697, number of negative: 40219\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142744 -> initscore=-1.792680\n",
      "[LightGBM] [Info] Start training from score -1.792680\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9932  131]\n",
      " [ 457 1209]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6664, number of negative: 40252\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142041 -> initscore=-1.798440\n",
      "[LightGBM] [Info] Start training from score -1.798440\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9913  117]\n",
      " [ 498 1201]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6688, number of negative: 40228\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142553 -> initscore=-1.794248\n",
      "[LightGBM] [Info] Start training from score -1.794248\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9929  125]\n",
      " [ 500 1175]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6714, number of negative: 40202\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143107 -> initscore=-1.789722\n",
      "[LightGBM] [Info] Start training from score -1.789722\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9960  120]\n",
      " [ 507 1142]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9204515843862341, 0.9350520890061281, 0.9337715810778379, 0.9176484595126613, 0.9159250856700069]\n",
      "precisions:  [0.9169278996865203, 0.9022388059701493, 0.9112291350531108, 0.9038461538461539, 0.9049128367670365]\n",
      "recalls:  [0.6989247311827957, 0.7256902761104442, 0.7068864037669217, 0.7014925373134329, 0.6925409338993329]\n",
      "f1s:  [0.796509410682639, 0.7957796852646638, 0.7955850681981336, 0.7944971835095043, 0.7944701288208627]\n",
      "roc_aucs:  [0.8484726481027409, 0.8490396282690609, 0.8479872107857056, 0.8473220553623839, 0.8473020952825435]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.9245697599305736\n",
      "Average Precision: 0.9078309662645943\n",
      "Average Recall: 0.7051069764545854\n",
      "Average F1 Score: 0.7953682952951606\n",
      "Average ROC AUC Score: 0.8480247275604871\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6688, number of negative: 40228\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142553 -> initscore=-1.794248\n",
      "[LightGBM] [Info] Start training from score -1.794248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9935  119]\n",
      " [ 507 1168]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6712, number of negative: 40204\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5678\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.143064 -> initscore=-1.790070\n",
      "[LightGBM] [Info] Start training from score -1.790070\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9975  103]\n",
      " [ 481 1170]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6704, number of negative: 40212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142894 -> initscore=-1.791461\n",
      "[LightGBM] [Info] Start training from score -1.791461\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9929  141]\n",
      " [ 485 1174]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6647, number of negative: 40269\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.141679 -> initscore=-1.801416\n",
      "[LightGBM] [Info] Start training from score -1.801416\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9885  128]\n",
      " [ 513 1203]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 6701, number of negative: 40215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5679\n",
      "[LightGBM] [Info] Number of data points in the train set: 46916, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142830 -> initscore=-1.791983\n",
      "[LightGBM] [Info] Start training from score -1.791983\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[[9940  127]\n",
      " [ 480 1182]]\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "losses:  [-1, -1, -1, -1, -1]\n",
      "aucs:  [0.9284517634623779, 0.9253886313045345, 0.9210030988625133, 0.920750722196343, 0.922334647871296]\n",
      "precisions:  [0.9075369075369075, 0.9190887666928516, 0.8927756653992396, 0.903831705484598, 0.9029793735676088]\n",
      "recalls:  [0.6973134328358209, 0.7086614173228346, 0.707655213984328, 0.701048951048951, 0.7111913357400722]\n",
      "f1s:  [0.7955905088391477, 0.7951682794853948, 0.793669979407288, 0.7958466668158525, 0.7941676357455945]\n",
      "roc_aucs:  [0.8483169777355519, 0.8475512480357904, 0.8475860109981845, 0.8487830740745371, 0.8480818539876207]\n",
      "Average Loss: -1.0\n",
      "Average AUC: 0.923585772739413\n",
      "Average Precision: 0.905242483736241\n",
      "Average Recall: 0.7051740701864013\n",
      "Average F1 Score: 0.7948886140586555\n",
      "Average ROC AUC Score: 0.8480638329663371\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class LGBMRandomForestClassifierModel:\n",
    "    def __init__(self, eval_metric='logloss', model_path='lgbm_rf_model.txt', bagging_freq=1, bagging_fraction=0.8, feature_fraction=0.8, **kwargs):\n",
    "        self.model_path = model_path\n",
    "        self.model = lgb.LGBMClassifier(boosting_type='rf', objective='binary', metric=eval_metric,\n",
    "                                        bagging_freq=bagging_freq, bagging_fraction=bagging_fraction,\n",
    "                                        feature_fraction=feature_fraction, **kwargs)\n",
    "        self.THRESHOLD = 0.5\n",
    "        self.load_model()  # Load the saved model if available\n",
    "\n",
    "    def fit(self, X_train, y_train, verbose=0, **kwargs):\n",
    "        # Train model from scratch\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.save_model()  # Save model state after training\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print(colored('predicting using lgbm_rf model', 'green'))\n",
    "        ret = self.model.predict(X_test).astype(float)\n",
    "        return ret.reshape(-1, 1)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        y_pred_proba = self.predict_proba(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        loss = -1  # Placeholder for loss\n",
    "        return [loss, auc, precision, recall]\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.booster_.save_model(self.model_path)\n",
    "\n",
    "    def f1max(self, *args, **kwargs):\n",
    "        print(self.summary())\n",
    "        if args:\n",
    "            print(f\"iter: {args[0]}, epochs: {args[1]}, splits: {args[2]}\")\n",
    "        print(kwargs)\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_exists():\n",
    "            # Load the booster and convert to LGBMClassifier\n",
    "            booster = lgb.Booster(model_file=self.model_path)\n",
    "            self.model._Booster = booster  # Inject the booster into the LGBMClassifier\n",
    "\n",
    "    def model_exists(self):\n",
    "        try:\n",
    "            with open(self.model_path, 'r'):\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "# Assuming xult and yult are your feature matrix and target vector\n",
    "# Split the data into training and test sets\n",
    "\n",
    "# yyyyyyy.shape \n",
    "for i in range(5):\n",
    "    # Initialize the LightGBM classifier model\n",
    "    if i % 2 == 0:\n",
    "        lgbm_rf_model = LGBMRandomForestClassifierModel(model_path=f'lgbm_rf_model{i}.txt')\n",
    "    else:\n",
    "        lgbm_rf_model = LGBMRandomForestClassifierModel(eval_metric='auc', model_path=f'lgbm_rf_model{i}.txt')\n",
    "    # Fit the model on training data\n",
    "    ktrain(lgbm_rf_model, xult, yult, epochs=1, batch_size=32, splits=5, random_state=i)\n",
    "    ens.add_model(lgbm_rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mpredicting using xgboost model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 11.31%\n",
      "Percentage of predictions less than or equal to 0.5: 88.69%\n",
      "\u001b[32mPredicting with encoding_dim 512...\u001b[0m\n",
      "\u001b[1m 608/1222\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 923us/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 922us/step\n",
      "Percentage of predictions greater than 0.5: 12.09%\n",
      "Percentage of predictions less than or equal to 0.5: 87.91%\n",
      "\u001b[32mPredicting with encoding_dim 256...\u001b[0m\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 787us/step\n",
      "Percentage of predictions greater than 0.5: 10.96%\n",
      "Percentage of predictions less than or equal to 0.5: 89.04%\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step\n",
      "Percentage of predictions greater than 0.5: 9.56%\n",
      "Percentage of predictions less than or equal to 0.5: 90.44%\n",
      "\u001b[32mpredicting using dense model of hidden 64\u001b[0m\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677us/step\n",
      "Percentage of predictions greater than 0.5: 11.80%\n",
      "Percentage of predictions less than or equal to 0.5: 88.20%\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 10.61%\n",
      "Percentage of predictions less than or equal to 0.5: 89.39%\n",
      "\u001b[32mpredicting using xgbrf model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 10.61%\n",
      "Percentage of predictions less than or equal to 0.5: 89.39%\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 10.54%\n",
      "Percentage of predictions less than or equal to 0.5: 89.46%\n",
      "\u001b[32mpredicting using lgbm model\u001b[0m\n",
      "Percentage of predictions greater than 0.5: 10.66%\n",
      "Percentage of predictions less than or equal to 0.5: 89.34%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 11.11%\n",
      "Percentage of predictions less than or equal to 0.5: 88.89%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 10.80%\n",
      "Percentage of predictions less than or equal to 0.5: 89.20%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 10.82%\n",
      "Percentage of predictions less than or equal to 0.5: 89.18%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 10.71%\n",
      "Percentage of predictions less than or equal to 0.5: 89.29%\n",
      "\u001b[32mpredicting using lgbm_rf model\u001b[0m\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Percentage of predictions greater than 0.5: 10.85%\n",
      "Percentage of predictions less than or equal to 0.5: 89.15%\n"
     ]
    }
   ],
   "source": [
    "ens.save(testx, 'finals.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
